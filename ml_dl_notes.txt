Part‑1 Neural Networks and Deep Learning
This is the ﬁrst course of the deep learning specializa on at Coursera which is moderated by DeepLearning.ai. The
course is taught by Andrew Ng.

Andrew NG Course Notes Collec on
Part‑1 Neural Networks and Deep Learning
Part 2 : Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Part‑3: Structuring Machine Learning Projects
Part‑4 :Convolu onal Neural Networks
Part‑5 : Sequence Models

Table of contents
Neural Networks and Deep Learning
Table of contents
Course summary
Introduc on to deep learning
What is a (Neural Network) NN?
Supervised learning with neural networks
Why is deep learning taking oﬀ?
Neural Networks Basics
Binary classiﬁca on
Logis c regression
Logis c regression cost func on
Gradient Descent
Deriva ves
More Deriva ves examples
Computa on graph
Deriva ves with a Computa on Graph
Logis c Regression Gradient Descent
Gradient Descent on m Examples
Vectoriza on
Vectorizing Logis c Regression
Notes on Python and NumPy
General Notes
Shallow neural networks
Neural Networks Overview
Neural Network Representa on
Compu ng a Neural Network’s Output
Vectorizing across mul ple examples
Ac va on func ons
Why do you need non‑linear ac va on func ons?
Deriva ves of ac va on func ons
Gradient descent for Neural Networks
Random Ini aliza on

Deep Neural Networks
Deep L‑layer neural network
Forward Propaga on in a Deep Network
Ge ng your matrix dimensions right
Why deep representa ons?
Building blocks of deep neural networks
Forward and Backward Propaga on
Parameters vs Hyperparameters
What does this have to do with the brain
Extra: Ian Goodfellow interview

Course summary
Here are the course summary as its given on the course link:
If you want to break into cu ng‑edge AI, this course will help you do so. Deep learning engineers are highly
sought a er, and mastering deep learning will give you numerous new career opportuni es. Deep learning is
also a new “superpower” that will let you build AI systems that just weren’t possible a few years ago.
In this course, you will learn the founda ons of deep learning. When you ﬁnish this class, you will:
Understand the major technology trends driving Deep Learning
Be able to build, train and apply fully connected deep neural networks
Know how to implement eﬃcient (vectorized) neural networks
Understand the key parameters in a neural network’s architecture
This course also teaches you how Deep Learning actually works, rather than presen ng only a cursory or
surface‑level descrip on. So a er comple ng it, you will be able to apply deep learning to a your own
applica ons. If you are looking for a job in AI, a er this course you will also be able to answer basic interview
ques ons.

Introduc on to deep learning
Be able to explain the major trends driving the rise of deep learning, and understand where and how it is
applied today.

What is a (Neural Network) NN?
Single neuron == linear regression
Simple NN graph:

Image taken from tutorialspoint.com
RELU stands for rec ﬁed linear unit is the most popular ac va on func on right now that makes deep NNs train
faster now.
Hidden layers predicts connec on between inputs automa cally, thats what deep learning is good at.
Deep NN consists of more hidden layers (Deeper layers)

Image taken from opennn.net
Each Input will be connected to the hidden layer and the NN will decide the connec ons.
Supervised learning means we have the (X,Y) and we need to get the func on that maps X to Y.

Supervised learning with neural networks
Diﬀerent types of neural networks for supervised learning which includes:
CNN or convolu onal neural networks (Useful in computer vision)
RNN or Recurrent neural networks (Useful in Speech recogni on or NLP)

Standard NN (Useful for Structured data)
Hybrid/custom NN or a Collec on of NNs types
Structured data is like the databases and tables.
Unstructured data is like h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/1‑
Neural Networks and Deep Learning/Images/, video, audio, and text.
Structured data gives more money because companies relies on predic on on its big data.

Why is deep learning taking oﬀ?
Deep learning is taking oﬀ for 3 reasons:
1. Data:
Using this image we can conclude:

For small data NN can perform as Linear regression or SVM (Support vector machine)
For big data a small NN is be er that SVM
For big data a big NN is be er that a medium NN is be er that small NN.
Hopefully we have a lot of data because the world is using the computer a li le bit more
Mobiles
IOT (Internet of things)
2. Computa on:
GPUs.
Powerful CPUs.
Distributed compu ng.
ASICs
3. Algorithm:
1. Crea ve algorithms has appeared that changed the way NN works.
For example using RELU func on is so much be er than using SIGMOID func on in training a NN
because it helps with the vanishing gradient problem.

Neural Networks Basics
Learn to set up a machine learning problem with a neural network mindset. Learn to use vectoriza on to
speed up your models.

Binary classiﬁca on
Mainly he is talking about how to do a logis c regression to make a binary classiﬁer.

Image taken from 3.bp.blogspot.com
He talked about an example of knowing if the current image contains a cat or not.
Here are some nota ons:
M is the number of training vectors
Nx is the size of the input vector
Ny is the size of the output vector
X(1) is the first input vector
Y(1) is the first output vector
X = [x(1) x(2).. x(M)]
Y = (y(1) y(2).. y(M))
We will use python in this course.
In NumPy we can make matrices and make opera ons on them in a fast and reliable me.

Logis c regression
Algorithm is used for classiﬁca on algorithm of 2 classes.
Equa ons:
Simple equa on: y = wx + b
If x is a vector: y = w(transpose)x + b
If we need y to be in between 0 and 1 (probability): y = sigmoid(w(transpose)x + b)
In some nota ons this might be used: y = sigmoid(w(transpose)x)
While b is w0 of w and we add x0 = 1 . but we won’t use this nota on in the course (Andrew
said that the ﬁrst nota on is be er).
In binary classiﬁca on Y has to be between 0 and 1 .
In the last equa on w is a vector of Nx and b is a real number

Logis c regression cost func on
First loss func on would be the square root error: L(y',y) = 1/2 (y' - y)^2
But we won’t use this nota on because it leads us to op miza on problem which is non convex, means it
contains local op mum points.
This is the func on that we will use: L(y',y) = - (y*log(y') + (1-y)*log(1-y'))
To explain the last func on lets see:
if y = 1 ==> L(y',1) = -log(y') ==> we want y' to be the largest ==> y ’ biggest value is 1
if y = 0 ==> L(y',0) = -log(1-y') ==> we want 1-y' to be the largest ==> y' to be smaller
as possible because it can only has 1 value.
Then the Cost func on will be: J(w,b) = (1/m) * Sum(L(y'[i],y[i]))
The loss func on computes the error for a single training example; the cost func on is the average of the loss
func ons of the en re training set.

Gradient Descent

We want to predict w and b that minimize the cost func on.
Our cost func on is convex.
First we ini alize w and b to 0,0 or ini alize them to a random value in the convex func on and then try to
improve the values the reach minimum value.
In Logis c regression people always use 0,0 instead of random.
The gradient decent algorithm repeats: w = w - alpha * dw
where alpha is the learning rate and dw is the deriva ve of w (Change to w )
The deriva ve is also the slope of w
Looks like greedy algorithms. the deriva ve give us the direc on to improve our parameters.
The actual equa ons we will implement:
w = w - alpha * d(J(w,b) / dw) (how much the func on slopes in the w direc on)
b = b - alpha * d(J(w,b) / db) (how much the func on slopes in the d direc on)

Deriva ves
We will talk about some of required calculus.
You don’t need to be a calculus geek to master deep learning but you’ll need some skills from it.
Deriva ve of a linear line is its slope.
ex. f(a) = 3a

d(f(a))/d(a) = 3

if a = 2 then f(a) = 6
if we move a a li le bit a = 2.001 then f(a) = 6.003 means that we mul plied the deriva ve (Slope)
to the moved area and added it to the last result.

More Deriva ves examples
f(a) = a^2 ==> d(f(a))/d(a) = 2a
a = 2 ==> f(a) = 4
a = 2.0001 ==> f(a) = 4.0004 approx.
f(a) = a^3 ==> d(f(a))/d(a) = 3a^2
f(a) = log(a) ==> d(f(a))/d(a) = 1/a
To conclude, Deriva ve is the slope and slope is diﬀerent in diﬀerent points in the func on thats why the
deriva ve is a func on.

Computa on graph
Its a graph that organizes the computa on from le to right.

Deriva ves with a Computa on Graph
Calculus chain rule says:
If x -> y -> z (x eﬀect y and y eﬀects z)
Then d(z)/d(x) = d(z)/d(y) * d(y)/d(x)
The video illustrates a big example.

We compute the deriva ves on a graph from right to le and it will be a lot more easier.
dvar means the deriva ves of a ﬁnal output variable with respect to various intermediate quan

es.

Logis c Regression Gradient Descent
In the video he discussed the deriva ves of gradient decent example for one sample with two features x1 and
x2 .

Gradient Descent on m Examples
Lets say we have these variables:
1
2
3
4
5
6
7

X1
X2
W1
W2
B
M
Y(i)

Feature
Feature
Weight of the first feature.
Weight of the second feature.
Logistic Regression parameter.
Number of training examples
Expected output of i

So we have:

Then from right to le we will calculate deriva ons compared to the result:
1
2
3
4
5

d(a) = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))
d(z) = d(l)/d(z) = a - y
d(W1) = X1 * d(z)
d(W2) = X2 * d(z)
d(B) = d(z)

From the above we can conclude the logis c regression pseudo code:
1
2
3
4
5
6
7
8
9
10
11

J = 0; dw1 = 0; dw2 =0; db = 0;
# Devs.
w1 = 0; w2 = 0; b=0;
for i = 1 to m
# Forward pass
z(i) = W1*x1(i) + W2*x2(i) + b
a(i) = Sigmoid(z(i))
J += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))
# Backward pass
dz(i) = a(i) - Y(i)
dw1 += dz(i) * x1(i)

12
13
14
15
16
17
18
19
20
21
22

dw2 += dz(i) * x2(i)
db += dz(i)
J /= m
dw1/= m
dw2/= m
db/= m
# Gradient descent
w1 = w1 - alpa * dw1
w2 = w2 - alpa * dw2
b = b - alpa * db

The above code should run for some itera ons to minimize error.
So there will be two inner loops to implement the logis c regression.
Vectoriza on is so important on deep learning to reduce loops. In the last code we can make the whole loop in
one step using vectoriza on!

Vectoriza on
Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why
we need vectoriza on to get rid of some of our for loops.
NumPy library (dot) func on is using vectoriza on by default.
The vectoriza on can be done on CPU or GPU thought the SIMD opera on. But its faster on GPU.
Whenever possible avoid for loops.
Most of the NumPy library methods are vectorized version.

Vectorizing Logis c Regression
We will implement Logis c Regression using one for loop then without any for loop.
As an input we have a matrix X and its [Nx, m] and a matrix Y and its [Ny, m] .
We will then compute at instance [z1,z2...zm] = W' * X + [b,b,...b] . This can be wri en in python
as:
Z = np.dot(W.T,X) + b
A = 1 / 1 + np.exp(-Z)

# Vectorization, then broadcasting, Z shape is (1
# Vectorization, A shape is (1, m)

Vectorizing Logis c Regression’s Gradient Output:
dz = A - Y
dw = np.dot(X, dz.T) / m
db = dz.sum() / m

# Vectorization, dz shape is (1, m)
# Vectorization, dw shape is (Nx, 1)
# Vectorization, dz shape is (1, 1)

Notes on Python and NumPy
In NumPy, obj.sum(axis = 0) sums the columns while obj.sum(axis = 1) sums the rows.
In NumPy, obj.reshape(1,4) changes the shape of the matrix by broadcas ng the values.
Reshape is cheap in calcula ons so put it everywhere you’re not sure about the calcula ons.
Broadcas ng works when you do a matrix opera on with matrices that doesn’t match for the opera on, in this
case NumPy automa cally makes the shapes ready for the opera on by broadcas ng the values.
In general principle of broadcas ng. If you have an (m,n) matrix and you add(+) or subtract(‑) or mul ply(*) or
divide(/) with a (1,n) matrix, then this will copy it m mes into an (m,n) matrix. The same with if you use those

opera ons with a (m , 1) matrix, then this will copy it n mes into (m, n) matrix. And then apply the addi on,
subtrac on, and mul plica on of division element wise.
Some tricks to eliminate all the strange bugs in the code:
If you didn’t specify the shape of a vector, it will take a shape of (m,) and the transpose opera on won’t
work. You have to reshape it to (m, 1)
Try to not use the rank one matrix in ANN
Don’t hesitate to use assert(a.shape == (5,1)) to check if your matrix shape is the required one.
If you’ve found a rank one matrix try to run reshape on it.
Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at
the same me. It runs in the browser and doesn’t need an IDE to run.
To open Jupyter Notebook, open the command line and call: jupyter-notebook It should be installed to
work.
To Compute the deriva ve of Sigmoid:
1
2

s = sigmoid(x)
ds = s * (1 - s)

# derivative

using calculus

To make an image of (width,height,depth) be a vector, use this:
1

v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)

#reshapes

Gradient descent converges faster a er normaliza on of the input matrices.

General Notes
The main steps for building a Neural Network are:
Deﬁne the model structure (such as number of input features and outputs)
Ini alize the model’s parameters.
Loop.
Calculate current loss (forward propaga on)
Calculate current gradient (backward propaga on)
Update parameters (gradient descent)
Preprocessing the dataset is important.
Tuning the learning rate (which is an example of a “hyperparameter”) can make a big diﬀerence to the algorithm.
kaggle.com is a good place for datasets and compe

ons.

Pieter Abbeel is one of the best in deep reinforcement learning.

Shallow neural networks
Learn to build a neural network with one hidden layer, using forward propaga on and backpropaga on.

Neural Networks Overview
In logis c regression we had:
1
2
3

X1
X2
X3

\
==>

z = XW + B ==> a = Sigmoid(z) ==> l(a,Y)

/

In neural networks with one layer we will have:
1

X1

\

2
3

X2
X3

=>

z1 = XW1 + B1 => a1 = Sigmoid(z1) => z2 = a1W2 + B2 => a2 = Sigmoid(

/

X is the input vector (X1, X2, X3) , and Y is the output variable (1x1)
NN is stack of logis c regression objects.

Neural Network Representa on
We will deﬁne the neural networks that has one hidden layer.
NN contains of input layers, hidden layers, output layers.
Hidden layer means we cant see that layers in the training set.
a0 = x (the input layer)
a1 will represent the ac va on of the hidden neurons.
a2 will represent the output layer.
We are talking about 2 layers NN. The input layer isn’t counted.

Compu ng a Neural Network’s Output
Equa ons of Hidden layers:

Here are some informa ons about the last image:
noOfHiddenNeurons = 4
Nx = 3
Shapes of the variables:
W1 is the matrix of the ﬁrst hidden layer, it has a shape of (noOfHiddenNeurons,nx)
b1 is the matrix of the ﬁrst hidden layer, it has a shape of (noOfHiddenNeurons,1)
z1 is the result of the equa on z1 = W1*X + b , it has a shape of (noOfHiddenNeurons,1)
a1 is the result of the equa on a1 = sigmoid(z1) , it has a shape of
(noOfHiddenNeurons,1)
W2 is the matrix of the second hidden layer, it has a shape of (1,noOfHiddenNeurons)
b2 is the matrix of the second hidden layer, it has a shape of (1,1)
z2 is the result of the equa on z2 = W2*a1 + b , it has a shape of (1,1)
a2 is the result of the equa on a2 = sigmoid(z2) , it has a shape of (1,1)

Vectorizing across mul ple examples
Pseudo code for forward propaga on for the 2 layers NN:
1
2
3
4
5

for i = 1
z[1, i]
a[1, i]
z[2, i]
a[2, i]

to m
= W1*x[i] + b1
= sigmoid(z[1, i])
= W2*a[1, i] + b2
= sigmoid(z[2, i])

#
#
#
#

shape
shape
shape
shape

of
of
of
of

z[1,
a[1,
z[2,
a[2,

i]
i]
i]
i]

is
is
is
is

(noOfHiddenNeurons,1)
(noOfHiddenNeurons,1)
(1,1)
(1,1)

Lets say we have X on shape (Nx,m) . So the new pseudo code:
1
2
3
4

Z1
A1
Z2
A2

=
=
=
=

W1X + b1
sigmoid(Z1)
W2A1 + b2
sigmoid(Z2)

#
#
#
#

shape
shape
shape
shape

of
of
of
of

Z1
A1
Z2
A2

(noOfHiddenNeurons,m)
(noOfHiddenNeurons,m)
is (1,m)
is (1,m)

If you no ce always m is the number of columns.
In the last example we can call X = A0 . So the previous step can be rewri en as:
1
2
3
4

Z1
A1
Z2
A2

=
=
=
=

W1A0 + b1
sigmoid(Z1)
W2A1 + b2
sigmoid(Z2)

#
#
#
#

shape
shape
shape
shape

of
of
of
of

Z1
A1
Z2
A2

(noOfHiddenNeurons,m)
(noOfHiddenNeurons,m)
is (1,m)
is (1,m)

Ac va on func ons
So far we are using sigmoid, but in some cases other func ons can be a lot be er.
Sigmoid can lead us to gradient decent problem where the updates are so low.
Sigmoid ac va on func on range is [0,1]
A = 1 / (1 + np.exp(-z)) # Where z is the input matrix
Tanh ac va on func on range is [‑1,1] (Shi ed version of sigmoid func on)
In NumPy we can implement Tanh using one of these methods:
A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) # Where z is the input
matrix
Or
A = np.tanh(z) # Where z is the input matrix
It turns out that the tanh ac va on usually works be er than sigmoid ac va on func on for hidden units because
the mean of its output is closer to zero, and so it centers the data be er for the next layer.
Sigmoid or Tanh func on disadvantage is that if the input is too small or too high, the slope will be near zero
which will cause us the gradient decent problem.
One of the popular ac va on func ons that solved the slow gradient decent is the RELU func on.
RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the
slope remains linear.
So here is some basic rule for choosing ac va on func ons, if your classiﬁca on is between 0 and 1, use the
output ac va on as sigmoid and the others as RELU.
Leaky RELU ac va on func on diﬀerent of RELU is that if the input is nega ve the slope will be so small. It works
as RELU but most people uses RELU.
Leaky_RELU = max(0.01z,z) #the 0.01 can be a parameter for your algorithm.
In NN you will decide a lot of choices like:
No of hidden layers.
No of neurons in each hidden layer.
Learning rate. (The most important parameter)

Ac va on func ons.
And others…
It turns out there are no guide lines for that. You should try all ac va on func ons for example.

Why do you need non‑linear ac va on func ons?
If we removed the ac va on func on from our algorithm that can be called linear ac va on func on.
Linear ac va on func on will output linear ac va ons
Whatever hidden layers you add, the ac va on will be always linear like logis c regression (So its useless in a
lot of complex problems)
You might use linear ac va on func on in one place ‑ in the output layer if the output is real numbers (regression
problem). But even in this case if the output value is non‑nega ve you could use RELU instead.

Deriva ves of ac va on func ons
Deriva on of Sigmoid ac va on func on:
1
2
3

g(z) = 1 / (1 + np.exp(-z))
g'(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))
g'(z) = g(z) * (1 - g(z))

Deriva on of Tanh ac va on func on:
1
2

g(z) = (e^z - e^-z) / (e^z + e^-z)
g'(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2

Deriva on of RELU ac va on func on:
1
2
3

g(z) = np.maximum(0,z)
g'(z) = { 0 if z < 0
1 if z >= 0 }

Deriva on of leaky RELU ac va on func on:
1
2
3

g(z) = np.maximum(0.01 * z, z)
g'(z) = { 0.01 if z < 0
1
if z >= 0
}

Gradient descent for Neural Networks
In this sec on we will have the full back propaga on of the neural network (Just the equa ons with no
explana ons).
Gradient descent algorithm:
NN parameters:
n[0] = Nx
n[1] = NoOfHiddenNeurons
n[2] = NoOfOutputNeurons = 1
W1 shape is (n[1],n[0])
b1 shape is (n[1],1)
W2 shape is (n[2],n[1])
b2 shape is (n[2],1)
Cost func on I = I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))
Then Gradient descent:

1
2
3
4
5
6
7

Repeat:
Compute predictions (y'[i], i = 0,...m)
Get derivatives: dW1, db1, dW2, db2
Update: W1 = W1 - LearningRate * dW1
b1 = b1 - LearningRate * db1
W2 = W2 - LearningRate * dW2
b2 = b2 - LearningRate * db2

Forward propaga on:
1
2
3
4

Z1
A1
Z2
A2

=
=
=
=

W1A0 + b1
g1(Z1)
W2A1 + b2
Sigmoid(Z2)

# A0 is X

# Sigmoid because the output is between 0 and 1

Backpropaga on (deriva ons):
1
2
3
4
5
6
7

dZ2 = A2 - Y
# derivative of cost function we used * derivative of the s
dW2 = (dZ2 * A1.T) / m
db2 = Sum(dZ2) / m
dZ1 = (W2.T * dZ2) * g'1(Z1) # element wise product (*)
dW1 = (dZ1 * A0.T) / m
# A0 = X
db1 = Sum(dZ1) / m
# Hint there are transposes with multiplication because to keep dimensions co

How we derived the 6 equa ons of the backpropaga on:

Random Ini aliza on
In logis c regression it wasn’t important to ini alize the weights randomly, while in NN we have to ini alize them
randomly.
If we ini alize all the weights with zeros in NN it won’t work (ini alizing bias with zero is OK):
all hidden units will be completely iden cal (symmetric) ‑ compute exactly the same func on
on each gradient descent itera on all the hidden units will always update the same
To solve this we ini alize the W’s with a small random numbers:

1
2

W1 = np.random.randn((2,2)) * 0.01
b1 = np.zeros((2,1))

# 0.01 to make it small enough
# its ok to have b as zero, it won't ge

We need small values because in sigmoid (or tanh), for example, if the weight is too large you are more likely to
end up even at the very start of training with very large values of Z. Which causes your tanh or your sigmoid
ac va on func on to be saturated, thus slowing down learning. If you don’t have any sigmoid or tanh ac va on
func ons throughout your neural network, this is less of an issue.
Constant 0.01 is alright for 1 hidden layer networks, but if the NN is deep this number can be changed but it will
always be a small number.

Deep Neural Networks
Understand the key computa ons underlying deep learning, use them to build and train deep neural
networks, and apply it to computer vision.

Deep L‑layer neural network
Shallow NN is a NN with one or two layers.
Deep NN is a NN with three or more layers.
We will use the nota on L to denote the number of layers in a NN.
n[l] is the number of neurons in a speciﬁc layer l .
n[0] denotes the number of neurons input layer. n[L] denotes the number of neurons in output layer.
g[l] is the ac va on func on.
a[l] = g[l](z[l])
w[l] weights is used for z[l]
x = a[0] , a[l] = y'
These were the nota on we will use for deep neural network.
So we have:
A vector n of shape (1, NoOfLayers+1)
A vector g of shape (1, NoOfLayers)
A list of diﬀerent shapes w based on the number of neurons on the previous and the current layer.
A list of diﬀerent shapes b based on the number of neurons on the current layer.

Forward Propaga on in a Deep Network
Forward propaga on general rule for one input:
1
2

z[l] = W[l]a[l-1] + b[l]
a[l] = g[l](a[l])

Forward propaga on general rule for m inputs:
1
2

Z[l] = W[l]A[l-1] + B[l]
A[l] = g[l](A[l])

We can’t compute the whole layers forward propaga on without a for loop so its OK to have a for loop here.
The dimensions of the matrices are so important you need to ﬁgure it out.

Ge ng your matrix dimensions right
The best way to debug your matrices dimensions is by a pencil and paper.

Dimension of W is (n[l],n[l-1]) . Can be thought by right to le .
Dimension of b is (n[l],1)
dw has the same shape as W , while db is the same shape as b
Dimension of Z[l],

A[l] , dZ[l] , and dA[l] is (n[l],m)

Why deep representa ons?
Why deep NN works well, we will discuss this ques on in this sec on.
Deep NN makes rela ons with data from simpler to complex. In each layer it tries to make a rela on with the
previous layer. E.g.:
1. Face recogni on applica on:
Image ==> Edges ==> Face parts ==> Faces ==> desired face
2. Audio recogni on applica on:
Audio ==> Low level sound features like (sss,bb) ==> Phonemes ==> Words ==> Sentences
Neural Researchers think that deep neural networks “think” like brains (simple ==> complex)
Circuit theory and deep learning:

When star ng on an applica on don’t start directly by dozens of hidden layers. Try the simplest solu ons (e.g.
Logis c Regression), then try the shallow neural network and so on.

Building blocks of deep neural networks
Forward and back propaga on for a layer l:

Deep NN blocks:

Forward and Backward Propaga on
Pseudo code for forward propaga on for layer l:
1
2
3
4

Input
Z[l] =
A[l] =
Output

A[l-1]
W[l]A[l-1] + b[l]
g[l](Z[l])
A[l], cache(Z[l])

Pseudo code for back propaga on for layer l:
1
2
3
4
5
6

Input da[l], Caches
dZ[l] = dA[l] * g'[l](Z[l])
dW[l] = (dZ[l]A[l-1].T) / m
db[l] = sum(dZ[l])/m
dA[l-1] = w[l].T * dZ[l]
Output dA[l-1], dW[l], db[l]

If we have used our loss func on then:
1

dA[L] = (-(y/a) + ((1-y)/(1-a)))

# Dont forget axis=1, keepdims=True
# The multiplication here are a dot produ

Parameters vs Hyperparameters
Main parameters of the NN is W and b
Hyper parameters (parameters that control the algorithm) are like:
Learning rate.
Number of itera on.
Number of hidden layers L .
Number of hidden units n .
Choice of ac va on func ons.
You have to try values yourself of hyper parameters.
In the earlier days of DL and ML learning rate was o en called a parameter, but it really is (and now everybody call
it) a hyperparameter.
On the next course we will see how to op mize hyperparameters.

What does this have to do with the brain
The analogy that “It is like the brain” has become really an oversimpliﬁed explana on.
There is a very simplis c analogy between a single logis c unit and a single neuron in the brain.
No human today understand how a human brain neuron works.
No human today know exactly how many neurons on the brain.
Deep learning in Andrew’s opinion is very good at learning very ﬂexible, complex func ons to learn X to Y
mappings, to learn input‑output mappings (supervised learning).
The ﬁeld of computer vision has taken a bit more inspira on from the human brains then other disciplines that
also apply deep learning.
NN is a small representa on of how brain work. The most near model of human brain is in the computer vision
(CNN)

Extra: Ian Goodfellow interview
Ian is one of the world’s most visible deep learning researchers.
Ian is mainly working with genera ve models. He is the creator of GANs.
We need to stabilize GANs. Stabilized GANs can become the best genera ve models.
Ian wrote the ﬁrst textbook on the modern version of deep learning with Yoshua Bengio and Aaron Courville.
Ian worked with OpenAI.com and Google on ML and NN applica ons.
Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will ﬁnd you.
Ian thinks that we need to start an cipa ng security problems with ML now and make sure that these algorithms
are secure from the start instead of trying to patch it in retroac vely years later.

Part 2 : Improving Deep Neural Networks:
Hyperparameter tuning, Regulariza on and
Op miza on
This is the second course of the deep learning specializa on at Coursera which is moderated by DeepLearning.ai. The
course is taught by Andrew Ng.

Andrew NG Course Notes Collec on
Part‑1 Neural Networks and Deep Learning
Part 2 : Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Part‑3: Structuring Machine Learning Projects
Part‑4 :Convolu onal Neural Networks
Part‑5 : Sequence Models

Table of contents
Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Table of contents
Course summary
Prac cal aspects of Deep Learning
Train / Dev / Test sets
Bias / Variance
Basic Recipe for Machine Learning
Regulariza on
Why regulariza on reduces overﬁ ng?
Dropout Regulariza on
Understanding Dropout
Other regulariza on methods
Normalizing inputs
Vanishing / Exploding gradients
Weight Ini aliza on for Deep Networks
Numerical approxima on of gradients
Gradient checking implementa on notes
Ini aliza on summary
Regulariza on summary
Op miza on algorithms
Mini‑batch gradient descent
Understanding mini‑batch gradient descent
Exponen ally weighted averages
Understanding exponen ally weighted averages
Bias correc on in exponen ally weighted averages
Gradient descent with momentum
RMSprop
Adam op miza on algorithm
Learning rate decay
The problem of local op ma
Hyperparameter tuning, Batch Normaliza on and Programming Frameworks
Tuning process
Using an appropriate scale to pick hyperparameters
Hyperparameters tuning in prac ce: Pandas vs. Caviar
Normalizing ac va ons in a network
Fi ng Batch Normaliza on into a neural network
Why does Batch normaliza on work?
Batch normaliza on at test me
So max Regression

Training a So max classiﬁer
Deep learning frameworks
TensorFlow
Extra Notes

Course summary
Here are the course summary as its given on the course link:
This course will teach you the “magic” of ge ng deep learning to work well. Rather than the deep learning
process being a black box, you will understand what drives performance, and be able to more systema cally
get good results. You will also learn TensorFlow.
A er 3 weeks, you will:
Understand industry best‑prac ces for building deep learning applica ons.
Be able to eﬀec vely use the common neural network “tricks”, including ini aliza on, L2 and dropout
regulariza on, Batch normaliza on, gradient checking,
Be able to implement and apply a variety of op miza on algorithms, such as mini‑batch gradient descent,
Momentum, RMSprop and Adam, and check for their convergence.
Understand new best‑prac ces for the deep learning era of how to set up train/dev/test sets and analyze
bias/variance
Be able to implement a neural network in TensorFlow.
This is the second course of the Deep Learning Specializa on.

Prac cal aspects of Deep Learning
Train / Dev / Test sets
Its impossible to get all your hyperparameters right on a new applica on from the ﬁrst me.
So the idea is you go through the loop: Idea ==> Code ==> Experiment .
You have to go through the loop many mes to ﬁgure out your hyperparameters.
Your data will be split into three parts:
Training set. (Has to be the largest set)
Hold‑out cross valida on set / Development or “dev” set.
Tes ng set.
You will try to build a model upon training set then try to op mize hyperparameters on dev set as much as
possible. Then a er your model is ready you try and evaluate the tes ng set.
so the trend on the ra o of spli ng the models:
If size of the dataset is 100 to 1000000 ==> 60/20/20
If size of the dataset is 1000000 to INF ==> 98/1/1 or 99.5/0.25/0.25
The trend now gives the training data the biggest sets.
Make sure the dev and test set are coming from the same distribu on.
For example if cat training pictures is from the web and the dev/test pictures are from users cell phone they
will mismatch. It is be er to make sure that dev and test set are from the same distribu on.
The dev set rule is to try them on some of the good models you’ve created.
Its OK to only have a dev set without a tes ng set. But a lot of people in this case call the dev set as the test set. A
be er terminology is to call it a dev set as its used in the development.

Bias / Variance
Bias / Variance techniques are Easy to learn, but diﬃcult to master.
So here the explana on of Bias / Variance:
If your model is underﬁ ng (logis c regression of non linear data) it has a “high bias”
If your model is overﬁ ng then it has a “high variance”
Your model will be alright if you balance the Bias / Variance
For more:

Another idea to get the bias / variance if you don’t have a 2D plo ng mechanism:
High variance (overﬁ ng) for example:
Training error: 1%
Dev error: 11%
high Bias (underﬁ ng) for example:
Training error: 15%
Dev error: 14%
high Bias (underﬁ ng) && High variance (overﬁ ng) for example:
Training error: 15%
Test error: 30%
Best:
Training error: 0.5%
Test error: 1%
These Assump ons came from that human has 0% error. If the problem isn’t like that you’ll need to use
human error as baseline.

Basic Recipe for Machine Learning
If your algorithm has a high bias:
Try to make your NN bigger (size of hidden units, number of layers)
Try a diﬀerent model that is suitable for your data.
Try to run it longer.
Diﬀerent (advanced) op miza on algorithms.
If your algorithm has a high variance:
More data.

Try regulariza on.
Try a diﬀerent model that is suitable for your data.
You should try the previous two points un l you have a low bias and low variance.
In the older days before deep learning, there was a “Bias/variance tradeoﬀ”. But because now you have more
op ons/tools for solving the bias and variance problem its really helpful to use deep learning.
Training a bigger neural network never hurts.

Regulariza on
Adding regulariza on to NN will help it reduce variance (overﬁ ng)
L1 matrix norm:
||W|| = Sum(|w[i,j]|) # sum of absolute values of all w
L2 matrix norm because of arcane technical math reasons is called Frobenius norm:
||W||^2 = Sum(|w[i,j]|^2) # sum of all w squared
Also can be calculated as ||W||^2 = W.T * W
Regulariza on for logis c regression:
The normal cost func on that we want to minimize is: J(w,b) = (1/m) * Sum(L(y(i),y'(i)))
The L2 regulariza on version: J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) *
Sum(|w[i]|^2)
The L1 regulariza on version: J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) *
Sum(|w[i]|)
The L1 regulariza on version makes a lot of w values become zeros, which makes the model size smaller.
L2 regulariza on is being used much more o en.
lambda here is the regulariza on parameter (hyperparameter)
Regulariza on for NN:
The normal cost func on that we want to minimize is:
J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i)))
The L2 regulariza on version:
J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2)
We stack the matrix as one vector (mn,1) and then we apply sqrt(w1^2 + w2^2.....)
To do back propaga on (old way):
dw[l] = (from back propagation)
The new way:
dw[l] = (from back propagation) + lambda/m * w[l]
So plugging it in weight update step:
1
2
3
4

w[l] =
=
=
=

w[l]
w[l]
w[l]
(1 -

- learning_rate * dw[l]
- learning_rate * ((from back propagation) + lambda/m * w[l
- (learning_rate*lambda/m) * w[l] - learning_rate * (from b
(learning_rate*lambda)/m) * w[l] - learning_rate * (from ba

In prac ce this penalizes large weights and eﬀec vely limits the freedom in your model.
The new term (1 - (learning_rate*lambda)/m) * w[l] causes the weight to decay in propor on
to its size.

Why regulariza on reduces overﬁ ng?
Here are some intui ons:
Intui on 1:

If lambda is too large ‑ a lot of w’s will be close to zeros which will make the NN simpler (you can think of it
as it would behave closer to logis c regression).
If lambda is good enough it will just reduce some weights that makes the neural network overﬁt.
Intui on 2 (with tanh ac va on func on):
If lambda is too large, w’s will be small (close to zero) ‑ will use the linear part of the tanh ac va on
func on, so we will go from non linear ac va on to roughly linear which would make the NN a roughly linear
classiﬁer.
If lambda good enough it will just make some of tanh ac va ons roughly linear which will prevent
overﬁ ng.

Implementa on p: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost
func on J as a func on of the number of itera ons of gradient descent and you want to see that the cost func on J
decreases monotonically a er every eleva on of gradient descent with regulariza on. If you plot the old deﬁni on of
J (no regulariza on) then you might not see it decrease monotonically.

Dropout Regulariza on
In most cases Andrew Ng tells that he uses the L2 regulariza on.
The dropout regulariza on eliminates some neurons/weights on each itera on based on a probability.
A most common technique to implement dropout is called “Inverted dropout”.
Code for Inverted dropout:
1
2
3
4
5
6
7
8
9
10

keep_prob = 0.8
# 0 <= keep_prob <= 1
l = 3 # this code is only for layer 3
# the generated number that are less than 0.8 will be dropped. 80% stay, 20%
d3 = np.random.rand(a[l].shape[0], a[l].shape[1]) < keep_prob
a3 = np.multiply(a3,d3)

# keep only the values in d3

# increase a3 to not reduce the expected value of output
# (ensures that the expected value of a3 remains the same) - to solve the sca
a3 = a3 / keep_prob

Vector d[l] is used for forward and back propaga on and is the same for them, but it is diﬀerent for each itera on
(pass) or training example.
At test me we don’t use dropout. If you implement dropout at test me ‑ it would add noise to predic ons.

Understanding Dropout
In the previous video, the intui on was that dropout randomly knocks out units in your network. So it’s as if on
every itera on you’re working with a smaller NN, and so using a smaller NN seems like it should have a
regularizing eﬀect.
Another intui on: can’t rely on any one feature, so have to spread out weights.
It’s possible to show that dropout has a similar eﬀect to L2 regulariza on.
Dropout can have diﬀerent keep_prob per layer.
The input layer dropout has to be near 1 (or 1 ‑ no dropout) because you don’t want to eliminate a lot of features.
If you’re more worried about some layers overﬁ ng than others, you can set a lower keep_prob for some
layers than others. The downside is, this gives you even more hyperparameters to search for using cross‑
valida on. One other alterna ve might be to have some layers where you apply dropout and some layers where
you don’t apply dropout and then just have one hyperparameter, which is a keep_prob for the layers for which
you do apply dropouts.

A lot of researchers are using dropout with Computer Vision (CV) because they have a very big input size and
almost never have enough data, so overﬁ ng is the usual problem. And dropout is a regulariza on technique to
prevent overﬁ ng.
A downside of dropout is that the cost func on J is not well deﬁned and it will be hard to debug (plot J by
itera on).
To solve that you’ll need to turn oﬀ dropout, set all the keep_prob s to 1, and then run the code and check
that it monotonically decreases J and then turn on the dropouts again.

Other regulariza on methods
Data augmenta on:
For example in a computer vision data:
You can ﬂip all your pictures horizontally this will give you m more data instances.
You could also apply a random posi on and rota on to an image to get more data.
For example in OCR, you can impose random rota ons and distor ons to digits/le ers.
New data obtained using this technique isn’t as good as the real independent data, but s ll can be used as a
regulariza on technique.
Early stopping:
In this technique we plot the training set and the dev set cost together for each itera on. At some itera on
the dev set cost will stop decreasing and will start increasing.
We will pick the point at which the training set error and dev set error are best (lowest training cost with
lowest dev cost).
We will take these parameters as the best parameters.

Andrew prefers to use L2 regulariza on instead of early stopping because this technique simultaneously tries
to minimize the cost func on and not to overﬁt which contradicts the orthogonaliza on approach (will be
discussed further).
But its advantage is that you don’t need to search a hyperparameter like in other regulariza on approaches
(like lambda in L2 regulariza on).
Model Ensembles:
Algorithm:
Train mul ple independent models.
At test me average their results.

It can get you extra 2% performance.
It reduces the generaliza on error.
You can use some snapshots of your NN at the training ensembles them and take the results.

Normalizing inputs
If you normalize your inputs this will speed up the training process a lot.
Normaliza on are going on these steps:
1. Get the mean of the training set: mean = (1/m) * sum(x(i))
2. Subtract the mean from each input: X = X - mean
This makes your inputs centered around 0.
3. Get the variance of the training set: variance = (1/m) * sum(x(i)^2)
4. Normalize the variance. X /= variance
These steps should be applied to training, dev, and tes ng sets (but using mean and variance of the train set).
Why normalize?
If we don’t normalize the inputs our cost func on will be deep and its shape will be inconsistent (elongated)
then op mizing it will take a long me.
But if we normalize it the opposite will occur. The shape of the cost func on will be consistent (look more
symmetric like circle in 2D example) and we can use a larger learning rate alpha ‑ the op miza on will be
faster.

Vanishing / Exploding gradients
The Vanishing / Exploding gradients occurs when your deriva ves become very small or very big.
To understand the problem, suppose that we have a deep neural network with number of layers L, and all the
ac va on func ons are linear and each b = 0
Then:
1

Y' = W[L]W[L-1].....W[2]W[1]X

Then, if we have 2 hidden units per layer and x1 = x2 = 1, we result in:
1
2
3
4

if W[l] = [1.5
0]
[0
1.5] (l != L because of different dimensions in the output
Y' = W[L] [1.5 0]^(L-1) X = 1.5^L
# which will be very large
[0 1.5]

1
2
3
4

if W[l] = [0.5 0]
[0 0.5]
Y' = W[L] [0.5 0]^(L-1) X = 0.5^L
[0 0.5]

# which will be very small

The last example explains that the ac va ons (and similarly deriva ves) will be decreased/increased exponen ally
as a func on of number of layers.
So If W > I (Iden ty matrix) the ac va on and gradients will explode.
And If W < I (Iden ty matrix) the ac va on and gradients will vanish.
Recently Microso trained 152 layers (ResNet)! which is a really big number. With such a deep neural network, if
your ac va ons or gradients increase or decrease exponen ally as a func on of L, then these values could get
really big or really small. And this makes training diﬃcult, especially if your gradients are exponen ally smaller
than L, then gradient descent will take ny li le steps. It will take a long me for gradient descent to learn
anything.

There is a par al solu on that doesn’t completely solve this problem but it helps a lot ‑ careful choice of how you
ini alize the weights (next video).

Weight Ini aliza on for Deep Networks
A par al solu on to the Vanishing / Exploding gradients in NN is be er or more careful choice of the random
ini aliza on of weights
In a single neuron (Perceptron model): Z = w1x1 + w2x2 + ... + wnxn
So if n_x is large we want W 's to be smaller to not explode the cost.
So it turns out that we need the variance which equals 1/n_x to be the range of W 's
So lets say when we ini alize W 's like this (be er to use with tanh ac va on):
1

np.random.rand(shape) * np.sqrt(1/n[l-1])

or varia on of this (Bengio et al.):
1

np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))

Se ng ini aliza on part inside sqrt to 2/n[l-1] for ReLU is be er:
1

np.random.rand(shape) * np.sqrt(2/n[l-1])

Number 1 or 2 in the nominator can also be a hyperparameter to tune (but not the ﬁrst to start with)
This is one of the best way of par ally solu on to Vanishing / Exploding gradients (ReLU + Weight Ini aliza on
with variance) which will help gradients not to vanish/explode too quickly
The ini aliza on in this video is called “He Ini aliza on / Xavier Ini aliza on” and has been published in 2015
paper.

Numerical approxima on of gradients
There is an technique called gradient checking which tells you if your implementa on of backpropaga on is
correct.
There’s a numerical way to calculate the deriva ve:

Gradient checking approximates the gradients and is very helpful for ﬁnding the errors in your backpropaga on
implementa on but it’s slower than gradient descent (so use only for debugging).
Implementa on of this is very simple.
Gradient checking:
First take W[1],b[1],...,W[L],b[L] and reshape into one big vector ( theta )
The cost func on will be L(theta)
Then take dW[1],db[1],...,dW[L],db[L] into one big vector ( d_theta )
Algorithm:
1
2
3

eps = 10^-7
# small number
for i in len(theta):
d_theta_approx[i] = (J(theta1,...,theta[i] + eps) -

J(theta1,...,theta[

Finally we evaluate this formula (||d_theta_approx - d_theta||) /
(||d_theta_approx||+||d_theta||) ( || ‑ Euclidean vector norm) and check (with eps = 10^‑7):
if it is < 10^‑7 ‑ great, very likely the backpropaga on implementa on is correct
if around 10^‑5 ‑ can be OK, but need to inspect if there are no par cularly big values in
d_theta_approx - d_theta vector
if it is >= 10^‑3 ‑ bad, probably there is a bug in backpropaga on implementa on

Gradient checking implementa on notes
Don’t use the gradient checking algorithm at training me because it’s very slow.
Use gradient checking only for debugging.
If algorithm fails grad check, look at components to try to iden fy the bug.
Don’t forget to add lamda/(2m) * sum(W[l]) to J if you are using L1 or L2 regulariza on.
Gradient checking doesn’t work with dropout because J is not consistent.
You can ﬁrst turn oﬀ dropout (set keep_prob = 1.0 ), run gradient checking and then turn on dropout
again.
Run gradient checking at random ini aliza on and train the network for a while maybe there’s a bug which can be
seen when w’s and b’s become larger (further from 0) and can’t be seen on the ﬁrst itera on (when w’s and b’s are
very small).

Ini aliza on summary
The weights W [l] should be ini alized randomly to break symmetry
It is however okay to ini alize the biases b[l] to zeros. Symmetry is s ll broken so long as W [l] is ini alized
randomly
Diﬀerent ini aliza ons lead to diﬀerent results
Random ini aliza on is used to break symmetry and make sure diﬀerent hidden units can learn diﬀerent things
Don’t in alize to values that are too large
He ini aliza on works well for networks with ReLU ac va ons.

Regulariza on summary
1. L2 Regulariza on
Observa ons:
The value of λ is a hyperparameter that you can tune using a dev set.

L2 regulariza on makes your decision boundary smoother. If λ is too large, it is also possible to “oversmooth”,
resul ng in a model with high bias.
What is L2‑regulariza on actually doing?:
L2‑regulariza on relies on the assump on that a model with small weights is simpler than a model with large
weights. Thus, by penalizing the square values of the weights in the cost func on you drive all the weights to
smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which
the output changes more slowly as the input changes.
What you should remember:
Implica ons of L2‑regulariza on on:
cost computa on:
A regulariza on term is added to the cost
backpropaga on func on:
There are extra terms in the gradients with respect to weight matrices
weights:
weights end up smaller (“weight decay”) ‑ are pushed to smaller values.

2. Dropout
What you should remember about dropout:
Dropout is a regulariza on technique.
You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test me.
Apply dropout both during forward and backward propaga on.
During training me, divide each dropout layer by keep_prob to keep the same expected value for the ac va ons.
For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be
scaled by 0.5 since only the remaining half are contribu ng to the solu on. Dividing by 0.5 is equivalent to
mul plying by 2. Hence, the output now has the same expected value. You can check that this works even when
keep_prob is other values than 0.5.

Op miza on algorithms
Mini‑batch gradient descent
Training NN with a large data is slow. So to ﬁnd an op miza on algorithm that runs faster is a good idea.
Suppose we have m = 50 million . To train this data it will take a huge processing me for one step.
because 50 million won’t ﬁt in the memory at once we need other processing to make such a thing.
It turns out you can make a faster algorithm to make gradient descent process some of your items even before you
ﬁnish the 50 million items.
Suppose we have split m to mini batches of size 1000.
X{1} = 0 ... 1000
X{2} = 1001 ... 2000
...
X{bs} = ...
We similarly split X & Y .
So the deﬁni on of mini batches ==> t: X{t}, Y{t}
In Batch gradient descent we run the gradient descent on the whole dataset.
While in Mini‑Batch gradient descent we run the gradient descent on the mini datasets.

Mini‑Batch algorithm pseudo code:
1
2
3
4
5

for t = 1:No_of_batches
AL, caches = forward_prop(X{t}, Y{t})
cost = compute_cost(AL, Y{t})
grads = backward_prop(AL, caches)
update_parameters(grads)

# this is called an epoch

The code inside an epoch should be vectorized.
Mini‑batch gradient descent works much faster in the large datasets.

Understanding mini‑batch gradient descent
In mini‑batch algorithm, the cost won’t go down with each step as it does in batch algorithm. It could contain
some ups and downs but generally it has to go down (unlike the batch gradient descent where cost func on
descreases on each itera on).

Mini‑batch size:
( mini batch size = m ) ==> Batch gradient descent
( mini batch size = 1 ) ==> Stochas c gradient descent (SGD)
( mini batch size = between 1 and m ) ==> Mini‑batch gradient descent
Batch gradient descent:
too long per itera on (epoch)
Stochas c gradient descent:
too noisy regarding cost minimiza on (can be reduced by using smaller learning rate)
won’t ever converge (reach the minimum cost)
lose speedup from vectoriza on
Mini‑batch gradient descent:
1. faster learning:
you have the vectoriza on advantage
make progress without wai ng to process the en re training set
2. doesn’t always exactly converge (oscelates in a very small region, but you can reduce learning rate)
Guidelines for choosing mini‑batch size:
1. If small training set (< 2000 examples) ‑ use batch gradient descent.

2. It has to be a power of 2 (because of the way computer memory is layed out and accessed, some mes your
code runs faster if your mini‑batch size is a power of 2):
64, 128, 256, 512, 1024, ...
3. Make sure that mini‑batch ﬁts in CPU/GPU memory.
Mini‑batch size is a hyperparameter .

Exponen ally weighted averages
There are op miza on algorithms that are be er than gradient descent, but you should ﬁrst learn about
Exponen ally weighted averages.
If we have data like the temperature of day through the year it could be like this:
1
2
3
4
5
6

t(1) =
t(2) =
t(3) =
...
t(180)
...

40
49
45
= 60

This data is small in winter and big in summer. If we plot this data we will ﬁnd it some noisy.
Now lets compute the Exponen ally weighted averages:
1
2
3
4
5

V0 =
V1 =
V2 =
V3 =
...

0
0.9 * V0 + 0.1 * t(1) = 4
0.9 * V1 + 0.1 * t(2) = 8.5
0.9 * V2 + 0.1 * t(3) = 12.15

# 0.9 and 0.1 are hyperparameters

General equa on
1

V(t) = beta * v(t-1) + (1-beta) * theta(t)

If we plot this it will represent averages over ~ (1 / (1 - beta)) entries:
beta = 0.9 will average last 10 entries
beta = 0.98 will average last 50 entries
beta = 0.5 will average last 2 entries
Best beta average for our case is between 0.9 and 0.98

Another imagery example:

(taken from investopedia.com)

Understanding exponen ally weighted averages
Intui ons:

We can implement this algorithm with more accurate results using a moving window. But the code is more
eﬃcient and faster using the exponen ally weighted averages algorithm.
Algorithm is very simple:
1
2
3
4
5
6

v = 0
Repeat
{
Get theta(t)
v = beta * v + (1-beta) * theta(t)
}

Bias correc on in exponen ally weighted averages
The bias correc on helps make the exponen ally weighted averages more accurate.
Because v(0) = 0 , the bias of the weighted averages is shi ed and the accuracy suﬀers at the start.
To solve the bias issue we have to use this equa on:
1

v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)

As t becomes larger the (1 - beta^t) becomes close to 1

Gradient descent with momentum
The momentum algorithm almost always works faster than standard gradient descent.
The simple idea is to calculate the exponen ally weighted averages for your gradients and then update your
weights with the new values.
Pseudo code:
1
2
3
4
5
6
7
8
9

vdW = 0, vdb = 0
on iteration t:
# can be mini-batch or batch gradient descent
compute dw, db on current mini-batch
vdW
vdb
W =
b =

=
=
W
b

beta * vdW + (1
beta * vdb + (1
- learning_rate
- learning_rate

*
*

beta) * dW
beta) * db
vdW
vdb

Momentum helps the cost func on to go to the minimum point in a more fast and consistent way.
beta is another hyperparameter . beta = 0.9 is very common and works very well in most cases.
In prac ce people don’t bother implemen ng bias correc on.

RMSprop
Stands for Root mean square prop.
This algorithm speeds up the gradient descent.
Pseudo code:
1
2
3
4
5
6
7
8
9

sdW = 0, sdb = 0
on iteration t:
# can be mini-batch or batch gradient descent
compute dw, db on current mini-batch
sdW
sdb
W =
b =

=
=
W
B

(beta * sdW) + (1
(beta * sdb) + (1
- learning_rate *
- learning_rate *

- beta) * dW^2
- beta) * db^2
dW / sqrt(sdW)
db / sqrt(sdb)

# squaring is element-wise
# squaring is element-wise

RMSprop will make the cost func on move slower on the ver cal direc on and faster on the horizontal direc on
in the following example:

Ensure that sdW is not zero by adding a small value epsilon (e.g. epsilon = 10^-8 ) to it:
W = W - learning_rate * dW / (sqrt(sdW) + epsilon)
With RMSprop you can increase your learning rate.
Developed by Geoﬀrey Hinton and ﬁrstly introduced on Coursera.org course.

Adam op miza on algorithm
Stands for Adap ve Moment Es ma on.
Adam op miza on and RMSprop are among the op miza on algorithms that worked very well with a lot of NN
architectures.
Adam op miza on simply puts RMSprop and momentum together!
Pseudo code:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

vdW = 0, vdW = 0
sdW = 0, sdb = 0
on iteration t:
# can be mini-batch or batch gradient descent
compute dw, db on current mini-batch
vdW = (beta1 * vdW) + (1 - beta1) * dW
vdb = (beta1 * vdb) + (1 - beta1) * db

# momentum
# momentum

sdW = (beta2 * sdW) + (1 - beta2) * dW^2
sdb = (beta2 * sdb) + (1 - beta2) * db^2

# RMSprop
# RMSprop

vdW = vdW / (1 - beta1^t)
vdb = vdb / (1 - beta1^t)

# fixing bias
# fixing bias

sdW = sdW / (1 - beta2^t)
sdb = sdb / (1 - beta2^t)

# fixing bias
# fixing bias

W = W - learning_rate * vdW / (sqrt(sdW) + epsilon)
b = B - learning_rate * vdb / (sqrt(sdb) + epsilon)

Hyperparameters for Adam:
Learning rate: needed to be tuned.

beta1 : parameter of the momentum ‑ 0.9 is recommended by default.
beta2 : parameter of the RMSprop ‑ 0.999 is recommended by default.
epsilon : 10^-8 is recommended by default.

Learning rate decay
Slowly reduce learning rate.
As men oned before mini‑batch gradient descent won’t reach the op mum point (converge). But by making the
learning rate decay with itera ons it will be much closer to it because the steps (and possible oscilla ons) near the
op mum are smaller.
One technique equa ons is learning_rate = (1 / (1 + decay_rate * epoch_num)) *
learning_rate_0
epoch_num is over all data (not a single mini‑batch).
Other learning rate decay methods (con nuous):
learning_rate = (0.95 ^ epoch_num) * learning_rate_0
learning_rate = (k / sqrt(epoch_num)) * learning_rate_0
Some people perform learning rate decay discretely ‑ repeatedly decrease a er some number of epochs.
Some people are making changes to the learning rate manually.
decay_rate is another hyperparameter .
For Andrew Ng, learning rate decay has less priority.

The problem of local op ma
The normal local op ma is not likely to appear in a deep neural network because data is usually high dimensional.
For point to be a local op ma it has to be a local op ma for each of the dimensions which is highly unlikely.
It’s unlikely to get stuck in a bad local op ma in high dimensions, it is much more likely to get to the saddle point
rather to the local op ma, which is not a problem.
Plateaus can make learning slow:
Plateau is a region where the deriva ve is close to zero for a long me.
This is where algorithms like momentum, RMSprop or Adam can help.

Hyperparameter tuning, Batch Normaliza on and Programming
Frameworks
Tuning process
We need to tune our hyperparameters to get the best out of them.
Hyperparameters importance are (as for Andrew Ng):
1. Learning rate.
2. Momentum beta.
3. Mini‑batch size.
4. No. of hidden units.
5. No. of layers.
6. Learning rate decay.
7. Regulariza on lambda.
8. Ac va on func ons.
9. Adam beta1 & beta2 .
Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.

One of the ways to tune is to sample a grid with N hyperparameter se ngs and then try all se ngs
combina ons on your problem.
Try random values: don’t use a grid.
You can use Coarse to fine sampling scheme :
When you ﬁnd some hyperparameters values that give you a be er performance ‑ zoom into a smaller region
around these values and sample more densely within this space.
These methods can be automated.

Using an appropriate scale to pick hyperparameters
Let’s say you have a speciﬁc range for a hyperparameter from “a” to “b”. It’s be er to search for the right ones
using the logarithmic scale rather then in linear scale:
Calculate: a_log = log(a) # e.g. a = 0.0001 then a_log = -4
Calculate: b_log = log(b) # e.g. b = 1 then b_log = 0
Then:
1
2
3

r = (a_log - b_log) * np.random.rand() + b_log
# In the example the range would be from [-4, 0] because rand range [0,1)
result = 10^r

It uniformly samples values in log scale from [a,b].
If we want to use the last method on exploring on the “momentum beta”:
Beta best range is from 0.9 to 0.999.
You should search for 1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999) and the
use a = 0.001 and b = 0.1 . Then:
1
2
3
4

a_log = -3
b_log = -1
r = (a_log - b_log) * np.random.rand() + b_log
beta = 1 - 10^r
# because 1 - beta = 10^r

Hyperparameters tuning in prac ce: Pandas vs. Caviar
Intui ons about hyperparameter se ngs from one applica on area may or may not transfer to a diﬀerent one.
If you don’t have much computa onal resources you can use the “babysi ng model”:
Day 0 you might ini alize your parameter as random and then start training.
Then you watch your learning curve gradually decrease over the day.
And each day you nudge your parameters a li le during training.
Called panda approach.
If you have enough computa onal resources, you can run some models in parallel and at the end of the day(s) you
check the results.
Called Caviar approach.

Normalizing ac va ons in a network
In the rise of deep learning, one of the most important ideas has been an algorithm called batch normaliza on,
created by two researchers, Sergey Ioﬀe and Chris an Szegedy.
Batch Normaliza on speeds up learning.
Before we normalized input by subtrac ng the mean and dividing by variance. This helped a lot for the shape of
the cost func on and for reaching the minimum point faster.

The ques on is: for any hidden layer can we normalize A[l] to train W[l] , b[l] faster? This is what batch
normaliza on is about.
There are some debates in the deep learning literature about whether you should normalize values before the
ac va on func on Z[l] or a er applying the ac va on func on A[l] . In prac ce, normalizing Z[l] is
done much more o en and that is what Andrew Ng presents.
Algorithm:
Given Z[l] = [z(1), ..., z(m)] , i = 1 to m (for each input)
Compute mean = 1/m * sum(z[i])
Compute variance = 1/m * sum((z[i] - mean)^2)
Then Z_norm[i] = (z(i) - mean) / np.sqrt(variance + epsilon) (add epsilon for
numerical stability if variance = 0)
Forcing the inputs to a distribu on with zero mean and variance of 1.
Then Z_tilde[i] = gamma * Z_norm[i] + beta
To make inputs belong to other distribu on (with other mean and variance).
gamma and beta are learnable parameters of the model.
Making the NN learn the distribu on of the outputs.

Note: if gamma = sqrt(variance + epsilon) and beta = mean then Z_tilde[i] =
Z_norm[i]

Fi ng Batch Normaliza on into a neural network
Using batch norm in 3 hidden layers NN:

Our NN parameters will be:
W[1] , b[1] , …, W[L] , b[L] , beta[1] , gamma[1] , …, beta[L] , gamma[L]
beta[1] , gamma[1] , …, beta[L] , gamma[L] are updated using any op miza on algorithms (like
GD, RMSprop, Adam)
If you are using a deep learning framework, you won’t have to implement batch norm yourself:
Ex. in Tensorﬂow you can add this line: tf.nn.batch-normalization()
Batch normaliza on is usually applied with mini‑batches.
If we are using batch normaliza on parameters b[1] , …, b[L] doesn’t count because they will be eliminated
a er mean subtrac on step, so:
1
2
3

Z[l] = W[l]A[l-1] + b[l] => Z[l] = W[l]A[l-1]
Z_norm[l] = ...
Z_tilde[l] = gamma[l] * Z_norm[l] + beta[l]

Taking the mean of a constant b[l] will eliminate the b[l]
So if you are using batch normaliza on, you can remove b[l] or make it always zero.
So the parameters will be W[l] , beta[l] , and alpha[l] .
Shapes:
Z[l] - (n[l], m)
beta[l] - (n[l], m)
gamma[l] - (n[l], m)

Why does Batch normaliza on work?
The ﬁrst reason is the same reason as why we normalize X.

The second reason is that batch normaliza on reduces the problem of input values changing (shi ing).
Batch normaliza on does some regulariza on:
Each mini batch is scaled by the mean/variance computed of that mini‑batch.
This adds some noise to the values Z[l] within that mini batch. So similar to dropout it adds some noise to
each hidden layer’s ac va ons.
This has a slight regulariza on eﬀect.
Using bigger size of the mini‑batch you are reducing noise and therefore regulariza on eﬀect.
Don’t rely on batch normaliza on as a regulariza on. It’s intended for normaliza on of hidden units,
ac va ons and therefore speeding up learning. For regulariza on use other regulariza on techniques (L2 or
dropout).

Batch normaliza on at test me
When we train a NN with Batch normaliza on, we compute the mean and the variance of the mini‑batch.
In tes ng we might need to process examples one at a me. The mean and the variance of one example won’t
make sense.
We have to compute an es mated value of mean and variance to use it in tes ng me.
We can use the weighted average across the mini‑batches.
We will use the es mated values of the mean and variance to test.
This method is also some mes called “Running average”.
In prac ce most o en you will use a deep learning framework and it will contain some default implementa on of
doing such a thing.

So max Regression
In every example we have used so far we were talking about binary classiﬁca on.
There are a generaliza on of logis c regression called So max regression that is used for mul class
classiﬁca on/regression.
For example if we are classifying by classes dog , cat , baby chick and none of that
Dog class = 1
Cat class = 2
Baby chick class = 3
None class = 0
To represent a dog vector y = [0 1 0 0]
To represent a cat vector y = [0 0 1 0]
To represent a baby chick vector y = [0 0 0 1]
To represent a none vector y = [1 0 0 0]
Nota ons:
C = no. of classes
Range of classes is (0, ..., C-1)
In output layer Ny = C
Each of C values in the output layer will contain a probability of the example to belong to each of the classes.
In the last layer we will have to ac vate the So max ac va on func on instead of the sigmoid ac va on.
So max ac va on equa ons:
1
2

t = e^(Z[L])
A[L] = e^(Z[L]) / sum(t)

Training a So max classiﬁer

# shape(C, m)
# shape(C, m), sum(t) - sum of t's for each

There’s an ac va on which is called hard max, which gets 1 for the maximum value and zeros for the others.
If you are using NumPy, its np.max over the ver cal axis.
The So max name came from so ening the values and not harding them like hard max.
So max is a generaliza on of logis c ac va on func on to C classes. If C = 2 so max reduces to logis c
regression.
The loss func on used with so max:
1

L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1

The cost func on used with so max:
1

J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m

Back propaga on with so max:
1

dZ[L] = Y_hat - Y

The deriva ve of so max is:
1

Y_hat * (1 - Y_hat)

Example:

Deep learning frameworks
It’s not prac cal to implement everything from scratch. Our numpy implementa ons were to know how NN
works.
There are many good deep learning frameworks.
Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on
going.
Here are some of the leading deep learning frameworks:
Caﬀe/ Caﬀe2
CNTK
DL4j
Keras
Lasagne

mxnet
PaddlePaddle
TensorFlow
Theano
Torch/Pytorch
These frameworks are ge ng be er month by month. Comparison between them can be found here.
How to choose deep learning framework:
Ease of programming (development and deployment)
Running speed
Truly open (open source with good governance)
Programming frameworks can not only shorten your coding me but some mes also perform op miza ons that
speed up your code.

TensorFlow
In this sec on we will learn the basic structure of TensorFlow programs.
Lets see how to implement a minimiza on func on:
Example func on: J(w) = w^2 - 10w + 25
The result should be w = 5 as the func on is (w-5)^2 = 0
Code v.1:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

import numpy as np
import tensorflow as tf

w = tf.Variable(0, dtype=tf.float32)
# creating a variable
cost = tf.add(tf.add(w**2, tf.multiply(-10.0, w)), 25.0)
# can be w
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
session.run(w)
# Runs the definition of w, if you print this it will pr
session.run(train)
print("W after one iteration:", session.run(w))
for i in range(1000):
session.run(train)
print("W after 1000 iterations:", session.run(w))

Code v.2 (we feed the inputs to the algorithm through coeﬃcients):
1
2
3
4
5
6
7
8
9
10

import numpy as np
import tensorflow as tf

coefficients = np.array([[1.], [-10.], [25.]])
x = tf.placeholder(tf.float32, [3, 1])
w = tf.Variable(0, dtype=tf.float32)
cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]

# Creating a variable

11
12
13
14
15
16
17
18
19
20
21
22
23
24

train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
session.run(w)
# Runs the definition of w, if you print this it will pr
session.run(train, feed_dict={x: coefficients})
print("W after one iteration:", session.run(w))
for i in range(1000):
session.run(train, feed_dict={x: coefficients})
print("W after 1000 iterations:", session.run(w))

In TensorFlow you implement only the forward propaga on and TensorFlow will do the backpropaga on by itself.
In TensorFlow a placeholder is a variable you can assign a value to later.
If you are using a mini‑batch training you should change the feed_dict={x: coefficients} to the current
mini‑batch data.
Almost all TensorFlow programs use this:
1
2
3

with tf.Session() as session:
session.run(init)
session.run(w)

# better for cleaning up in case of error

In deep learning frameworks there are a lot of things that you can do with one line of code like changing the
op mizer.

Side notes:
Wri ng and running programs in TensorFlow has the following steps:
1. Create Tensors (variables) that are not yet executed/evaluated.
2. Write opera ons between those Tensors.
3. Ini alize your Tensors.
4. Create a Session.
5. Run the Session. This will run the opera ons you’d wri en above.
Instead of needing to write code to compute the cost func on we know, we can use this line in TensorFlow :
tf.nn.sigmoid_cross_entropy_with_logits(logits = ..., labels = ...)
To ini alize weights in NN using TensorFlow use:
1
2
3

W1 = tf.get_variable("W1", [25,12288], initializer = tf.contrib.layers.xavier
b1 = tf.get_variable("b1", [25,1], initializer = tf.zeros_initializer())

For 3‑layer NN, it is important to note that the forward propaga on stops at Z3 . The reason is that in
TensorFlow the last linear layer output is given as input to the func on compu ng the loss. Therefore, you don’t
need A3 !
To reset the graph use tf.reset_default_graph()

Extra Notes
If you want a good papers in deep learning look at the ICLR proceedings (Or NIPS proceedings) and that will give
you a really good view of the ﬁeld.

Who is Yuanqing Lin?
Head of Baidu research.
First one to win ImageNet
Works in PaddlePaddle deep learning pla orm.

Part‑3: Structuring Machine Learning
Projects
This is the third course of the deep learning specializa on at Coursera which is moderated by DeepLearning.ai. The
course is taught by Andrew Ng.

Andrew NG Course Notes Collec on
Part‑1 Neural Networks and Deep Learning
Part 2 : Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Part‑3: Structuring Machine Learning Projects
Part‑4 :Convolu onal Neural Networks
Part‑5 : Sequence Models

Table of contents
Structuring Machine Learning Projects
Table of contents
Course summary
ML Strategy 1
Why ML Strategy
Orthogonaliza on
Single number evalua on metric
Sa sfying and Op mizing metric
Train/dev/test distribu ons
Size of the dev and test sets
When to change dev/test sets and metrics
Why human‑level performance?
Avoidable bias
Understanding human‑level performance
Surpassing human‑level performance
Improving your model performance
ML Strategy 2
Carrying out error analysis
Cleaning up incorrectly labeled data
Build your ﬁrst system quickly, then iterate
Training and tes ng on diﬀerent distribu ons
Bias and Variance with mismatched data distribu ons
Addressing data mismatch
Transfer learning
Mul ‑task learning

What is end‑to‑end deep learning?
Whether to use end‑to‑end deep learning

Course summary
Here are the course summary as its given on the course link:
You will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI,
and know how to set direc on for your team’s work, this course will show you how.
Much of this content has never been taught elsewhere, and is drawn from my experience building and
shipping many deep learning products. This course also has two “ﬂight simulators” that let you prac ce
decision‑making as a machine learning project leader. This provides “industry experience” that you might
otherwise get only a er years of ML work experience.
A er 2 weeks, you will:
Understand how to diagnose errors in a machine learning system, and
Be able to priori ze the most promising direc ons for reducing error
Understand complex ML se ngs, such as mismatched training/test sets, and comparing to and/or
surpassing human‑level performance
Know how to apply end‑to‑end learning, transfer learning, and mul ‑task learning
I’ve seen teams waste months or years through not understanding the principles taught in this course. I hope
this two week course will save you months of me.
This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This
is the third course in the Deep Learning Specializa on.

ML Strategy 1
Why ML Strategy
You have a lot of ideas for how to improve the accuracy of your deep learning system:
Collect more data.
Collect more diverse training set.
Train algorithm longer with gradient descent.
Try diﬀerent op miza on algorithm (e.g. Adam).
Try bigger network.
Try smaller network.
Try dropout.
Add L2 regulariza on.
Change network architecture (ac va on func ons, # of hidden units, etc.)
This course will give you some strategies to help analyze your problem to go in a direc on that will help you get
be er results.

Orthogonaliza on
Some deep learning developers know exactly what hyperparameter to tune in order to try to achieve one eﬀect.
This is a process we call orthogonaliza on.

In orthogonaliza on, you have some controls, but each control does a speciﬁc task and doesn’t aﬀect other
controls.
For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that
four things hold true ‑ chain of assump ons in machine learning:
1. You’ll have to ﬁt training set well on cost func on (near human level performance if possible).
If it’s not achieved you could try bigger network, another op miza on algorithm (like Adam)…
2. Fit dev set well on cost func on.
If its not achieved you could try regulariza on, bigger training set…
3. Fit test set well on cost func on.
If its not achieved you could try bigger dev. set…
4. Performs well in real world.
If its not achieved you could try change dev. set, change cost func on…

Single number evalua on metric
Its be er and faster to set a single number evalua on metric for your project before you start it.
Diﬀerence between precision and recall (in cat classiﬁca on example):
Suppose we run the classiﬁer on 10 h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/3‑ Structuring Machine Learning Projects/Images/ which are 5 cats and 5 non‑cats. The
classiﬁer iden ﬁes that there are 4 cats, but it iden ﬁed 1 wrong cat.
Confusion matrix:
Predicted cat

Predicted non‑cat

Actual cat

3

2

Actual non‑cat

1

4

Precision: percentage of true cats in the recognized result: P = 3/(3 + 1)
Recall: percentage of true recogni on cat of the all cat predic ons: R = 3/(3 + 2)
Accuracy: (3+4)/10
Using a precision/recall for evalua on is good in a lot of cases, but separately they don’t tell you which algothims
is be er. Ex:
Classiﬁer

Precision

Recall

A

95%

90%

B

98%

85%

A be er thing is to combine precision and recall in one single (real) number evalua on metric. There a metric
called F1 score, which combines them
You can think of F1 score as an average of precision and recall
F1 = 2 / ((1/P) + (1/R))

Sa sfying and Op mizing metric
Its hard some mes to get a single number evalua on metric. Ex:
Classiﬁer

F1

Running me

A

90%

80 ms

B

92%

95 ms

Classiﬁer

F1

Running me

C

92%

1,500 ms

So we can solve that by choosing a single op mizing metric and decide that other metrics are sa sfying. Ex:
1
2

Maximize F1
# optimizing metric
subject to running time < 100ms # satisficing metric

So as a general rule:
1
2

Maximize 1
# optimizing metric (one optimizing metric)
subject to N-1 # satisficing metric (N-1 satisficing metrics)

Train/dev/test distribu ons
Dev and test sets have to come from the same distribu on.
Choose dev set and test set to reﬂect data you expect to get in the future and consider important to do well on.
Se ng up the dev set, as well as the valida on metric is really deﬁning what target you want to aim at.

Size of the dev and test sets
An old way of spli ng the data was 70% training, 30% test or 60% training, 20% dev, 20% test.
The old way was valid for a number of examples ~ <100000
In the modern deep learning if you have a million or more examples a reasonable split would be 98% training, 1%
dev, 1% test.

When to change dev/test sets and metrics
Let’s take an example. In a cat classiﬁca on example we have these metric results:
Metric

Classiﬁca on error
3% error (But a lot of porn h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑

Algorithm

Summary/master/3‑ Structuring Machine Learning Projects/Images/ are treated as cat

A

h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/3‑
Structuring Machine Learning Projects/Images/ here)

Algorithm

5% error

B

In the last example if we choose the best algorithm by metric it would be “A”, but if the users decide it will be
“B”
Thus in this case, we want and need to change our metric.
OldMetric = (1/m) * sum(y_pred[i] != y[i] ,m)
Where m is the number of Dev set items.
NewMetric = (1/sum(w[i])) * sum(w[i] * (y_pred[i] != y[i]) ,m)
where:
w[i] = 1 if x[i] is not porn
w[i] = 10 if x[i] is porn
This is actually an example of an orthogonaliza on where you should take a machine learning problem and break it
into dis nct steps:
1. Figure out how to deﬁne a metric that captures what you want to do ‑ place the target.

2. Worry about how to actually do well on this metric ‑ how to aim/shoot accurately at the target.
Conclusion: if doing well on your metric + dev/test set doesn’t correspond to doing well in your applica on,
change your metric and/or dev/test set.

Why human‑level performance?
We compare to human‑level performance because of two main reasons:
1. Because of advances in deep learning, machine learning algorithms are suddenly working much be er and so
it has become much more feasible in a lot of applica on areas for machine learning algorithms to actually
become compe

ve with human‑level performance.

2. It turns out that the workﬂow of designing and building a machine learning system is much more eﬃcient
when you’re trying to do something that humans can also do.
A er an algorithm reaches the human level performance the progress and accuracy slow down.

You won’t surpass an error that’s called “Bayes op mal error”.
There isn’t much error range between human‑level error and Bayes op mal error.
Humans are quite good at a lot of tasks. So as long as Machine learning is worse than humans, you can:
Get labeled data from humans.
Gain insight from manual error analysis: why did a person get it right?
Be er analysis of bias/variance.

Avoidable bias
Suppose that the cat classiﬁca on algorithm gives these results:
Humans

1%

7.5%

Training error

8%

8%

Dev Error

10%

10%

In the le example, because the human level error is 1% then we have to focus on the bias.
In the right example, because the human level error is 7.5% then we have to focus on the variance.
The human‑level error as a proxy (es mate) for Bayes op mal error. Bayes op mal error is always less (be er),
but human‑level in most cases is not far from it.

You can’t do be er then Bayes error unless you are overﬁ ng.
Avoidable bias = Training error - Human (Bayes) error
Variance = Dev error - Training error

Understanding human‑level performance
When choosing human‑level performance, it has to be chosen in the terms of what you want to achieve with the
system.
You might have mul ple human‑level performances based on the human experience. Then you choose the human‑
level performance (proxy for Bayes error) that is more suitable for the system you’re trying to build.
Improving deep learning algorithms is harder once you reach a human‑level performance.
Summary of bias/variance with human‑level performance:
1. human‑level error (proxy for Bayes error)
Calculate avoidable bias = training error - human-level error
If avoidable bias diﬀerence is the bigger, then it’s bias problem and you should use a strategy for bias
resolving.
2. training error
Calculate variance = dev error - training error
If variance diﬀerence is bigger, then you should use a strategy for variance resolving.
3. Dev error
So having an es mate of human‑level performance gives you an es mate of Bayes error. And this allows you to
more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the
variance of your algorithm.
These techniques will tend to work well un l you surpass human‑level performance, whereupon you might no
longer have a good es mate of Bayes error that s ll helps you make this decision really clearly.

Surpassing human‑level performance
In some problems, deep learning has surpassed human‑level performance. Like:
Online adver sing.
Product recommenda on.
Loan approval.
The last examples are not natural percep on task, rather learning on structural data. Humans are far be er in
natural percep on tasks like computer vision and speech recogni on.
It’s harder for machines to surpass human‑level performance in natural percep on task. But there are already
some systems that achieved it.

Improving your model performance
The two fundamental asssump ons of supervised learning:
1. You can ﬁt the training set pre y well. This is roughly saying that you can achieve low avoidable bias.
2. The training set performance generalizes pre y well to the dev/test set. This is roughly saying that variance is
not too bad.
To improve your deep learning supervised system follow these guidelines:
1. Look at the diﬀerence between human level error and the training error ‑ avoidable bias.
2. Look at the diﬀerence between the dev/test set and training set error ‑ Variance.
3. If avoidable bias is large you have these op ons:
Train bigger model.
Train longer/be er op miza on algorithm (like Momentum, RMSprop, Adam).
Find be er NN architecture/hyperparameters search.

4. If variance is large you have these op ons:
Get more training data.
Regulariza on (L2, Dropout, data augumenta on).
Find be er NN architecture/hyperparameters search.

ML Strategy 2
Carrying out error analysis
Error analysis ‑ process of manually examining mistakes that your algorithm is making. It can give you insights into
what to do next. E.g.:
In the cat classiﬁca on example, if you have 10% error on your dev set and you want to decrease the error.
You discovered that some of the mislabeled data are dog pictures that look like cats. Should you try to make
your cat classiﬁer do be er on dogs (this could take some weeks)?
Error analysis approach:
Get 100 mislabeled dev set examples at random.
Count up how many are dogs.
if 5 of 100 are dogs then training your classiﬁer to do be er on dogs will decrease your error up to 9.5%
(called ceiling), which can be too li le.
if 50 of 100 are dogs then you could decrease your error up to 5%, which is reasonable and you should
work on that.
Based on the last example, error analysis helps you to analyze the error before taking an ac on that could take lot
of me with no need.
Some mes, you can evaluate mul ple error analysis ideas in parallel and choose the best idea. Create a
spreadsheet to do that and decide, e.g.:
Image

Dog

1

✓

2

✓

Great Cats

blurry

✓

Instagram ﬁlters

Comments

✓

Pitbul

✓

3

Rainy day at zoo

✓

4
…
% totals

8%

43%

61%

12%

In the last example you will decide to work on great cats or blurry
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/3‑ Structuring Machine
Learning Projects/Images/ to improve your performance.
This quick coun ng procedure, which you can o en do in, at most, small numbers of hours can really help you
make much be er priori za on decisions, and understand how promising diﬀerent approaches are to work on.

Cleaning up incorrectly labeled data
DL algorithms are quite robust to random errors in the training set but less robust to systema c errors. But it’s OK
to go and ﬁx these labels if you can.
If you want to check for mislabeled data in dev/test set, you should also try error analysis with the mislabeled
column. Ex:

Image

Dog

1

✓

2

✓

Great Cats

blurry

Mislabeled

Comments

✓

3

✓

4
…
% totals

8%

43%

61%

6%

Then:
If overall dev set error: 10%
Then errors due to incorrect data: 0.6%
Then errors due to other causes: 9.4%
Then you should focus on the 9.4% error rather than the incorrect data.
Consider these guidelines while correc ng the dev/test mislabeled examples:
Apply the same process to your dev and test sets to make sure they con nue to come from the same
distribu on.
Consider examining examples your algorithm got right as well as ones it got wrong. (Not always done if you
reached a good accuracy)
Train and (dev/test) data may now come from a slightly diﬀerent distribu ons.
It’s very important to have dev and test sets to come from the same distribu on. But it could be OK for a
train set to come from slighly other distribu on.

Build your ﬁrst system quickly, then iterate
The steps you take to make your deep learning project:
Setup dev/test set and metric
Build ini al system quickly
Use Bias/Variance analysis & Error analysis to priori ze next steps.

Training and tes ng on diﬀerent distribu ons
A lot of teams are working with deep learning applica ons that have training sets that are diﬀerent from the
dev/test sets due to the hunger of deep learning to data.
There are some strategies to follow up when training set distribu on diﬀers from dev/test sets distribu on.
Op on one (not recommended): shuﬄe all the data together and extract randomly training and dev/test sets.
Advantages: all the sets now come from the same distribu on.
Disadvantages: the other (real world) distribu on that was in the dev/test sets will occur less in the new
dev/test sets and that might be not what you want to achieve.
Op on two: take some of the dev/test set examples and add them to the training set.
Advantages: the distribu on you care about is your target now.
Disadvantage: the distribu ons in training and dev/test sets are now diﬀerent. But you will get a be er
performance over a long me.

Bias and Variance with mismatched data distribu ons
Bias and Variance analysis changes when training and Dev/test set is from the diﬀerent distribu on.

Example: the cat classiﬁca on example. Suppose you’ve worked in the example and reached this
Human error: 0%
Train error: 1%
Dev error: 10%
In this example, you’ll think that this is a variance problem, but because the distribu ons aren’t the same you
can’t tell for sure. Because it could be that train set was easy to train on, but the dev set was more diﬃcult.
To solve this issue we create a new set called train‑dev set as a random subset of the training set (so it has the
same distribu on) and we get:
Human error: 0%
Train error: 1%
Train‑dev error: 9%
Dev error: 10%
Now we are sure that this is a high variance problem.
Suppose we have a diﬀerent situa on:
Human error: 0%
Train error: 1%
Train‑dev error: 1.5%
Dev error: 10%
In this case we have something called Data mismatch problem.
Conclusions:
1. Human‑level error (proxy for Bayes error)
2. Train error
Calculate avoidable bias = training error - human level error
If the diﬀerence is big then its Avoidable bias problem then you should use a strategy for high bias.
3. Train‑dev error
Calculate variance = training-dev error - training error
If the diﬀerence is big then its high variance problem then you should use a strategy for solving it.
4. Dev error
Calculate data mismatch = dev error - train-dev error
If diﬀerence is much bigger then train‑dev error its Data mismatch problem.
5. Test error
Calculate degree of overfitting to dev set = test error - dev error
Is the diﬀerence is big (posi ve) then maybe you need to ﬁnd a bigger dev set (dev set and test set come
from the same distribu on, so the only way for there to be a huge gap here, for it to do much be er on
the dev set than the test set, is if you somehow managed to overﬁt the dev set).
Unfortunately, there aren’t many systema c ways to deal with data mismatch. There are some things to try about
this in the next sec on.

Addressing data mismatch
There aren’t completely systema c solu ons to this, but there some things you could try.
1. Carry out manual error analysis to try to understand the diﬀerence between training and dev/test sets.
2. Make training data more similar, or collect more data similar to dev/test sets.
If your goal is to make the training data more similar to your dev set one of the techniques you can use Ar ﬁcial
data synthesis that can help you make more training data.
Combine some of your training data with something that can convert it to the dev/test set distribu on.
Examples:
1. Combine normal audio with car noise to get audio with car noise example.

2. Generate cars using 3D graphics in a car classiﬁca on example.
Be cau ous and bear in mind whether or not you might be accidentally simula ng data only from a ny
subset of the space of all possible examples because your NN might overﬁt these generated data (like
par cular car noise or a par cular design of 3D graphics cars).

Transfer learning
Apply the knowledge you took in a task A and apply it in another task B.
For example, you have trained a cat classiﬁer with a lot of data, you can use the part of the trained NN it to solve
x‑ray classiﬁca on problem.
To do transfer learning, delete the last layer of NN and it’s weights and:
1. Op on 1: if you have a small data set ‑ keep all the other weights as a ﬁxed weights. Add a new last layer(‑s)
and ini alize the new layer weights and feed the new data to the NN and learn the new weights.
2. Op on 2: if you have enough data you can retrain all the weights.
Op on 1 and 2 are called ﬁne‑tuning and training on task A called pretraining.
When transfer learning make sense:
Task A and B have the same input X (e.g. image, audio).
You have a lot of data for the task A you are transferring from and rela vely less data for the task B your
transferring to.
Low level features from task A could be helpful for learning task B.

Mul ‑task learning
Whereas in transfer learning, you have a sequen al process where you learn from task A and then transfer that to
task B. In mul ‑task learning, you start oﬀ simultaneously, trying to have one neural network do several things at
the same me. And then each of these tasks helps hopefully all of the other tasks.
Example:
You want to build an object recogni on system that detects pedestrians, cars, stop signs, and traﬃc lights
(image has mul ple labels).
Then Y shape will be (4,m) because we have 4 classes and each one is a binary one.
Then
Cost = (1/m) * sum(sum(L(y_hat(i)_j, y(i)_j))), i = 1..m, j = 1..4 , where
L = - y(i)_j * log(y_hat(i)_j) - (1 - y(i)_j) * log(1 - y_hat(i)_j)
In the last example you could have trained 4 neural networks separately but if some of the earlier features in
neural network can be shared between these diﬀerent types of objects, then you ﬁnd that training one neural
network to do four things results in be er performance than training 4 completely separate neural networks to do
the four tasks separately.
Mul ‑task learning will also work if y isn’t complete for some labels. For example:
1
2
3

Y = [1 ? 1 ...]
[0 0 1 ...]
[? 1 ? ...]

And in this case it will do good with the missing data, just the loss func on will be diﬀerent:
Loss = (1/m) * sum(sum(L(y_hat(i)_j, y(i)_j) for all j which y(i)_j != ?))
Mul ‑task learning makes sense:
1. Training on a set of tasks that could beneﬁt from having shared lower‑level features.
2. Usually, amount of data you have for each task is quite similar.
3. Can train a big enough network to do well on all the tasks.
If you can train a big enough NN, the performance of the mul ‑task learning compared to spli ng the tasks is
be er.

Today transfer learning is used more o en than mul ‑task learning.

What is end‑to‑end deep learning?
Some systems have mul ple stages to implement. An end‑to‑end deep learning system implements all these
stages with a single NN.
Example 1:
Speech recogni on system:
1
2

Audio ---> Features --> Phonemes --> Words --> Transcript
Audio ---------------------------------------> Transcript

# non-end-to# end-to-end

End‑to‑end deep learning gives data more freedom, it might not use phonemes when training!
To build the end‑to‑end deep learning system that works well, we need a big dataset (more data then in non end‑
to‑end system). If we have a small dataset the ordinary implementa on could work just ﬁne.
Example 2:
Face recogni on system:
1
2

Image ---------------------> Face recognition
Image --> Face detection --> Face recognition

# end-to-end deep learnin
# deep learning system -

In prac ce, the best approach is the second one for now.
In the second implementa on, it’s a two steps approach where both parts are implemented using deep
learning.
Its working well because it’s harder to get a lot of pictures with people in front of the camera than ge ng
faces of people and compare them.
In the second implementa on at the last step, the NN takes two faces as an input and outputs if the two
faces are the same person or not.
Example 3:
Machine transla on system:
1
2

English --> Text analysis --> ... --> French
English ----------------------------> French

# non-end-to-end system
# end-to-end deep learning

Here end‑to‑end deep leaning system works be er because we have enough data to build it.
Example 4:
Es ma ng child’s age from the x‑ray picture of a hand:
1
2

Image --> Bones --> Age
Image ------------> Age

# non-end-to-end system - best approach for now
# end-to-end system

In this example non‑end‑to‑end system works be er because we don’t have enough data to train end‑to‑end
system.

Whether to use end‑to‑end deep learning
Pros of end‑to‑end deep learning:
Let the data speak. By having a pure machine learning approach, your NN learning input from X to Y may be
more able to capture whatever sta s cs are in the data, rather than being forced to reﬂect human
preconcep ons.
Less hand‑designing of components needed.

Cons of end‑to‑end deep learning:
May need a large amount of data.
Excludes poten ally useful hand‑design components (it helps more on the smaller dataset).
Applying end‑to‑end deep learning:
Key ques on: Do you have suﬃcient data to learn a func on of the complexity needed to map x to y?
Use ML/DL to learn some individual components.
When applying supervised learning you should carefully choose what types of X to Y mappings you want to
learn depending on what task you can get data for.

Part‑4 :Convolu onal Neural Networks
This is the fourth course of the deep learning specializa on at Coursera which is moderated by DeepLearning.ai. The
course is taught by Andrew Ng.

Andrew NG Course Notes Collec on
Part‑1 Neural Networks and Deep Learning
Part 2 : Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Part‑3: Structuring Machine Learning Projects
Part‑4 :Convolu onal Neural Networks
Part‑5 : Sequence Models

Table of contents
Convolu onal Neural Networks
Table of contents
Course summary
Founda ons of CNNs
Computer vision
Edge detec on example
Padding
Strided convolu on
Convolu ons over volumes
One Layer of a Convolu onal Network
A simple convolu on network example
Pooling layers
Convolu onal neural network example
Why convolu ons?
Deep convolu onal models: case studies
Why look at case studies?
Classic networks
Residual Networks (ResNets)
Why ResNets work
Network in Network and 1×1 convolu ons
Incep on network mo va on
Incep on network (GoogleNet)
Using Open‑Source Implementa on

Transfer Learning
Data Augmenta on
State of Computer Vision
Object detec on
Object Localiza on
Landmark Detec on
Object Detec on
Convolu onal Implementa on of Sliding Windows
Bounding Box Predic ons
Intersec on Over Union
Non‑max Suppression
Anchor Boxes
YOLO Algorithm
Region Proposals (R‑CNN)
Special applica ons: Face recogni on & Neural style transfer
Face Recogni on
What is face recogni on?
One Shot Learning
Siamese Network
Triplet Loss
Face Veriﬁca on and Binary Classiﬁca on
Neural Style Transfer
What is neural style transfer?
What are deep ConvNets learning?
Cost Func on
Content Cost Func on
Style Cost Func on
1D and 3D Generaliza ons
Extras
Keras

Course summary
Here is the course summary as given on the course link:
This course will teach you how to build convolu onal neural networks and apply it to image data. Thanks to
deep learning, computer vision is working far be er than just two years ago, and this is enabling numerous
exci ng applica ons ranging from safe autonomous driving, to accurate face recogni on, to automa c reading
of radiology h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑
Convolu onal Neural Networks/Images/.
You will:
Understand how to build a convolu onal neural network, including recent varia ons such as residual
networks.
Know how to apply convolu onal networks to visual detec on and recogni on tasks.
Know to use neural style transfer to generate art.
Be able to apply these algorithms to a variety of image, video, and other 2D or 3D data.
This is the fourth course of the Deep Learning Specializa on.

Founda ons of CNNs
Learn to implement the founda onal layers of CNNs (pooling, convolu ons) and to stack them properly in a
deep network to solve mul ‑class image classiﬁca on problems.

Computer vision
Computer vision is one of the applica ons that are rapidly ac ve thanks to deep learning.
Some of the applica ons of computer vision that are using deep learning includes:
Self driving cars.
Face recogni on.
Deep learning is also enabling new types of art to be created.
Rapid changes to computer vision are making new applica ons that weren’t possible a few years ago.
Computer vision deep leaning techniques are always evolving making a new architectures which can help us in
other areas other than computer vision.
For example, Andrew Ng took some ideas of computer vision and applied it in speech recogni on.
Examples of a computer vision problems includes:
Image classiﬁca on.
Object detec on.
Detect object and localize them.
Neural style transfer
Changes the style of an image using another image.
One of the challenges of computer vision problem that
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ can be so large and we want a fast and accurate algorithm to work with that.
For example, a 1000x1000 image will represent 3 million feature/input to the full connected neural
network. If the following hidden layer contains 1000, then we will want to learn weights of the shape
[1000, 3 million] which is 3 billion parameter only in the ﬁrst layer and thats so computa onally
expensive!
One of the solu ons is to build this using convolu on layers instead of the fully connected layers.

Edge detec on example
The convolu on opera on is one of the fundamentals blocks of a CNN. One of the examples about convolu on is
the image edge detec on opera on.
Early layers of CNN might detect edges then the middle layers will detect parts of objects and the later layers will
put the these parts together to produce an output.
In an image we can detect ver cal edges, horizontal edges, or full edge detector.
Ver cal edge detec on:
An example of convolu on opera on to detect ver cal edges:

In the last example a 6x6 matrix convolved with 3x3 ﬁlter/kernel gives us a 4x4 matrix.
If you make the convolu on opera on in TensorFlow you will ﬁnd the func on tf.nn.conv2d . In keras
you will ﬁnd Conv2d func on.
The ver cal edge detec on ﬁlter will ﬁnd a 3x3 place in an image where there are a bright region followed
by a dark region.
If we applied this ﬁlter to a white region followed by a dark region, it should ﬁnd the edges in between the
two colors as a posi ve value. But if we applied the same ﬁlter to a dark region followed by a white region it
will give us nega ve values. To solve this we can use the abs func on to make it posi ve.
Horizontal edge detec on
Filter would be like this
1
2
3

1
0
-1

1
0
-1

1
0
-1

There are a lot of ways we can put number inside the horizontal or ver cal edge detec ons. For example here are
the ver cal Sobel ﬁlter (The idea is taking care of the middle row):
1
2
3

1
2
1

0
0
0

-1
-2
-1

Also something called Scharr ﬁlter (The idea is taking great care of the middle row):
1
2
3

3
10
3

0
0
0

-3
-10
-3

What we learned in the deep learning is that we don’t need to hand cra these numbers, we can treat them as
weights and then learn them. It can learn horizontal, ver cal, angled, or any edge type automa cally rather than
ge ng them by hand.

Padding
In order to to use deep neural networks we really need to use paddings.
In the last sec on we saw that a 6x6 matrix convolved with 3x3 ﬁlter/kernel gives us a 4x4 matrix.

To give it a general rule, if a matrix nxn is convolved with fxf ﬁlter/kernel give us n-f+1,n-f+1 matrix.
The convolu on opera on shrinks the matrix if f>1.
We want to apply convolu on opera on mul ple mes, but if the image shrinks we will lose a lot of data on this
process. Also the edges pixels are used less than other pixels in an image.
So the problems with convolu ons are:
Shrinks output.
throwing away a lot of informa on that are in the edges.
To solve these problems we can pad the input image before convolu on by adding some rows and columns to it.
We will call the padding amount P the number of row/columns that we will insert in top, bo om, le and right
of the image.
In almost all the cases the padding values are zeros.
The general rule now, if a matrix nxn is convolved with fxf ﬁlter/kernel and padding p give us n+2pf+1,n+2p-f+1 matrix.
If n = 6, f = 3, and p = 1 Then the output image will have n+2p-f+1 = 6+2-3+1 = 6 . We maintain the size of
the image.
Same convolu ons is a convolu on with a pad so that output size is the same as the input size. Its given by the
equa on:
1

P = (f-1) / 2

In computer vision f is usually odd. Some of the reasons is that its have a center value.

Strided convolu on
Strided convolu on is another piece that are used in CNNs.
We will call stride S .
When we are making the convolu on opera on we used S to tell us the number of pixels we will jump when we
are convolving ﬁlter/kernel. The last examples we described S was 1.
Now the general rule are:
if a matrix nxn is convolved with fxf ﬁlter/kernel and padding p and stride s it give us (n+2p-f)/s +
1,(n+2p-f)/s + 1 matrix.
In case (n+2p-f)/s + 1 is frac on we can take ﬂoor of this value.
In math textbooks the conv opera on is ﬁlpping the ﬁlter before using it. What we were doing is called cross‑
correla on opera on but the state of art of deep learning is using this as conv opera on.
Same convolu ons is a convolu on with a padding so that output size is the same as the input size. Its given by
the equa on:
1
2

p = (n*s - n + f - s) / 2
When s = 1 ==> P = (f-1) / 2

Convolu ons over volumes
We see how convolu on works with 2D h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/4‑ Convolu onal Neural Networks/Images/, now lets see if we want to convolve 3D
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ (RGB image)
We will convolve an image of height, width, # of channels with a ﬁlter of a height, width, same # of channels. Hint
that the image number channels and the ﬁlter number of channels are the same.
We can call this as stacked ﬁlters for each channel!
Example:
Input image: 6x6x3

Filter: 3x3x3
Result image: 4x4x1
In the last result p=0, s=1
Hint the output here is only 2D.
We can use mul ple ﬁlters to detect mul ple features or edges. Example.
Input image: 6x6x3
10 Filters: 3x3x3
Result image: 4x4x10
In the last result p=0, s=1

One Layer of a Convolu onal Network
First we convolve some ﬁlters to a given input and then add a bias to each ﬁlter output and then get RELU of the
result. Example:
Input image: 6x6x3
10 Filters: 3x3x3

# a0
#W1

Result image: 4x4x10

#W1a0

Add b (bias) with 10x1 will get us : 4x4x10 image #W1a0 + b
Apply RELU will get us: 4x4x10 image #A1 = RELU(W1a0 + b)
In the last result p=0, s=1
Hint number of parameters here are: (3x3x3x10) + 10 = 280
The last example forms a layer in the CNN.
Hint: no ma er the size of the input, the number of the parameters is same if ﬁlter size is same. That makes it less
prone to overﬁ ng.
Here are some nota ons we will use. If layer l is a conv layer:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Hyperparameters
f[l] = filter size
p[l] = padding # Default is zero
s[l] = stride
nc[l] = number of filters
Input: n[l-1] x n[l-1] x nc[l-1]
Or
nH[l-1] x nW[l-1] x nc[l-1]
Output: n[l] x n[l] x nc[l]
Or
nH[l] x nW[l] x nc[l]
Where n[l] = (n[l-1] + 2p[l] - f[l] / s[l]) + 1
Each filter is: f[l] x f[l] x nc[l-1]
Activations: a[l] is nH[l] x nW[l] x nc[l]
A[l] is m x nH[l] x nW[l] x nc[l]
Weights: f[l] * f[l] * nc[l-1] * nc[l]
bias: (1, 1, 1, nc[l])

A simple convolu on network example
Lets build a big example.
Input Image are: a0 = 39x39x3
n0 = 39 and nc0 = 3
First layer (Conv layer):
f1 = 3 , s1 = 1 , and p1 = 0
number of filters = 10

# In batch or minbat

Then output are a1 = 37x37x10
n1 = 37 and nc1 = 10
Second layer (Conv layer):
f2 = 5 , s2 = 2 , p2 = 0
number of filters = 20
The output are a2 = 17x17x20
n2 = 17 , nc2 = 20
Hint shrinking goes much faster because the stride is 2
Third layer (Conv layer):
f3 = 5 , s3 = 2 , p3 = 0
number of filters = 40
The output are a3 = 7x7x40
n3 = 7 , nc3 = 40
Forth layer (Fully connected So max)
a3 = 7x7x40 = 1960 as a vector…
In the last example you seen that the image are ge ng smaller a er each layer and thats the trend now.
Types of layer in a convolu onal network:
Convolu on. #Conv
Pooling #Pool
Fully connected #FC

Pooling layers
Other than the conv layers, CNNs o en uses pooling layers to reduce the size of the inputs, speed up
computa on, and to make some of the features it detects more robust.
Max pooling example:

This example has f = 2 , s = 2 , and p = 0 hyperparameters
The max pooling is saying, if the feature is detected anywhere in this ﬁlter then keep a high number. But the main
reason why people are using pooling because its works well in prac ce and reduce computa ons.
Max pooling has no parameters to learn.
Example of Max pooling on 3D input:
Input: 4x4x10
Max pooling size = 2 and stride = 2
Output: 2x2x10

Average pooling is taking the averages of the values instead of taking the max values.
Max pooling is used more o en than average pooling in prac ce.
If stride of pooling equals the size, it will then apply the eﬀect of shrinking.
Hyperparameters summary
f : ﬁlter size.
s : stride.
Padding are rarely uses here.
Max or average pooling.

Convolu onal neural network example
Now we will deal with a full CNN example. This example is something like the LeNet‑5 that was invented by Yann
Lecun.
Input Image are: a0 = 32x32x3
n0 = 32 and nc0 = 3
First layer (Conv layer): #Conv1
f1 = 5 , s1 = 1 , and p1 = 0
number of filters = 6
Then output are a1 = 28x28x6
n1 = 28 and nc1 = 6
Then apply (Max pooling): #Pool1
f1p = 2 , and s1p = 2
The output are a1 = 14x14x6
Second layer (Conv layer): #Conv2
f2 = 5 , s2 = 1 , p2 = 0
number of filters = 16
The output are a2 = 10x10x16
n2 = 10 , nc2 = 16
Then apply (Max pooling): #Pool2
f2p = 2 , and s2p = 2
The output are a2 = 5x5x16
Third layer (Fully connected) #FC3
Number of neurons are 120
The output a3 = 120 x 1 . 400 came from 5x5x16
Forth layer (Fully connected) #FC4
Number of neurons are 84
The output a4 = 84 x 1 .
Fi h layer (So max)
Number of neurons is 10 if we need to iden fy for example the 10 digits.
Hint a Conv1 and Pool1 is treated as one layer.
Some sta s cs about the last example:

Hyperparameters are a lot. For choosing the value of each you should follow the guideline that we will discuss
later or check the literature and takes some ideas and numbers from it.
Usually the input size decreases over layers while the number of ﬁlters increases.
A CNN usually consists of one or more convolu on (Not just one as the shown examples) followed by a pooling.
Fully connected layers has the most parameters in the network.
To consider using these blocks together you should look at other working examples ﬁrsts to get some intui ons.

Why convolu ons?
Two main advantages of Convs are:
Parameter sharing.
A feature detector (such as a ver cal edge detector) that’s useful in one part of the image is probably
useful in another part of the image.
sparsity of connec ons.
In each layer, each output value depends only on a small number of inputs which makes it transla on
invariance.
Pu ng it all together:

Deep convolu onal models: case studies
Learn about the prac cal tricks and methods used in deep CNNs straight from the research papers.

Why look at case studies?
We learned about Conv layer, pooling layer, and fully connected layers. It turns out that computer vision
researchers spent the past few years on how to put these layers together.
To get some intui ons you have to see the examples that has been made.
Some neural networks architecture that works well in some tasks can also work well in other tasks.
Here are some classical CNN networks:
LeNet‑5
AlexNet
VGG
The best CNN architecture that won the last ImageNet compe

on is called ResNet and it has 152 layers!

There are also an architecture called Incep on that was made by Google that are very useful to learn and apply to
your tasks.
Reading and trying the men oned models can boost you and give you a lot of ideas to solve your task.

Classic networks
In this sec on we will talk about classic networks which are LeNet‑5, AlexNet, and VGG.
LeNet‑5
The goal for this model was to iden fy handwri en digits in a 32x32x1 gray image. Here are the drawing
of it:

This model was published in 1998. The last layer wasn’t using so max back then.
It has 60k parameters.
The dimensions of the image decreases as the number of channels increases.
Conv ==> Pool ==> Conv ==> Pool ==> FC ==> FC ==> softmax this type of arrangement is
quite common.
The ac va on func on used in the paper was Sigmoid and Tanh. Modern implementa on uses RELU in most
of the cases.
LeCun et al., 1998. Gradient‑based learning applied to document recogni on

(h p://ieeexplore.ieee.org/document/726791/?reload=true)
AlexNet
Named a er Alex Krizhevsky who was the ﬁrst author of this paper. The other authors includes Geoﬀrey
Hinton.
The goal for the model was the ImageNet challenge which classiﬁes
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ into 1000 classes. Here are the drawing of the model:

Summary:
1

Conv => Max-pool => Conv => Max-pool => Conv => Conv => Conv => Max-poo

Similar to LeNet‑5 but bigger.
Has 60 Million parameter compared to 60k parameter of LeNet‑5.
It used the RELU ac va on func on.
The original paper contains Mul ple GPUs and Local Response normaliza on (RN).
Mul ple GPUs were used because the GPUs were not so fast back then.
Researchers proved that Local Response normaliza on doesn’t help much so for now don’t bother
yourself for understanding or implemen ng it.
This paper convinced the computer vision researchers that deep learning is so important.
Krizhevsky et al., 2012. ImageNet classiﬁca on with deep convolu onal neural networks

(h ps://papers.nips.cc/paper/4824‑

imagenet‑classiﬁca on‑with‑deep‑convolu onal‑neural‑networks.pdf)
VGG‑16
A modiﬁca on for AlexNet.
Instead of having a lot of hyperparameters lets have some simpler network.
Focus on having only these blocks:
CONV = 3 X 3 ﬁlter, s = 1, same
MAX‑POOL = 2 X 2 , s = 2
Here are the architecture:

This network is large even by modern standards. It has around 138 million parameters.
Most of the parameters are in the fully connected layers.
It has a total memory of 96MB per image for only forward propaga on!
Most memory are in the earlier layers.
Number of ﬁlters increases from 64 to 128 to 256 to 512. 512 was made twice.

Pooling was the only one who is responsible for shrinking the dimensions.
There are another version called VGG‑19 which is a bigger version. But most people uses the VGG‑16 instead
of the VGG‑19 because it does the same.
VGG paper is a rac ve it tries to make some rules regarding using CNNs.
Simonyan & Zisserman 2015. Very deep convolu onal networks for large‑scale image recogni on

(h ps://arxiv.org/abs/1409.1556)

Residual Networks (ResNets)
Very, very deep NNs are diﬃcult to train because of vanishing and exploding gradients problems.
In this sec on we will learn about skip connec on which makes you take the ac va on from one layer and
suddenly feed it to another layer even much deeper in NN which allows you to train large NNs even with layers
greater than 100.
Residual block
ResNets are built out of some Residual blocks.

They add a shortcut/skip connec on before the second ac va on.
The authors of this block ﬁnd that you can train a deeper NNs using stacking this block.
He et al., 2015. Deep residual networks for image recogni on

(h ps://arxiv.org/abs/1512.03385)

Residual Network
Are a NN that consists of some Residual blocks.

These networks can go deeper without hur ng the performance. In the normal NN ‑ Plain networks ‑ the
theory tell us that if we go deeper we will get a be er solu on to our problem, but because of the vanishing
and exploding gradients problems the performance of the network suﬀers as it goes deeper. Thanks to
Residual Network we can go deeper as we want now.

On the le is the normal NN and on the right are the ResNet. As you can see the performance of ResNet
increases as the network goes deeper.
In some cases going deeper won’t eﬀect the performance and that depends on the problem on your hand.
Some people are trying to train 1000 layer now which isn’t used in prac ce.
[He et al., 2015. Deep residual networks for image recogni on]

Why ResNets work
Lets see some example that illustrates why resNet work.
We have a big NN as the following:
X --> Big NN --> a[l]
Lets add two layers to this network as a residual block:
X --> Big NN --> a[l] --> Layer1 --> Layer2 --> a[l+2]
And a [l] has a direct connec on to a[l+2]
Suppose we are using RELU ac va ons.
Then:
1
2

a[l+2] = g( z[l+2] + a[l] )
= g( W[l+2] a[l+1] + b[l+2] + a[l] )

Then if we are using L2 regulariza on for example, W[l+2] will be zero. Lets say that b[l+2] will be
zero too.
Then a[l+2] = g( a[l] ) = a[l] with no nega ve values.
This show that iden ty func on is easy for a residual block to learn. And that why it can train deeper NNs.
Also that the two layers we added doesn’t hurt the performance of big NN we made.
Hint: dimensions of z[l+2] and a[l] have to be the same in resNets. In case they have diﬀerent dimensions
what we put a matrix parameters (Which can be learned or ﬁxed)
a[l+2] = g( z[l+2] + ws * a[l] ) # The added Ws should make the dimentions
equal
ws also can be a zero padding.
Using a skip‑connec on helps the gradient to backpropagate and thus helps you to train deeper networks
Lets take a look at ResNet on h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/4‑ Convolu onal Neural Networks/Images/.
Here are the architecture of ResNet‑34:

All the 3x3 Conv are same Convs.
Keep it simple in design of the network.
spa al size /2 => # ﬁlters x2
No FC layers, No dropout is used.
Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions
are same or diﬀerent. You are going to implement both of them.
The do ed lines is the case when the dimensions are diﬀerent. To solve then they down‑sample the input by
2 and then pad zeros to match the two dimensions. There’s another trick which is called bo leneck which we
will explore later.
Useful concept (Spectrum of Depth):

Taken from icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
Residual blocks types:
Iden ty block:

Hint the conv is followed by a batch norm BN before RELU . Dimensions here are same.
This skip is over 2 layers. The skip connec on can jump n connec ons where n>2
This drawing represents Keras layers.
The convolu onal block:

The conv can be bo leneck 1 x 1 conv

Network in Network and 1 X 1 convolu ons
A 1 x 1 convolu on ‑ We also call it Network in Network‑ is so useful in many CNN models.
What does a 1 X 1 convolu on do? Isn’t it just mul plying by a number?
Lets ﬁrst consider an example:
Input: 6x6x1
Conv: 1x1x1 one ﬁlter. # The 1 x 1 Conv
Output: 6x6x1
Another example:
Input: 6x6x32
Conv: 1x1x32 5 ﬁlters. # The 1 x 1 Conv
Output: 6x6x5
The Network in Network is proposed in [Lin et al., 2013. Network in network]
It has been used in a lot of modern CNN implementa ons like ResNet and Incep on models.
A 1 x 1 convolu on is useful when:
We want to shrink the number of channels. We also call this feature transforma on.
In the second discussed example above we have shrinked the input from 32 to 5 channels.
We will later see that by shrinking it we can save a lot of computa ons.
If we have speciﬁed the number of 1 x 1 Conv ﬁlters to be the same as the input number of channels then the
output will contain the same number of channels. Then the 1 x 1 Conv will act like a non linearity and will
learn non linearity operator.
Replace fully connected layers with 1 x 1 convolu ons as Yann LeCun believes they are the same.
In Convolu onal Nets, there is no such thing as “fully‑connected layers”. There are only convolu on
layers with 1x1 convolu on kernels and a full connec on table. Yann LeCun
Lin et al., 2013. Network in network

(h ps://arxiv.org/abs/1312.4400)

Incep on network mo va on
When you design a CNN you have to decide all the layers yourself. Will you pick a 3 x 3 Conv or 5 x 5 Conv or
maybe a max pooling layer. You have so many choices.
What incep on tells us is, Why not use all of them at once?
Incep on module, naive version:

Hint that max‑pool are same here.
Input to the incep on module are 28 x 28 x 192 and the output are 28 x 28 x 256
We have done all the Convs and pools we might want and will let the NN learn and decide which it want to
use most.
Szegedy et al. 2014. Going deeper with convolu ons

(h ps://arxiv.org/abs/1409.4842)

The problem of computa onal cost in Incep on model:
If we have just focused on a 5 x 5 Conv that we have done in the last example.
There are 32 same ﬁlters of 5 x 5, and the input are 28 x 28 x 192.
Output should be 28 x 28 x 32
The total number of mul plica ons needed here are:
Number of outputs * Filter size * Filter size * Input dimensions
Which equals: 28 * 28 * 32 * 5 * 5 * 192 = 120 Mil
120 Mil mul ply opera on s ll a problem in the modern day computers.
Using a 1 x 1 convolu on we can reduce 120 mil to just 12 mil. Lets see how.
Using 1 X 1 convolu on to reduce computa onal cost:
The new architecture are:
X0 shape is (28, 28, 192)
We then apply 16 (1 x 1 Convolu on)
That produces X1 of shape (28, 28, 16)
Hint, we have reduced the dimensions here.
Then apply 32 (5 x 5 Convolu on)
That produces X2 of shape (28, 28, 32)
Now lets calculate the number of mul plica ons:
For the ﬁrst Conv: 28 * 28 * 16 * 1 * 1 * 192 = 2.5 Mil
For the second Conv: 28 * 28 * 32 * 5 * 5 * 16 = 10 Mil
So the total number are 12.5 Mil approx. which is so good compared to 120 Mil
A 1 x 1 Conv here is called Bo leneck BN .
It turns out that the 1 x 1 Conv won’t hurt the performance.
Incep on module, dimensions reduc on version:

Example of incep on model in Keras:

Incep on network (GoogleNet)
The incep on network consist of concatenated blocks of the Incep on module.
The name incep on was taken from a meme image which was taken from Incep on movie
Here are the full model:

Some mes a Max‑Pool block is used before the incep on module to reduce the dimensions of the inputs.
There are a 3 Sofmax branches at diﬀerent posi ons to push the network toward its goal. and helps to ensure that
the intermediate features are good enough to the network to learn and it turns out that so max0 and sofmax1
gives regulariza on eﬀect.
Since the development of the Incep on module, the authors and the others have built another versions of this
network. Like incep on v2, v3, and v4. Also there is a network that has used the incep on module and the ResNet
together.
Szegedy et al., 2014, Going Deeper with Convolu ons

(h ps://arxiv.org/abs/1409.4842)

Using Open‑Source Implementa on
We have learned a lot of NNs and ConvNets architectures.
It turns out that a lot of these NN are diﬃcult to replicated. because there are some details that may not
presented on its papers. There are some other reasons like:
Learning decay.
Parameter tuning.
A lot of deep learning researchers are opening sourcing their code into Internet on sites like Github.
If you see a research paper and you want to build over it, the ﬁrst thing you should do is to look for an open
source implementa on for this paper.
Some advantage of doing this is that you might download the network implementa on along with its
parameters/weights. The author might have used mul ple GPUs and spent some weeks to reach this result and its
right in front of you a er you download it.

Transfer Learning

If you are using a speciﬁc NN architecture that has been trained before, you can use this pretrained
parameters/weights instead of random ini aliza on to solve your problem.
It can help you boost the performance of the NN.
The pretrained models might have trained on a large datasets like ImageNet, Ms COCO, or pascal and took a lot of
me to learn those parameters/weights with op mized hyperparameters. This can save you a lot of me.
Lets see an example:
Lets say you have a cat classiﬁca on problem which contains 3 classes Tigger, Misty and neither.
You don’t have much a lot of data to train a NN on these
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/.
Andrew recommends to go online and download a good NN with its weights, remove the so max ac va on
layer and put your own one and make the network learn only the new layer while other layer weights are
ﬁxed/frozen.
Frameworks have op ons to make the parameters frozen in some layers using trainable = 0 or
freeze = 0
One of the tricks that can speed up your training, is to run the pretrained NN without ﬁnal so max layer and
get an intermediate representa on of your
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ and save them to disk. And then use these representa on to a shallow NN network. This
can save you the me needed to run an image through all the layers.
Its like conver ng your h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/4‑ Convolu onal Neural Networks/Images/ into vectors.
Another example:
What if in the last example you have a lot of pictures for your cats.
One thing you can do is to freeze few layers from the beginning of the pretrained network and learn the other
weights in the network.
Some other idea is to throw away the layers that aren’t frozen and put your own layers there.
Another example:
If you have enough data, you can ﬁne tune all the layers in your pretrained network but don’t random
ini alize the parameters, leave the learned parameters as it is and learn from there.

Data Augmenta on
If data is increased, your deep NN will perform be er. Data augmenta on is one of the techniques that deep
learning uses to increase the performance of deep NN.
The majority of computer vision applica ons needs more data right now.
Some data augmenta on methods that are used for computer vision tasks includes:
Mirroring.
Random cropping.
The issue with this technique is that you might take a wrong crop.
The solu on is to make your crops big enough.
Rota on.
Shearing.
Local warping.
Color shi ing.
For example, we add to R, G, and B some distor ons that will make the image iden ﬁed as the same for
the human but is diﬀerent for the computer.
In prac ce the added value are pulled from some probability distribu on and these shi s are some small.
Makes your algorithm more robust in changing colors in
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal

Neural Networks/Images/.
There are an algorithm which is called PCA color augmenta on that decides the shi s needed
automa cally.
Implemen ng distor ons during training:
You can use a diﬀerent CPU thread to make you a distorted mini batches while you are training your NN.
Data Augmenta on has also some hyperparameters. A good place to start is to ﬁnd an open source data
augmenta on implementa on and then use it or ﬁne tune these hyperparameters.

State of Computer Vision
For a speciﬁc problem we may have a li le data for it or a lots of data.
Speech recogni on problems for example has a big amount of data, while image recogni on has a medium amount
of data and the object detec on has a small amount of data nowadays.
If your problem has a large amount of data, researchers are tend to use:
Simpler algorithms.
Less hand engineering.
If you don’t have that much data people tend to try more hand engineering for the problem “Hacks”. Like choosing
a more complex NN architecture.
Because we haven’t got that much data in a lot of computer vision problems, it relies a lot on hand engineering.
We will see in the next chapter that because the object detec on has less data, a more complex NN architectures
will be presented.
Tips for doing well on benchmarks/winning compe

ons:

Ensembling.
Train several networks independently and average their outputs. Merging down some classiﬁers.
A er you decide the best architecture for your problem, ini alize some of that randomly and train them
independently.
This can give you a push by 2%
But this will slow down your produc on by the number of the ensembles. Also it takes more memory as
it saves all the models in the memory.
People use this in compe

ons but few uses this in a real produc on.

Mul ‑crop at test me.
Run classiﬁer on mul ple versions of test versions and average results.
There is a technique called 10 crops that uses this.
This can give you a be er result in the produc on.
Use open source code
Use architectures of networks published in the literature.
Use open source implementa ons if possible.
Use pretrained models and ﬁne‑tune on your dataset.

Object detec on
Learn how to apply your knowledge of CNNs to one of the toughest but ho est ﬁeld of computer vision:
Object detec on.

Object Localiza on
Object detec on is one of the areas in which deep learning is doing great in the past two years.
What are localiza on and detec on?
Image Classiﬁca on:

Classify an image to a speciﬁc class. The whole image represents one class. We don’t want to know
exactly where are the object. Usually only one object is presented.

Classiﬁca on with localiza on:
Given an image we want to learn the class of the image and where are the class loca on in the image.
We need to detect a class and a rectangle of where that object is. Usually only one object is presented.

Object detec on:
Given an image we want to detect all the object in the image that belong to a speciﬁc classes and give
their loca on. An image can contain more than one object with diﬀerent classes.

Seman c Segmenta on:
We want to Label each pixel in the image with a category label. Seman c Segmenta on Don’t
diﬀeren ate instances, only care about pixels. It detects no objects just pixels.
If there are two objects of the same class is intersected, we won’t be able to separate them.

Instance Segmenta on
This is like the full problem. Rather than we want to predict the bounding box, we want to know which
pixel label but also dis nguish them.

To make image classiﬁca on we use a Conv Net with a So max a ached to the end of it.
To make classiﬁca on with localiza on we use a Conv Net with a so max a ached to the end of it and a four
numbers bx , by , bh , and bw to tell you the loca on of the class in the image. The dataset should contain
this four numbers with the class too.
Deﬁning the target label Y in classiﬁca on with localiza on problem:
1
2
3
4
5
6
7
8
9
10

Y = [
Pc
bx
by
bh
bw
c1
c2
...

#
#
#
#
#
#

Probability of an object
Bounding box
Bounding box
Bounding box
Bounding box
The classes

]

Example (Object is present):
1
2
3
4
5
6
7
8
9
10

Y = [
1
0
0
100
100
0
1
0

# Object is present

]

Example (When object isn’t presented):
1
2
3
4
5
6
7
8
9
10

Y = [
0
?
?
?
?
?
?
?

# Object isn't presented
# ? means we dont care with other value

]

The loss func on for the Y we have created (Example of the square error):

1
2
3
4

L(y',y) = {
(y1'-y1)^2 + (y2'-y2)^2 + ...
(y1'-y1)^2

if y1 = 1

}

In prac ce we use logis c regression for pc , log likely hood loss for classes, and squared error for the
bounding box.

Landmark Detec on
In some of the computer vision problems you will need to output some points. That is called landmark detec on.
For example, if you are working in a face recogni on problem you might want some points on the face like corners
of the eyes, corners of the mouth, and corners of the nose and so on. This can help in a lot of applica on like
detec ng the pose of the face.
Y shape for the face recogni on problem that needs to output 64 landmarks:
1
2
3
4
5
6
7
8

Y = [
THereIsAface
l1x,
l1y,
....,
l64x,
l64y

# Probability of f

]

Another applica on is when you need to get the skeleton of the person using diﬀerent landmarks/points in the
person which helps in some applica ons.
Hint, in your labeled data, if l1x,l1y is the le corner of le eye, all other l1x,l1y of the other examples
has to be the same.

Object Detec on
We will use a Conv net to solve the object detec on problem using a technique called the sliding windows
detec on algorithm.
For example lets say we are working on Car object detec on.
The ﬁrst thing, we will train a Conv net on cropped car
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ and non car h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/4‑ Convolu onal Neural Networks/Images/.

A er we ﬁnish training of this Conv net we will then use it with the sliding windows technique.
Sliding windows detec on algorithm:
1. Decide a rectangle size.
2. Split your image into rectangles of the size you picked. Each region should be covered. You can use some
strides.
3. For each rectangle feed the image into the Conv net and decide if its a car or not.
4. Pick larger/smaller rectangles and repeat the process from 2 to 3.
5. Store the rectangles that contains the cars.
6. If two or more rectangles intersects choose the rectangle with the best accuracy.
Disadvantage of sliding window is the computa on me.
In the era of machine learning before deep learning, people used a hand cra ed linear classiﬁers that classiﬁes the
object and then use the sliding window technique. The linear classier make it a cheap computa on. But in the
deep learning era that is so computa onal expensive due to the complexity of the deep learning model.
To solve this problem, we can implement the sliding windows with a Convolu onal approach.
One other idea is to compress your deep learning model.

Convolu onal Implementa on of Sliding Windows
Turning FC layer into convolu onal layers (predict image class from four classes):

As you can see in the above image, we turned the FC layer into a Conv layer using a convolu on with the
width and height of the ﬁlter is the same as the width and height of the input.
Convolu on implementa on of sliding windows:
First lets consider that the Conv net you trained is like this (No FC all is conv layers):

Say now we have a 16 x 16 x 3 image that we need to apply the sliding windows in. By the normal
implementa on that have been men oned in the sec on before this, we would run this Conv net four mes
each rectangle size will be 16 x 16.
The convolu on implementa on will be as follows:

Simply we have feed the image into the same Conv net we have trained.
The le cell of the result “The blue one” will represent the the ﬁrst sliding window of the normal
implementa on. The other cells will represent the others.
Its more eﬃcient because it now shares the computa ons of the four mes needed.
Another example would be:

This example has a total of 16 sliding windows that shares the computa on together.
Sermanet et al., 2014, OverFeat: Integrated recogni on, localiza on and detec on using convolu onal networks

(h ps://arxiv.org/abs/1312.6229)
The weakness of the algorithm is that the posi on of the rectangle wont be so accurate. Maybe none of the
rectangles is exactly on the object you want to recognize.

In red, the rectangle we want and in blue is the required car rectangle.

Bounding Box Predic ons
A be er algorithm than the one described in the last sec on is the YOLO algorithm.
YOLO stands for you only look once and was developed back in 2015.
Yolo Algorithm:

1. Lets say we have an image of 100 X 100
2. Place a 3 x 3 grid on the image. For more smother results you should use 19 x 19 for the 100 x 100
3. Apply the classiﬁca on and localiza on algorithm we discussed in a previous sec on to each sec on of the
grid. bx and by will represent the center point of the object in each grid and will be rela ve to the box so
the range is between 0 and 1 while bh and bw will represent the height and width of the object which
can be greater than 1.0 but s ll a ﬂoa ng point value.

4. Do everything at once with the convolu on sliding window. If Y shape is 1 x 8 as we discussed before then
the output of the 100 x 100 image should be 3 x 3 x 8 which corresponds to 9 cell results.
5. Merging the results using predicted localiza on mid point.
We have a problem if we have found more than one object in one grid box.
One of the best advantages that makes the YOLO algorithm popular is that it has a great speed and a Conv net
implementa on.
How is YOLO diﬀerent from other Object detectors? YOLO uses a single CNN
network for both classiﬁca on and localizing the object using bounding boxes.
In the next sec ons we will see some ideas that can make the YOLO algorithm be er.

Intersec on Over Union
Intersec on Over Union is a func on used to evaluate the object detec on algorithm.
It computes size of intersec on and divide it by the union. More generally, IoU is a measure of the overlap

between two bounding boxes.
For example:

The red is the labeled output and the purple is the predicted output.
To compute Intersec on Over Union we ﬁrst compute the union area of the two rectangles which is “the ﬁrst
rectangle + second rectangle” Then compute the intersec on area between these two rectangles.
Finally IOU = intersection area / Union area
If IOU >=0.5 then its good. The best answer will be 1.
The higher the IOU the be er is the accuracy.

Non‑max Suppression
One of the problems we have addressed in YOLO is that it can detect an object mul ple mes.
Non‑max Suppression is a way to make sure that YOLO detects the object just once.
For example:

Each car has two or more detec ons with diﬀerent probabili es. This came from some of the grids that thinks
that this is the center point of the object.
Non‑max suppression algorithm:
1. Lets assume that we are targe ng one class as an output class.
2. Y shape should be [Pc, bx, by, bh, hw] Where Pc is the probability if that object occurs.
3. Discard all boxes with Pc < 0.6
4. While there are any remaining boxes:
1. Pick the box with the largest Pc Output that as a predic on.
2. Discard any remaining box with IoU > 0.5 with that box output in the previous step i.e any box with
high overlap(greater than overlap threshold of 0.5).
If there are mul ple classes/object types c you want to detect, you should run the Non‑max suppression c
mes, once for every output class.

Anchor Boxes
In YOLO, a grid only detects one object. What if a grid cell wants to detect mul ple object?

Car and person grid is same here.
In prac ce this happens rarely.

The idea of Anchor boxes helps us solving this issue.
If Y = [Pc, bx, by, bh, bw, c1, c2, c3] Then to use two anchor boxes like this:
Y = [Pc, bx, by, bh, bw, c1, c2, c3, Pc, bx, by, bh, bw, c1, c2, c3] We simply
have repeated the one anchor Y.
The two anchor boxes you choose should be known as a shape:

So Previously, each object in training image is assigned to grid cell that contains that object’s midpoint.
With two anchor boxes, Each object in training image is assigned to grid cell that contains object’s midpoint and
anchor box for the grid cell with highest IoU. You have to check where your object should be based on its
rectangle closest to which anchor box.
Example of data:

Where the car was near the anchor 2 than anchor 1.
You may have two or more anchor boxes but you should know their shapes.
how do you choose the anchor boxes and people used to just choose them by hand. Maybe ﬁve or ten anchor
box shapes that spans a variety of shapes that cover the types of objects you seem to detect frequently.
You may also use a k‑means algorithm on your dataset to specify that.
Anchor boxes allows your algorithm to specialize, means in our case to easily detect wider
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ or taller ones.

YOLO Algorithm
YOLO is a state‑of‑the‑art object detec on model that is fast and accurate
Lets sum up and introduce the whole YOLO algorithm given an example.
Suppose we need to do object detec on for our autonomous driver system.It needs to iden fy three classes:
1. Pedestrian (Walks on ground).
2. Car.
3. Motorcycle.
We decided to choose two anchor boxes, a taller one and a wide one.
Like we said in prac ce they use ﬁve or more anchor boxes hand made or generated using k‑means.
Our labeled Y shape will be [Ny, HeightOfGrid, WidthOfGrid, 16] , where Ny is number of instances
and each row (of size 16) is as follows:
[Pc, bx, by, bh, bw, c1, c2, c3, Pc, bx, by, bh, bw, c1, c2, c3]
Your dataset could be an image with a mul ple labels and a rectangle for each label, we should go to your dataset
and make the shape and values of Y like we agreed.
An example:

We ﬁrst ini alize all of them to zeros and ?, then for each label and rectangle choose its closest grid point
then the shape to ﬁll it and then the best anchor point based on the IOU. so that the shape of Y for one image
should be [HeightOfGrid, WidthOfGrid,16]
Train the labeled h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑
Convolu onal Neural Networks/Images/ on a Conv net. you should receive an output of [HeightOfGrid,
WidthOfGrid,16] for our case.
To make predic ons, run the Conv net on an image and run Non‑max suppression algorithm for each class you
have in our case there are 3 classes.
You could get something like that:

Total number of generated boxes are grid_width * grid_height * no_of_anchors = 3 x 3 x 2
By removing the low probability predic ons you should have:

Then get the best probability followed by the IOU ﬁltering:

YOLO are not good at detec ng smaller object.
YOLO9000 Be er, faster, stronger
Summary:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

_________________________________________________________________________
Layer (type)
Output Shape
Param #
Connec
=========================================================================
input_1 (InputLayer)
(None, 608, 608, 3)
0
_________________________________________________________________________
conv2d_1 (Conv2D)
(None, 608, 608, 32) 864
input_
_________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 608, 608, 32) 128
conv2d
_________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)
(None, 608, 608, 32) 0
batch_normal
_________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)
(None, 304, 304, 32) 0
leaky_
_________________________________________________________________________
conv2d_2 (Conv2D)
(None, 304, 304, 64) 18432
max_po
_________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 304, 304, 64) 256
conv2d
_________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)
(None, 304, 304, 64) 0
batch_normal
_________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)
(None, 152, 152, 64) 0
leaky_
_________________________________________________________________________
conv2d_3 (Conv2D)
(None, 152, 152, 128) 73728
max_po
_________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 152, 152, 128) 512
conv2d
_________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)
(None, 152, 152, 128) 0
batch_normal
_________________________________________________________________________
conv2d_4 (Conv2D)
(None, 152, 152, 64) 8192
leaky_
_________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 152, 152, 64) 256
conv2d
_________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)
(None, 152, 152, 64) 0
batch_normal
_________________________________________________________________________
conv2d_5 (Conv2D)
(None, 152, 152, 128) 73728
leaky_

35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92

_________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 152, 152, 128) 512
conv2d
_________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)
(None, 152, 152, 128) 0
batch_normal
_________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)
(None, 76, 76, 128)
0
leaky_
_________________________________________________________________________
conv2d_6 (Conv2D)
(None, 76, 76, 256)
294912
max_po
_________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 76, 76, 256)
1024
conv2d
_________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)
(None, 76, 76, 256)
0
batch_normal
_________________________________________________________________________
conv2d_7 (Conv2D)
(None, 76, 76, 128)
32768
leaky_
_________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 76, 76, 128)
512
conv2d
_________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)
(None, 76, 76, 128)
0
batch_normal
_________________________________________________________________________
conv2d_8 (Conv2D)
(None, 76, 76, 256)
294912
leaky_
_________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 76, 76, 256)
1024
conv2d
_________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)
(None, 76, 76, 256)
0
batch_normal
_________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)
(None, 38, 38, 256)
0
leaky_
_________________________________________________________________________
conv2d_9 (Conv2D)
(None, 38, 38, 512)
1179648
max_po
_________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 38, 38, 512)
2048
conv2d
_________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)
(None, 38, 38, 512)
0
batch_normal
_________________________________________________________________________
conv2d_10 (Conv2D)
(None, 38, 38, 256)
131072
leaky_
_________________________________________________________________________
batch_normalization_10 (BatchNor (None, 38, 38, 256)
1024
conv2d
_________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)
(None, 38, 38, 256)
0
batch_normali
_________________________________________________________________________
conv2d_11 (Conv2D)
(None, 38, 38, 512)
1179648
leaky_r
_________________________________________________________________________
batch_normalization_11 (BatchNor (None, 38, 38, 512)
2048
conv2d
_________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)
(None, 38, 38, 512)
0
batch_normali
_________________________________________________________________________
conv2d_12 (Conv2D)
(None, 38, 38, 256)
131072
leaky_
_________________________________________________________________________
batch_normalization_12 (BatchNor (None, 38, 38, 256)
1024
conv2d
_________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)
(None, 38, 38, 256)
0
batch_normaliz
_________________________________________________________________________
conv2d_13 (Conv2D)
(None, 38, 38, 512)
1179648
leaky_
_________________________________________________________________________
batch_normalization_13 (BatchNor (None, 38, 38, 512)
2048
conv2d
_________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)
(None, 38, 38, 512)
0
batch_normali
_________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)
(None, 19, 19, 512)
0
leaky_

93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150

_________________________________________________________________________
conv2d_14 (Conv2D)
(None, 19, 19, 1024) 4718592
max_po
_________________________________________________________________________
batch_normalization_14 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
conv2d_15 (Conv2D)
(None, 19, 19, 512)
524288
leaky_
_________________________________________________________________________
batch_normalization_15 (BatchNor (None, 19, 19, 512)
2048
conv2d
_________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)
(None, 19, 19, 512)
0
batch_normali
_________________________________________________________________________
conv2d_16 (Conv2D)
(None, 19, 19, 1024) 4718592
leaky_
_________________________________________________________________________
batch_normalization_16 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
conv2d_17 (Conv2D)
(None, 19, 19, 512)
524288
leaky_
_________________________________________________________________________
batch_normalization_17 (BatchNor (None, 19, 19, 512)
2048
conv2d
_________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)
(None, 19, 19, 512)
0
batch_normali
_________________________________________________________________________
conv2d_18 (Conv2D)
(None, 19, 19, 1024) 4718592
leaky_
_________________________________________________________________________
batch_normalization_18 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
conv2d_19 (Conv2D)
(None, 19, 19, 1024) 9437184
leaky_
_________________________________________________________________________
batch_normalization_19 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________
conv2d_21 (Conv2D)
(None, 38, 38, 64)
32768
leaky_
_________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
batch_normalization_21 (BatchNor (None, 38, 38, 64)
256
conv2d
_________________________________________________________________________
conv2d_20 (Conv2D)
(None, 19, 19, 1024) 9437184
leaky_
_________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)
(None, 38, 38, 64)
0
batch_normali
_________________________________________________________________________
batch_normalization_20 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________
space_to_depth_x2 (Lambda)
(None, 19, 19, 256)
0
leaky_
_________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
concatenate_1 (Concatenate)
(None, 19, 19, 1280) 0
space_to
leaky_r
_________________________________________________________________________
conv2d_22 (Conv2D)
(None, 19, 19, 1024) 11796480
concat
_________________________________________________________________________
batch_normalization_22 (BatchNor (None, 19, 19, 1024) 4096
conv2d
_________________________________________________________________________

151
152
153
154
155
156
157
158

leaky_re_lu_22 (LeakyReLU)
(None, 19, 19, 1024) 0
batch_normali
_________________________________________________________________________
conv2d_23 (Conv2D)
(None, 19, 19, 425)
435625
leaky_
=========================================================================
Total params: 50,983,561
Trainable params: 50,962,889
Non-trainable params: 20,672
_________________________________________________________________________

You can ﬁnd implementa ons for YOLO here:
h ps://github.com/allanzelener/YAD2K
h ps://github.com/thtrieu/darkﬂow
h ps://pjreddie.com/darknet/yolo/

Region Proposals (R‑CNN)
R‑CNN is an algorithm that also makes an object detec on.
Yolo tells that its faster:
Our model has several advantages over classiﬁer‑based systems. It looks at the whole image at test
me so its predic ons are informed by global context in the image. It also makes predic ons with a
single network evalua on unlike systems like R‑CNN which require thousands for a single image.
This makes it extremely fast, more than 1000x faster than R‑CNN and 100x faster than Fast R‑CNN.
See our paper for more details on the full system.
But one of the downsides of YOLO that it process a lot of areas where no objects are present.
R‑CNN stands for regions with Conv Nets.
R‑CNN tries to pick a few windows and run a Conv net (your conﬁdent classiﬁer) on top of them.
The algorithm R‑CNN uses to pick windows is called a segmenta on algorithm. Outputs something like this:

If for example the segmenta on algorithm produces 2000 blob then we should run our classiﬁer/CNN on top of
these blobs.
There has been a lot of work regarding R‑CNN tries to make it faster:
R‑CNN:
Propose regions. Classify proposed regions one at a me. Output label + bounding box.
Downside is that its slow.
Girshik et. al, 2013. Rich feature hierarchies for accurate object detec on and seman c segmenta on

(h ps://arxiv.org/abs/1311.2524)
Fast R‑CNN:
Propose regions. Use convolu on implementa on of sliding windows to classify all the proposed regions.

Girshik, 2015. Fast R‑CNN

(h ps://arxiv.org/abs/1504.08083)

Faster R‑CNN:
Use convolu onal network to propose regions.
Ren et. al, 2016. Faster R‑CNN: Towards real‑ me object detec on with region proposal networks

(h ps://arxiv.org/abs/1506.01497)
Mask R‑CNN:
h ps://arxiv.org/abs/1703.06870
Most of the implementa on of faster R‑CNN are s ll slower than YOLO.
Andew Ng thinks that the idea behind YOLO is be er than R‑CNN because you are able to do all the things in just
one me instead of two mes.
Other algorithms that uses one shot to get the output includes SSD and Mul Box.
Wei Liu, et. al 2015 SSD: Single Shot Mul Box Detector

(h ps://arxiv.org/abs/1512.02325)

R‑FCN is similar to Faster R‑CNN but more eﬃcient.
Jifeng Dai, et. al 2016 R‑FCN: Object Detec on via Region‑based Fully Convolu onal Networks

(h ps://arxiv.org/abs/1605.06409)

Special applica ons: Face recogni on & Neural style transfer
Discover how CNNs can be applied to mul ple ﬁelds, including art genera on and face recogni on.
Implement your own algorithm to generate art and recognize faces!

Face Recogni on
What is face recogni on?
Face recogni on system iden ﬁes a person’s face. It can work on both
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ or videos.
Liveness detec on within a video face recogni on system prevents the network from iden fying a face in an
image. It can be learned by supervised deep learning using a dataset for live human and in‑live human and
sequence learning.
Face veriﬁca on vs. face recogni on:
Veriﬁca on:
Input: image, name/ID. (1 : 1)
Output: whether the input image is that of the claimed person.
“is this the claimed person?”
Recogni on:
Has a database of K persons
Get an input image
Output ID if the image is any of the K persons (or not recognized)
“who is this person?”
We can use a face veriﬁca on system to make a face recogni on system. The accuracy of the veriﬁca on system
has to be high (around 99.9% or more) to be use accurately within a recogni on system because the recogni on
system accuracy will be less than the veriﬁca on system given K persons.

One Shot Learning
One of the face recogni on challenges is to solve one shot learning problem.
One Shot Learning: A recogni on system is able to recognize a person, learning from one image.
Historically deep learning doesn’t work well with a small number of data.

Instead to make this work, we will learn a similarity func on:
d( img1, img2 ) = degree of diﬀerence between
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/.
We want d result to be low in case of the same faces.
We use tau T as a threshold for d:
If d( img1, img2 ) <= T Then the faces are the same.
Similarity func on helps us solving the one shot learning. Also its robust to new inputs.

Siamese Network
We will implement the similarity func on using a type of NNs called Siamease Network in which we can pass
mul ple inputs to the two or more networks with the same architecture and parameters.
Siamese network architecture are as the following:

We make 2 iden cal conv nets which encodes an input image into a vector. In the above image the vector
shape is (128, )
The loss func on will be d(x1, x2) = || f(x1) - f(x2) ||^2
If X1 , X2 are the same person, we want d to be low. If they are diﬀerent persons, we want d to be high.
Taigman et. al., 2014. DeepFace closing the gap to human level performance

(h ps://www.cv‑

founda on.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)

Triplet Loss
Triplet Loss is one of the loss func ons we can use to solve the similarity distance in a Siamese network.
Our learning objec ve in the triplet loss func on is to get the distance between an Anchor image and a posi ve
or a nega ve image.
Posi ve means same person, while nega ve means diﬀerent person.
The triplet name came from that we are comparing an anchor A with a posi ve P and a nega ve N image.
Formally we want:
Posi ve distance to be less than nega ve distance
||f(A) - f(P)||^2 <= ||f(A) - f(N)||^2
Then
||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= 0
To make sure the NN won’t get an output of zeros easily:
||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= -alpha
Alpha is a small number. Some mes its called the margin.

Then
||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + alpha <= 0
Final Loss func on:
Given 3 h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑
Convolu onal Neural Networks/Images/ (A, P, N)
L(A, P, N) = max (||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + alpha , 0)
J = Sum(L(A[i], P[i], N[i]) , i) for all triplets of
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/.
You need mul ple h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑
Convolu onal Neural Networks/Images/ of the same person in your dataset. Then get some triplets out of your
dataset. Dataset should be big enough.
Choosing the triplets A, P, N:
During training if A, P, N are chosen randomly (Subjet to A and P are the same and A and N aren’t the same)
then one of the problems this constrain is easily sa sﬁed
d(A, P) + alpha <= d (A, N)
So the NN wont learn much
What we want to do is choose triplets that are hard to train on.
So for all the triplets we want this to be sa sﬁed:
d(A, P) + alpha <= d (A, N)
This can be achieved by for example same poses!
Find more at the paper.
Details are in this paper

Schroﬀ et al.,2015, FaceNet: A uniﬁed embedding for face recogni on and clustering

(h ps://arxiv.org/abs/1503.03832)
Commercial recogni on systems are trained on a large datasets like 10/100 million
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/.
There are a lot of pretrained models and parameters online for face recogni on.

Face Veriﬁca on and Binary Classiﬁca on
Triplet loss is one way to learn the parameters of a conv net for face recogni on there’s another way to learn
these parameters as a straight binary classiﬁca on problem.
Learning the similarity func on another way:

The ﬁnal layer is a sigmoid layer.
Y' = wi * Sigmoid ( f(x(i)) - f(x(j)) ) + b where the subtrac on is the Manha an
distance between f(x(i)) and f(x(j))
Some other similari es can be Euclidean and Ki square similarity.
The NN here is Siamese means the top and bo om convs has the same parameters.
The paper for this work:

Taigman et. al., 2014. DeepFace closing the gap to human level performance

(h ps://www.cv‑

founda on.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)

A good performance/deployment trick:
Pre‑compute all the h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑
Convolu onal Neural Networks/Images/ that you are using as a comparison to the vector f(x(j))
When a new image that needs to be compared, get its vector f(x(i)) then put it with all the pre computed
vectors and pass it to the sigmoid func on.
This version works quite as well as the triplet loss func on.
Available implementa ons for face recogni on using deep learning includes:
Openface
FaceNet
DeepFace

Neural Style Transfer
What is neural style transfer?
Neural style transfer is one of the applica on of Conv nets.
Neural style transfer takes a content image C and a style image S and generates the content image G with
the style of style image.

In order to implement this you need to look at the features extracted by the Conv net at the shallower and deeper
layers.
It uses a previously trained convolu onal network like VGG, and builds on top of that. The idea of using a network
trained on a diﬀerent task and applying it to a new task is called transfer learning.

What are deep ConvNets learning?
Visualizing what a deep network is learning:
Given this AlexNet like Conv net:

Pick a unit in layer l. Find the nine image patches that maximize the unit’s ac va on.
No ce that a hidden unit in layer one will see rela vely small por on of NN, so if you plo ed it it will
match a small image in the shallower layers while it will get larger image in deeper layers.

Repeat for other units and layers.
It turns out that layer 1 are learning the low level representa ons like colors and edges.
You will ﬁnd out that each layer are learning more complex representa ons.

The ﬁrst layer was created using the weights of the ﬁrst layer. Other
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/ are generated using the recep ve ﬁeld in the image that triggered the neuron to be max.
Zeiler and Fergus., 2013, Visualizing and understanding convolu onal networks

(h ps://arxiv.org/abs/1311.2901)

A good explana on on how to get recep ve ﬁeld given a layer:

From A guide to recep ve ﬁeld arithme c for Convolu onal Neural Networks

Cost Func on
We will deﬁne a cost func on for the generated image that measures how good it is.
Give a content image C, a style image S, and a generated image G:
J(G) = alpha * J(C,G) + beta * J(S,G)
J(C, G) measures how similar is the generated image to the Content image.
J(S, G) measures how similar is the generated image to the Style image.
alpha and beta are rela ve weigh ng to the similarity and these are hyperparameters.

Find the generated image G:
1. Ini ate G randomly
For example G: 100 X 100 X 3
2. Use gradient descent to minimize J(G)
G = G - dG We compute the gradient image and use gradient decent to minimize the cost func on.
The itera ons might be as following image:
To Generate this:

You will go through this:

Content Cost Func on
In the previous sec on we showed that we need a cost func on for the content image and the style image to
measure how similar is them to each other.
Say you use hidden layer l to compute content cost.
If we choose l to be small (like layer 1), we will force the network to get similar output to the original
content image.
In prac ce l is not too shallow and not too deep but in the middle.
Use pre‑trained ConvNet. (E.g., VGG network)
Let a(c)[l] and a(G)[l] be the ac va on of layer l on the
h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑Summary/master/4‑ Convolu onal Neural
Networks/Images/.
If a(c)[l] and a(G)[l] are similar then they will have the same content
J(C, G) at a layer l = 1/2 || a(c)[l] - a(G)[l] ||^2

Style Cost Func on
Meaning of the style of an image:
Say you are using layer l’s ac va on to measure style.
Deﬁne style as correla on between ac va ons across channels.
That means given an ac va on like this:

How correlate is the orange channel with the yellow channel?
Correlated means if a value appeared in a speciﬁc channel a speciﬁc value will appear too (Depends on
each other).
Uncorrelated means if a value appeared in a speciﬁc channel doesn’t mean that another value will appear
(Not depend on each other)
The correla on tells you how a components might occur or not occur together in the same image.
The correla on of style image channels should appear in the generated image channels.
Style matrix (Gram matrix):
Let a(l)[i, j, k] be the ac va on at l with (i=H, j=W, k=C)
Also G(l)(s) is matrix of shape nc(l) x nc(l)
We call this matrix style matrix or Gram matrix.
In this matrix each cell will tell us how correlated is a channel to another channel.
To populate the matrix we use these equa ons to compute style matrix of the style image and the generated
image.

As it appears its the sum of the mul plica on of each member in the matrix.
To compute gram matrix eﬃciently:
Reshape ac va on from H X W X C to HW X C
Name the reshaped ac va on F.
G[l] = F * F.T
Finally the cost func on will be as following:
J(S, G) at layer l = (1/ 2 * H * W * C) || G(l)(s) - G(l)(G) ||
And if you have used it from some layers
J(S, G) = Sum (lamda[l]*J(S, G)[l], for all layers)
Steps to be made if you want to create a tensorﬂow model for neural style transfer:
1. Create an Interac ve Session.
2. Load the content image.
3. Load the style image
4. Randomly ini alize the image to be generated
5. Load the VGG16 model
6. Build the TensorFlow graph:

Run the content image through the VGG16 model and compute the content cost
Run the style image through the VGG16 model and compute the style cost
Compute the total cost
Deﬁne the op mizer and the learning rate
7. Ini alize the TensorFlow graph and run it for a large number of itera ons, upda ng the generated image at
every step.

1D and 3D Generaliza ons
So far we have used the Conv nets for h ps://raw.githubusercontent.com/ashishpatel26/DeepLearning.ai‑
Summary/master/4‑ Convolu onal Neural Networks/Images/ which are 2D.
Conv nets can work with 1D and 3D data as well.
An example of 1D convolu on:
Input shape (14, 1)
Applying 16 ﬁlters with F = 5 , S = 1
Output shape will be 10 X 16
Applying 32 ﬁlters with F = 5, S = 1
Output shape will be 6 X 32
The general equa on (N - F)/S + 1 can be applied here but here it gives a vector rather than a 2D matrix.
1D data comes from a lot of resources such as waves, sounds, heartbeat signals.
In most of the applica ons that uses 1D data we use Recurrent Neural Network RNN.
3D data also are available in some applica ons like CT scan:

Example of 3D convolu on:
Input shape (14, 14,14, 1)
Applying 16 ﬁlters with F = 5 , S = 1
Output shape (10, 10, 10, 16)
Applying 32 ﬁlters with F = 5, S = 1
Output shape will be (6, 6, 6, 32)

Extras
Keras
Keras is a high‑level neural networks API (programming framework), wri en in Python and capable of running on
top of several lower‑level frameworks including TensorFlow, Theano, and CNTK.
Keras was developed to enable deep learning engineers to build and experiment with diﬀerent models very
quickly.
Just as TensorFlow is a higher‑level framework than Python, Keras is an even higher‑level framework and provides
addi onal abstrac ons.
Keras will work ﬁne for many common models.
Layers in Keras:
Dense (Fully connected layers).
A linear func on followed by a non linear func on.
Convolu onal layer.
Pooling layer.
Normalisa on layer.
A batch normaliza on layer.
Fla en layer
Fla en a matrix into vector.
Ac va on layer
Diﬀerent ac va ons include: relu, tanh, sigmoid, and so max.
To train and test a model in Keras there are four steps:
1. Create the model.
2. Compile the model by calling model.compile(optimizer = "...", loss = "...", metrics =
["accuracy"])
3. Train the model on train data by calling model.fit(x = ..., y = ..., epochs = ...,
batch_size = ...)
You can add a valida on set while training too.
4. Test the model on test data by calling model.evaluate(x = ..., y = ...)
Summarize of step in Keras: Create‑>Compile‑>Fit/Train‑>Evaluate/Test
Model.summary() gives a lot of useful informa ons regarding your model including each layers inputs,
outputs, and number of parameters at each layer.
To choose the Keras backend you should go to $HOME/.keras/keras.json and change the ﬁle to the
desired backend like Theano or Tensorﬂow or whatever backend you want.
A er you create the model you can run it in a tensorﬂow session without compiling, training, and tes ng
capabili es.
You can save your model with model_save and load your model using model_load This will save your
whole trained model to disk with the trained weights.

Part‑5 : Sequence Models
This is the ﬁ h and ﬁnal course of the deep learning specializa on at Coursera which is moderated by deeplearning.ai.
The course is taught by Andrew Ng.

Andrew NG Course Notes Collec on

Part‑1 Neural Networks and Deep Learning
Part 2 : Improving Deep Neural Networks: Hyperparameter tuning, Regulariza on and Op miza on
Part‑3: Structuring Machine Learning Projects
Part‑4 :Convolu onal Neural Networks
Part‑5 : Sequence Models

Table of contents
Sequence Models
Table of contents
Course summary
Recurrent Neural Networks
Why sequence models
Nota on
Recurrent Neural Network Model
Backpropaga on through me
Diﬀerent types of RNNs
Language model and sequence genera on
Sampling novel sequences
Vanishing gradients with RNNs
Gated Recurrent Unit (GRU)
Long Short Term Memory (LSTM)
Bidirec onal RNN
Deep RNNs
Back propaga on with RNNs
Natural Language Processing & Word Embeddings
Introduc on to Word Embeddings
Word Representa on
Using word embeddings
Proper es of word embeddings
Embedding matrix
Learning Word Embeddings: Word2vec & GloVe
Learning word embeddings
Word2Vec
Nega ve Sampling
GloVe word vectors
Applica ons using Word Embeddings
Sen ment Classiﬁca on
Debiasing word embeddings
Sequence models & A en on mechanism
Various sequence to sequence architectures
Basic Models
Picking the most likely sentence
Beam Search
Reﬁnements to Beam Search
Error analysis in beam search
BLEU Score
A en on Model Intui on
A en on Model
Speech recogni on ‑ Audio data

Speech recogni on
Trigger Word Detec on
Extras
Machine transla on a en on model (From notebooks)

Course summary
Here are the course summary as its given on the course link:
This course will teach you how to build models for natural language, audio, and other sequence data. Thanks
to deep learning, sequence algorithms are working far be er than just two years ago, and this is enabling
numerous exci ng applica ons in speech recogni on, music synthesis, chatbots, machine transla on, natural
language understanding, and many others.
You will:
Understand how to build and train Recurrent Neural Networks (RNNs), and commonly‑used variants such
as GRUs and LSTMs.
Be able to apply sequence models to natural language problems, including text synthesis.
Be able to apply sequence models to audio applica ons, including speech recogni on and music
synthesis.
This is the ﬁ h and ﬁnal course of the Deep Learning Specializa on.

Recurrent Neural Networks
Learn about recurrent neural networks. This type of model has been proven to perform extremely well on
temporal data. It has several variants including LSTMs, GRUs and Bidirec onal RNNs, which you are going to
learn about in this sec on.

Why sequence models
Sequence Models like RNN and LSTMs have greatly transformed learning on sequences in the past few years.
Examples of sequence data in applica ons:
Speech recogni on (sequence to sequence):
X: wave sequence
Y: text sequence
Music genera on (one to sequence):
X: nothing or an integer
Y: wave sequence
Sen ment classiﬁca on (sequence to one):
X: text sequence
Y: integer ra ng from one to ﬁve
DNA sequence analysis (sequence to sequence):
X: DNA sequence
Y: DNA Labels
Machine transla on (sequence to sequence):
X: text sequence (in one language)
Y: text sequence (in other language)
Video ac vity recogni on (sequence to one):
X: video frames

Y: label (ac vity)
Name en ty recogni on (sequence to sequence):
X: text sequence
Y: label sequence
Can be used by seach engines to index diﬀerent type of words inside a text.
All of these problems with diﬀerent input and output (sequence or not) can be addressed as supervised learning
with label data X, Y as the training set.

Nota on
In this sec on we will discuss the nota ons that we will use through the course.
Mo va ng example:
Named en ty recogni on example:
X: “Harry Po er and Hermoine Granger invented a new spell.”
Y: 1 1 0 1 1 0 0 0 0
Both elements has a shape of 9. 1 means its a name, while 0 means its not a name.
We will index the ﬁrst element of x by x<1>, the second x<2> and so on.
<1>

x

= Harry

x

= Po er

<2>

Similarly, we will index the ﬁrst element of y by y<1>, the second y<2> and so on.
<1>

y

=1

y

=1

<2>

Tx is the size of the input sequence and Ty is the size of the output sequence.
Tx = Ty = 9 in the last example although they can be diﬀerent in other problems.
(i)<t>

x

is the element t of the sequence of input vector i. Similarly y(i)<t> means the t‑th element in the output

sequence of the i training example.
Tx(i) the input sequence length for training example i. It can be diﬀerent across the examples. Similarly for Ty(i) will
be the length of the output sequence in the i‑th training example.
Represen ng words:
We will now work in this course with NLP which stands for natural language processing. One of the
challenges of NLP is how can we represent a word?
1. We need a vocabulary list that contains all the words in our target sets.
Example:
[a … And … Harry … Po er … Zulu]
Each word will have a unique index that it can be represented with.
The sor ng here is in alphabe cal order.
Vocabulary sizes in modern applica ons are from 30,000 to 50,000. 100,000 is not uncommon. Some of
the bigger companies use even a million.
To build vocabulary list, you can read all the texts you have and get m words with the most occurrence,
or search online for m most occurrent words.
2. Create a one‑hot encoding sequence for each word in your dataset given the vocabulary you have created.
While conver ng, what if we meet a word thats not in your dic onary?
We can add a token in the vocabulary with name <UNK> which stands for unknown text and use its
index for your one‑hot vector.

Full example:

The goal is given this representa on for x to learn a mapping using a sequence model to then target output y as a
supervised learning problem.

Recurrent Neural Network Model
Why not to use a standard network for sequence tasks? There are two problems:
Inputs, outputs can be diﬀerent lengths in diﬀerent examples.
This can be solved for normal NNs by paddings with the maximum lengths but it’s not a good solu on.
Doesn’t share features learned across diﬀerent posi ons of text/sequence.
Using a feature sharing like in CNNs can signiﬁcantly reduce the number of parameters in your model.
That’s what we will do in RNNs.
Recurrent neural network doesn’t have either of the two men oned problems.
Lets build a RNN that solves name en ty recogni on task:

In this problem Tx = Ty. In other problems where they aren’t equal, the RNN architecture may be diﬀerent.
<0>

a

is usually ini alized with zeros, but some others may ini alize it randomly in some cases.

There are three weight matrices here: Wax, Waa, and Wya with shapes:
Wax: (NoOfHiddenNeurons, nx)
Waa: (NoOfHiddenNeurons, NoOfHiddenNeurons)
Wya: (ny, NoOfHiddenNeurons)
The weight matrix Waa is the memory the RNN is trying to maintain from the previous layers.

A lot of papers and books write the same architecture this way:

It’s harder to interpreter. It’s easier to roll this drawings to the unrolled version.
In the discussed RNN architecture, the current output ŷ<t> depends on the previous inputs and ac va ons.
Let’s have this example ‘He Said, “Teddy Roosevelt was a great president”’. In this example Teddy is a person name
but we know that from the word president that came a er Teddy not from He and said that were before it.
So limita on of the discussed architecture is that it can not learn from elements later in the sequence. To address
this problem we will later discuss Bidirec onal RNN (BRNN).
Now let’s discuss the forward propaga on equa ons on the discussed architecture:

The ac va on func on of a is usually tanh or ReLU and for y depends on your task choosing some ac va on
func ons like sigmoid and so max. In name en ty recogni on task we will use sigmoid because we only have
two classes.
In order to help us develop complex RNN architectures, the last equa ons needs to be simpliﬁed a bit.

Simpliﬁed RNN nota on:

wa is waa and wax stacked horizontaly.
<t‑1>

[a

<t>

,x

<t‑1>

] is a

<t>

and x

stacked ver caly.

wa shape: (NoOfHiddenNeurons, NoOfHiddenNeurons + nx)
<t‑1>

[a

<t>

,x

] shape: (NoOfHiddenNeurons + nx, 1)

Backpropaga on through me
Let’s see how backpropaga on works with the RNN architecture.
Usually deep learning frameworks do backpropaga on automa cally for you. But it’s useful to know how it works
in RNNs.
Here is the graph:

Where wa, ba, wy, and by are shared across each element in a sequence.
We will use the cross‑entropy loss func on:

Where the ﬁrst equa on is the loss for one example and the loss for the whole sequence is given by the
summa on over all the calculated single example losses.

Graph with losses:

The backpropaga on here is called backpropaga on through me because we pass ac va on a from one
sequence element to another like backwards in me.

Diﬀerent types of RNNs
So far we have seen only one RNN architecture in which Tx equals TY. In some other problems, they may not
equal so we need diﬀerent architectures.
The ideas in this sec on was inspired by Andrej Karpathy blog. Mainly this image has all types:

The architecture we have descried before is called Many to Many.
In sen ment analysis problem, X is a text while Y is an integer that rangers from 1 to 5. The RNN architecture for
that is Many to One as in Andrej Karpathy image.

A One to Many architecture applica on would be music genera on.

Note that star ng the second layer we are feeding the generated output back to the network.
There are another interes ng architecture in Many To Many. Applica ons like machine transla on inputs and
outputs sequences have diﬀerent lengths in most of the cases. So an alterna ve Many To Many architecture that
ﬁts the transla on would be as follows:

There are an encoder and a decoder parts in this architecture. The encoder encodes the input sequence into
one matrix and feed it to the decoder to generate the outputs. Encoder and decoder have diﬀerent weight
matrices.
Summary of RNN types:

There is another architecture which is the a en on architecture which we will talk about in chapter 3.

Language model and sequence genera on
RNNs do very well in language model problems. In this sec on, we will build a language model using RNNs.
What is a language model
Let’s say we are solving a speech recogni on problem and someone says a sentence that can be interpreted
into to two sentences:
The apple and pair salad

The apple and pear salad
Pair and pear sounds exactly the same, so how would a speech recogni on applica on choose from the two.
That’s where the language model comes in. It gives a probability for the two sentences and the applica on
decides the best based on this probability.
The job of a language model is to give a probability of any given sequence of words.
How to build language models with RNNs?
The ﬁrst thing is to get a training set: a large corpus of target language text.
Then tokenize this training set by ge ng the vocabulary and then one‑hot each word.
Put an end of sentence token <EOS> with the vocabulary and include it with each converted sentence.
Also, use the token <UNK> for the unknown words.
Given the sentence "Cats average 15 hours of sleep a day. <EOS> "
In training me we will use this:

The loss func on is deﬁned by cross‑entropy loss:

i is for all elements in the corpus, t ‑ for all mesteps.
To use this model:
1. For predic ng the chance of next word, we feed the sentence to the RNN and then get the ﬁnal y^<t> hot
vector and sort it by maximum probability.
2. For taking the probability of a sentence, we compute this:
p(y<1>, y<2>, y<3>) = p(y<1>) * p(y<2> | y<1>) * p(y<3> | y<1>, y<2>)
This is simply feeding the sentence into the RNN and mul plying the probabili es (outputs).

Sampling novel sequences
A er a sequence model is trained on a language model, to check what the model has learned you can apply it to
sample novel sequence.
Lets see the steps of how we can sample a novel sequence from a trained sequence language model:

1. Given this model:

2. We ﬁrst pass a<0> = zeros vector, and x<1> = zeros vector.
3. Then we choose a predic on randomly from distribu on obtained by ŷ<1>. For example it could be “The”.
In numpy this can be implemented using: numpy.random.choice(...)
This is the line where you get a random beginning of the sentence each me you sample run a novel
sequence.
4. We pass the last predicted word with the calculated a<1>
5. We keep doing 3 & 4 steps for a ﬁxed length or un l we get the <EOS> token.
6. You can reject any <UNK> token if you mind ﬁnding it in your output.
So far we have to build a word‑level language model. It’s also possible to implement a character‑level language
model.
In the character‑level language model, the vocabulary will contain [a-zA-Z0-9] , punctua on, special
characters and possibly token.
Character‑level language model has some pros and cons compared to the word‑level language model
Pros:
1. There will be no <UNK> token ‑ it can create any word.
Cons:
1. The main disadvantage is that you end up with much longer sequences.
2. Character‑level language models are not as good as word‑level language models at capturing long range
dependencies between how the the earlier parts of the sentence also aﬀect the later part of the
sentence.
3. Also more computa onally expensive and harder to train.
The trend Andrew has seen in NLP is that for the most part, a word‑level language model is s ll used, but as
computers get faster there are more and more applica ons where people are, at least in some special cases,
star ng to look at more character‑level models. Also, they are used in specialized applica ons where you might
need to deal with unknown words or other vocabulary words a lot. Or they are also used in more specialized
applica ons where you have a more specialized vocabulary.

Vanishing gradients with RNNs
One of the problems with naive RNNs that they run into vanishing gradient problem.
An RNN that process a sequence data with the size of 10,000 me steps, has 10,000 deep layers which is very
hard to op mize.
Let’s take an example. Suppose we are working with language modeling problem and there are two sequences that
model tries to learn:
“The cat, which already ate …, was full”
“The cats, which already ate …, were full”
Dots represent many words in between.
What we need to learn here that “was” came with “cat” and that “were” came with “cats”. The naive RNN is not
very good at capturing very long‑term dependencies like this.
As we have discussed in Deep neural networks, deeper networks are ge ng into the vanishing gradient problem.
That also happens with RNNs with a long sequence size.

For compu ng the word “was”, we need to compute the gradient for everything behind. Mul plying frac ons
tends to vanish the gradient, while mul plica on of large number tends to explode it.
Therefore some of your weights may not be updated properly.
In the problem we descried it means that its hard for the network to memorize “was” word all over back to “cat”.
So in this case, the network won’t iden fy the singular/plural words so that it gives it the right grammar form of
verb was/were.
The conclusion is that RNNs aren’t good in long‑term dependencies.
In theory, RNNs are absolutely capable of handling such “long‑term dependencies.” A human could
carefully pick parameters for them to solve toy problems of this form. Sadly, in prac ce, RNNs don’t seem
to be able to learn them. h p://colah.github.io/posts/2015‑08‑Understanding‑LSTMs/

Vanishing gradients problem tends to be the bigger problem with RNNs than the exploding gradients problem. We
will discuss how to solve it in next sec ons.
Exploding gradients can be easily seen when your weight values become NaN . So one of the ways solve
exploding gradient is to apply gradient clipping means if your gradient is more than some threshold ‑ re‑scale
some of your gradient vector so that is not too big. So there are cliped according to some maximum value.

Extra:
Solu ons for the Exploding gradient problem:
Truncated backpropaga on.

Not to update all the weights in the way back.
Not op mal. You won’t update all the weights.
Gradient clipping.
Solu on for the Vanishing gradient problem:
Weight ini aliza on.
Like He ini aliza on.
Echo state networks.
Use LSTM/GRU networks.
Most popular.
We will discuss it next.

Gated Recurrent Unit (GRU)
GRU is an RNN type that can help solve the vanishing gradient problem and can remember the long‑term
dependencies.
The basic RNN unit can be visualized to be like this:

We will represent the GRU with a similar drawings.
Each layer in GRUs has a new variable C which is the memory cell. It can tell to whether memorize something or
not.
In GRUs, C<t> = a<t>
Equa ons of the GRUs:

The update gate is between 0 and 1
To understand GRUs imagine that the update gate is either 0 or 1 most of the me.
So we update the memory cell based on the update cell and the previous cell.
Lets take the cat sentence example and apply it to understand this equa ons:
Sentence: “The cat, which already ate …, was full”
We will suppose that U is 0 or 1 and is a bit that tells us if a singular word needs to be memorized.

Spli ng the words and get values of C and U at each place:
‑

Word

Update gate(U)

The

0

val

cat

1

new_val

which

0

new_val

already

0

new_val

…

0

new_val

was

1 (I don’t need it anymore)

newer_val

full

…

…

Drawing for the GRUs

Drawings like in h p://colah.github.io/posts/2015‑08‑Understanding‑LSTMs/ is so popular and makes it
easier to understand GRUs and LSTMs. But Andrew Ng ﬁnds it’s be er to look at the equa ons.
Because the update gate U is usually a small number like 0.00001, GRUs doesn’t suﬀer the vanishing gradient
problem.
In the equa on this makes C<t> = C<t‑1> in a lot of cases.
Shapes:
a<t> shape is (NoOfHiddenNeurons, 1)
<t>

c

<t>

is the same as a

c~<t> is the same as a<t>
u<t> is also the same dimensions of a<t>
The mul plica on in the equa ons are element wise mul plica on.
What has been descried so far is the Simpliﬁed GRU unit. Let’s now describe the full one:
The full GRU contains a new gate that is used with to calculate the candidate C. The gate tells you how
relevant is C<t‑1> to C<t>
Equa ons:

Shapes are the same
So why we use these architectures, why don’t we change them, how we know they will work, why not add
another gate, why not use the simpler GRU instead of the full GRU; well researchers has experimented over years
all the various types of these architectures with many many diﬀerent versions and also addressing the vanishing
gradient problem. They have found that full GRUs are one of the best RNN architectures to be used for many
diﬀerent problems. You can make your design but put in mind that GRUs and LSTMs are standards.

Long Short Term Memory (LSTM)
LSTM ‑ the other type of RNN that can enable you to account for long‑term dependencies. It’s more powerful and
general than GRU.
<t>

In LSTM , C

<t>

!= a

Here are the equa ons of an LSTM unit:

~<t>

In GRU we have an update gate U , a relevance gate r , and a candidate cell variables C

while in LSTM we

have an update gate U (some mes it’s called input gate I), a forget gate F , an output gate O , and a candidate
~<t>

cell variables C

Drawings (inspired by h p://colah.github.io/posts/2015‑08‑Understanding‑LSTMs/):

Some variants on LSTM includes:
LSTM with peephole connec ons.
<t‑1>

The normal LSTM with C

included with every gate.

There isn’t a universal superior between LSTM and it’s variants. One of the advantages of GRU is that it’s simpler
and can be used to build much bigger network but the LSTM is more powerful and general.

Bidirec onal RNN
There are s ll some ideas to let you build much more powerful sequence models. One of them is bidirec onal
RNNs and another is Deep RNNs.
As we saw before, here is an example of the Name en ty recogni on task:

The name Teddy cannot be learned from He and said, but can be learned from bears.
BiRNNs ﬁxes this issue.
Here is BRNNs architecture:

Note, that BiRNN is an acyclic graph.
Part of the forward propaga on goes from le to right, and part ‑ from right to le . It learns from both sides.
<t>

To make predic ons we use ŷ

by using the two ac va ons that come from le and right.

The blocks here can be any RNN block including the basic RNNs, LSTMs, or GRUs.
For a lot of NLP or text processing problems, a BiRNN with LSTM appears to be commonly used.
The disadvantage of BiRNNs that you need the en re sequence before you can process it. For example, in live
speech recogni on if you use BiRNNs you will need to wait for the person who speaks to stop to take the en re
sequence and then make your predic ons.

Deep RNNs
In a lot of cases the standard one layer RNNs will solve your problem. But in some problems its useful to stack
some RNN layers to make a deeper network.

For example, a deep RNN with 3 layers would look like this:

In feed‑forward deep nets, there could be 100 or even 200 layers. In deep RNNs stacking 3 layers is already
considered deep and expensive to train.
In some cases you might see some feed‑forward network layers connected a er recurrent cell.

Back propaga on with RNNs
In modern deep learning frameworks, you only have to implement the forward pass, and the framework
takes care of the backward pass, so most deep learning engineers do not need to bother with the details
of the backward pass. If however you are an expert in calculus and want to see the details of backprop in
RNNs, you can work through this op onal por on of the notebook.
The quote is taken from this notebook. If you want the details of the back propaga on with programming notes
look at the linked notebook.

Natural Language Processing & Word Embeddings
Natural language processing with deep learning is an important combina on. Using word vector
representa ons and embedding layers you can train recurrent neural networks with outstanding
performances in a wide variety of industries. Examples of applica ons are sen ment analysis, named en ty
recogni on and machine transla on.

Introduc on to Word Embeddings
Word Representa on
NLP has been revolu onized by deep learning and especially by RNNs and deep RNNs.
Word embeddings is a way of represen ng words. It lets your algorithm automa cally understand the analogies
between words like “king” and “queen”.
So far we have deﬁned our language by a vocabulary. Then represented our words with a one‑hot vector that
represents the word in the vocabulary.

An image example would be:

We will use the annota on O idx for any word that is represented with one‑hot like in the image.
One of the weaknesses of this representa on is that it treats a word as a thing that itself and it doesn’t allow
an algorithm to generalize across words.
For example: “I want a glass of orange ______”, a model should predict the next word as juice.
A similar example “I want a glass of apple ______”, a model won’t easily predict juice here if it wasn’t
trained on it. And if so the two examples aren’t related although orange and apple are similar.
Inner product between any one‑hot encoding vector is zero. Also, the distances between them are the same.
So, instead of a one‑hot presenta on, won’t it be nice if we can learn a featurized representa on with each of
these words: man, woman, king, queen, apple, and orange?

Each word will have a, for example, 300 features with a type of ﬂoat point number.
Each word column will be a 300‑dimensional vector which will be the representa on.
We will use the nota on e5391 to describe man word features vector.
Now, if we return to the examples we described again:
“I want a glass of orange ______”
I want a glass of apple ______
Orange and apple now share a lot of similar features which makes it easier for an algorithm to generalize
between them.
We call this representa on Word embeddings.
To visualize word embeddings we use a t‑SNE algorithm to reduce the features to 2 dimensions which makes it
easy to visualize:

You will get a sense that more related words are closer to each other.
The word embeddings came from that we need to embed a unique vector inside a n‑dimensional space.

Using word embeddings
Let’s see how we can take the feature representa on we have extracted from each word and apply it in the
Named en ty recogni on problem.
Given this example (from named en ty recogni on):

Sally Johnson is a person’s name.
A er training on this sentence the model should ﬁnd out that the sentence “Robert Lin is an apple farmer”
contains Robert Lin as a name, as apple and orange have near representa ons.
Now if you have tested your model with this sentence “Mahmoud Badry is a durian cul vator” the network
should learn the name even if it hasn’t seen the word durian before (during training). That’s the power of word
representa ons.
The algorithms that are used to learn word embeddings can examine billions of words of unlabeled text ‑ for
example, 100 billion words and learn the representa on from them.
Transfer learning and word embeddings:
1. Learn word embeddings from large text corpus (1‑100 billion of words).
Or download pre‑trained embedding online.
2. Transfer embedding to new task with the smaller training set (say, 100k words).
3. Op onal: con nue to ﬁnetune the word embeddings with new data.
You bother doing this if your smaller training set (from step 2) is big enough.
Word embeddings tend to make the biggest diﬀerence when the task you’re trying to carry out has a rela vely
smaller training set.
Also, one of the advantages of using word embeddings is that it reduces the size of the input!
10,000 one hot compared to 300 features vector.

Word embeddings have an interes ng rela onship to the face recogni on task:

In this problem, we encode each face into a vector and then check how similar are these vectors.
Words encoding and embeddings have a similar meaning here.
In the word embeddings task, we are learning a representa on for each word in our vocabulary (unlike in image
encoding where we have to map each new image to some n‑dimensional vector). We will discuss the algorithm in
next sec ons.

Proper es of word embeddings
One of the most fascina ng proper es of word embeddings is that they can also help with analogy reasoning.
While analogy reasoning may not be by itself the most important NLP applica on, but it might help convey a
sense of what these word embeddings can do.
Analogies example:
Given this word embeddings table:

Can we conclude this rela on:
Man ==> Woman
King ==> ??
Lets subtract eMan from eWoman. This will equal the vector [-2 0 0 0]
Similar eKing ‑ eQueen = [-2 0 0 0]

So the diﬀerence is about the gender in both.

This vector represents the gender.
This drawing is a 2D visualiza on of the 4D vector that has been extracted by a t‑SNE algorithm. It’s a
drawing just for visualiza on. Don’t rely on the t‑SNE algorithm for ﬁnding parallels.
So we can reformulate the problem to ﬁnd:
eMan ‑ eWoman ≈ eKing ‑ e??
It can also be represented mathema cally by:

It turns out that eQueen is the best solu on here that gets the the similar vector.
Cosine similarity ‑ the most commonly used similarity func on:
Equa on:

CosineSimilarity(u, v) =

u.v
= cos(θ)
∣∣u∣∣2 ∣∣v∣∣2

The top part represents the inner product of u and v vectors. It will be large if the vectors are very
similar.
You can also use Euclidean distance as a similarity func on (but it rather measures a dissimilarity, so you should
take it with nega ve sign).
We can use this equa on to calculate the similari es between word embeddings and on the analogy problem
where u = ew and v = eking ‑ eman + ewoman

Embedding matrix
When you implement an algorithm to learn a word embedding, what you end up learning is a embedding matrix.
Let’s take an example:
Suppose we are using 10,000 words as our vocabulary (plus token).

The algorithm should create a matrix E of the shape (300, 10000) in case we are extrac ng 300 features.

If O6257 is the one hot encoding of the word orange of shape (10000, 1), then

np.dot( E ,O6257) = e6257 which shape is (300, 1).
Generally np.dot( E , Oj) = ej
In the next sec ons, you will see that we ﬁrst ini alize E randomly and then try to learn all the parameters of
this matrix.
In prac ce it’s not eﬃcient to use a dot mul plica on when you are trying to extract the embeddings of a speciﬁc
word, instead, we will use slicing to slice a speciﬁc column. In Keras there is an embedding layer that extracts this
column with no mul plica on.

Learning Word Embeddings: Word2vec & GloVe
Learning word embeddings
Let’s start learning some algorithms that can learn word embeddings.
At the start, word embeddings algorithms were complex but then they got simpler and simpler.
We will start by learning the complex examples to make more intui on.
Neural language model:
Let’s start with an example:

We want to build a language model so that we can predict the next word.
So we use this neural network to learn the language model

We get ej by np.dot( E ,o<sub>j</sub>)
NN layer has parameters W1 and b1 while so max layer has parameters W2 and b2
Input dimension is (300*6, 1) if the window size is 6 (six previous words).
Here we are op mizing E matrix and layers parameters. We need to maximize the likelihood to predict
the next word given the context (previous words).

This model was build in 2003 and tends to work pre y decent for learning word embeddings.
In the last example we took a window of 6 words that fall behind the word that we want to predict. There are
other choices when we are trying to learn word embeddings.
Suppose we have an example: “I want a glass of orange juice to go along with my cereal”
To learn juice, choices of context are:
1. Last 4 words.
We use a window of last 4 words (4 is a hyperparameter), “a glass of orange” and try to predict the
next word from it.
2. 4 words on the le and on the right.
“a glass of orange” and “to go along with”
3. Last 1 word.
“orange”
4. Nearby 1 word.
“glass” word is near juice.
This is the idea of skip grams model.
The idea is much simpler and works remarkably well.
We will talk about this in the next sec on.
Researchers found that if you really want to build a language model, it’s natural to use the last few words as a
context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and
they will result in very meaningful work embeddings as well.
To summarize, the language modeling problem poses a machines learning problem where you input the context
(like the last four words) and predict some target words. And posing that problem allows you to learn good word
embeddings.

Word2Vec
Before presen ng Word2Vec, lets talk about skip‑grams:
For example, we have the sentence: “I want a glass of orange juice to go along with my cereal”
We will choose context and target.
The target is chosen randomly based on a window with a speciﬁc size.
Context

Target

How far

orange

juice

+1

orange

glass

‑2

orange

my

+6

We have converted the problem into a supervised problem.
This is not an easy learning problem because learning within ‑10/+10 words (10 ‑ an example) is hard.
We want to learn this to get our word embeddings model.
Word2Vec model:
Vocabulary size = 10,000 words
Let’s say that the context word are c and the target word is t
We want to learn c to t
We get ec by E . oc
We then use a so max layer to get P(t|c) which is ŷ
Also we will use the cross‑entropy loss func on.
This model is called skip‑grams model.

The last model has a problem with the so max layer:

Here we are summing 10,000 numbers which corresponds to the number of words in our vocabulary.
If this number is larger say 1 million, the computa on will become very slow.
One of the solu ons for the last problem is to use “Hierarchical so max classiﬁer” which works as a tree
classiﬁer.

In prac ce, the hierarchical so max classiﬁer doesn’t use a balanced tree like the drawn one. Common words are
at the top and less common are at the bo om.
How to sample the context c?
One way is to choose the context by random from your corpus.
If you have done it that way, there will be frequent words like “the, of, a, and, to, …” that can dominate other
words like “orange, apple, durian,…”
In prac ce, we don’t take the context uniformly random, instead there are some heuris cs to balance the
common words and the non‑common words.
word2vec paper includes 2 ideas of learning word embeddings. One is skip‑gram model and another is CBoW
(con nious bag‑of‑words).

Nega ve Sampling
Nega ve sampling allows you to do something similar to the skip‑gram model, but with a much more eﬃcient
learning algorithm. We will create a diﬀerent learning problem.
Given this example:
“I want a glass of orange juice to go along with my cereal”
The sampling will look like this:
Context

Word

target

orange

juice

1

orange

king

0

orange

book

0

orange

the

0

orange

of

0

We get posi ve example by using the same skip‑grams technique, with a ﬁxed window that goes around.
To generate a nega ve example, we pick a word randomly from the vocabulary.
No ce, that we got word “of” as a nega ve example although it appeared in the same sentence.
So the steps to generate the samples are:

1. Pick a posi ve context
2. Pick a k nega ve contexts from the dic onary.
k is recommended to be from 5 to 20 in small datasets. For larger ones ‑ 2 to 5.
We will have a ra o of k nega ve examples to 1 posi ve ones in the data we are collec ng.
Now let’s deﬁne the model that will learn this supervised learning problem:
Lets say that the context word are c and the word are t and y is the target.
We will apply the simple logis c regression model.

The logis c regression model can be drawn like this:

So we are like having 10,000 binary classiﬁca on problems, and we only train k+1 classiﬁer of them in each
itera on.
How to select nega ve samples:
We can sample according to empirical frequencies in words corpus which means according to how o en
diﬀerent words appears. But the problem with that is that we will have more frequent words like the, of, and…
The best is to sample with this equa on (according to authors):

GloVe word vectors
GloVe is another algorithm for learning the word embedding. It’s the simplest of them.
This is not used as much as word2vec or skip‑gram models, but it has some enthusiasts because of its simplicity.
GloVe stands for Global vectors for word representa on.
Let’s use our previous example: “I want a glass of orange juice to go along with my cereal”.
We will choose a context and a target from the choices we have men oned in the previous sec ons.
Then we will calculate this for every pair: Xct = # mes t appears in context of c
Xct = Xtc if we choose a window pair, but they will not equal if we choose the previous words for example. In
GloVe they use a window which means they are equal

The model is deﬁned like this:

f(x) ‑ the weigh ng term, used for many reasons which include:
The log(0) problem, which might occur if there are no pairs for the given target and context values.
Giving not too much weight for stop words like “is”, “the”, and “this” which occur many mes.
Giving not too li le weight for infrequent words.
Theta and e are symmetric which helps ge ng the ﬁnal word embedding.

Conclusions on word embeddings:
If this is your ﬁrst try, you should try to download a pre‑trained model that has been made and actually works
best.
If you have enough data, you can try to implement one of the available algorithms.
Because word embeddings are very computa onally expensive to train, most ML prac

oners will load a pre‑

trained set of embeddings.
A ﬁnal note that you can’t guarantee that the axis used to represent the features will be well‑aligned with
what might be easily humanly interpretable axis like gender, royal, age.

Applica ons using Word Embeddings
Sen ment Classiﬁca on
As we have discussed before, Sen ment classiﬁca on is the process of ﬁnding if a text has a posi ve or a nega ve
review. Its so useful in NLP and is used in so many applica ons. An example would be:

One of the challenges with it, is that you might not have a huge labeled training data for it, but using word
embeddings can help ge ng rid of this.
The common dataset sizes varies from 10,000 to 100,000 words.

A simple sen ment classiﬁca on model would be like this:

The embedding matrix may have been trained on say 100 billion words.
Number of features in word embedding is 300.
We can use sum or average given all the words then pass it to a so max classiﬁer. That makes this classiﬁer
works for short or long sentences.
One of the problems with this simple model is that it ignores words order. For example “Completely lacking in
good taste, good service, and good ambience” has the word good 3 mes but its a nega ve review.
A be er model uses an RNN for solving this problem:

And so if you train this algorithm, you end up with a pre y decent sen ment classiﬁca on algorithm.
Also, it will generalize be er even if words weren’t in your dataset. For example you have the sentence
“Completely absent of good taste, good service, and good ambience”, then even if the word “absent” is not in
your label training set, if it was in your 1 billion or 100 billion word corpus used to train the word embeddings,
it might s ll get this right and generalize much be er even to words that were in the training set used to train
the word embeddings but not necessarily in the label training set that you had for speciﬁcally the sen ment
classiﬁca on problem.

Debiasing word embeddings
We want to make sure that our word embeddings are free from undesirable forms of bias, such as gender bias,
ethnicity bias and so on.

Horrifying results on the trained word embeddings in the context of Analogies:
Man : Computer_programmer as Woman : Homemaker
Father : Doctor as Mother : Nurse
Word embeddings can reﬂect gender, ethnicity, age, sexual orienta on, and other biases of text used to train the
model.
Learning algorithms by general are making important decisions and it mustn’t be biased.
Andrew thinks we actually have be er ideas for quickly reducing the bias in AI than for quickly reducing the bias
in the human race, although it s ll needs a lot of work to be done.
Addressing bias in word embeddings steps:
Idea from the paper: h ps://arxiv.org/abs/1607.06520
Given these learned embeddings:

We need to solve the gender bias here. The steps we will discuss can help solve any bias problem but we are
focusing here on gender bias.
Here are the steps:
1. Iden fy the direc on:
Calculate the diﬀerence between:
ehe ‑ eshe
emale ‑ efemale
…
Choose some k diﬀerences and average them.
This will help you ﬁnd this:

By that we have found the bias direc on which is 1D vector and the non‑bias vector which is 299D
vector.
2. Neutralize: For every word that is not deﬁni onal, project to get rid of bias.

Babysi er and doctor need to be neutral so we project them on non‑bias axis with the direc on of
the bias:

A er that they will be equal in the term of gender.
To do this the authors of the paper trained a classiﬁer to tell the words that need to be
neutralized or not.
3. Equalize pairs
We want each pair to have diﬀerence only in gender. Like:
Grandfather ‑ Grandmother
He ‑ She
Boy ‑ Girl
We want to do this because the distance between grandfather and babysi er is bigger than
babysi er and grandmother:

To do that, we move grandfather and grandmother to a point where they will be in the middle of the
non‑bias axis.
There are some words you need to do this for in your steps. Number of these words is rela vely
small.

Sequence models & A en on mechanism
Sequence models can be augmented using an a en on mechanism. This algorithm will help your model
understand where it should focus its a en on given a sequence of inputs. This week, you will also learn about
speech recogni on and how to deal with audio data.

Various sequence to sequence architectures
Basic Models
In this sec on we will learn about sequence to sequence ‑ Many to Many ‑ models which are useful in various
applica ons including machine transla on and speech recogni on.
Let’s start with the basic model:
Given this machine transla on problem in which X is a French sequence and Y is an English sequence.

Our architecture will include encoder and decoder.
The encoder is RNN ‑ LSTM or GRU are included ‑ and takes the input sequence and then outputs a vector
that should represent the whole input.
A er that the decoder network, also RNN, takes the sequence built by the encoder and outputs the new
sequence.

These ideas are from the following papers:
Sutskever et al., 2014. Sequence to sequence learning with neural networks
Cho et al., 2014. Learning phrase representa ons using RNN encoder‑decoder for sta s cal machine
transla on
An architecture similar to the men oned above works for image cap oning problem:
In this problem X is an image, while Y is a sentence (cap on).
The model architecture image:

The architecture uses a pretrained CNN (like AlexNet) as an encoder for the image, and the decoder is an
RNN.
Ideas are from the following papers (they share similar ideas):
Maoet et. al., 2014. Deep cap oning with mul modal recurrent neural networks
Vinyals et. al., 2014. Show and tell: Neural image cap on generator

Karpathy and Li, 2015. Deep visual‑seman c alignments for genera ng image descrip ons

Picking the most likely sentence
There are some similari es between the language model we have learned previously, and the machine transla on
model we have just discussed, but there are some diﬀerences as well.
The language model we have learned is very similar to the decoder part of the machined transla on model, except
<0>

for a

Problems formula ons also are diﬀerent:
<1>

In language model: P(y

<Ty>

, …, y
<1>

In machine transla on: P(y

)

<Ty>

, …, y

<1>

|x

<Tx>

, …, x

)

What we don’t want in machine transla on model, is not to sample the output at random. This may provide some
choices as an output. Some mes you may sample a bad output.
Example:
X = “Jane visite l’Afrique en septembre.”
Y may be:
Jane is visi ng Africa in September.
Jane is going to be visi ng Africa in September.
In September, Jane will visit Africa.
So we need to get the best output it can be:

The most common algorithm is the beam search, which we will explain in the next sec on.
Why not use greedy search? Why not get the best choices each me?
It turns out that this approach doesn’t really work!
Lets explain it with an example:
The best output for the example we talked about is “Jane is visi ng Africa in September.”
Suppose that when you are choosing with greedy approach, the ﬁrst two words were “Jane is”, the word
that may come a er that will be “going” as “going” is the most common word that comes a er " is" so the
result may look like this: “Jane is going to be visi ng Africa in September.”. And that isn’t the best/op mal
solu on.
So what is be er than greedy approach, is to get an approximate solu on, that will try to maximize the output (the
last equa on above).

Beam Search

Beam search is the most widely used algorithm to get the best output sequence. It’s a heuris c search algorithm.
To illustrate the algorithm we will s ck with the example from the previous sec on. We need Y = “Jane is visi ng
Africa in September.”
The algorithm has a parameter B which is the beam width. Lets take B = 3 which means the algorithm will
get 3 outputs at a me.
For the ﬁrst step you will get [“in”, “jane”, “september”] words that are the best candidates.
Then for each word in the ﬁrst output, get B next (second) words and select top best B combina ons where the
best are those what give the highest value of mul plying both probabili es ‑ P(y<1>|x) * P(y<2>|x,y<1>). Se we will
have then [“in september”, “jane is”, “jane visit”]. No ce, that we automa cally discard september as a ﬁrst word.
Repeat the same process and get the best B words for [“september”, “is”, “visit”] and so on.
In this algorithm, keep only B instances of your network.
If B = 1 this will become the greedy search.

Reﬁnements to Beam Search
In the previous sec on, we have discussed the basic beam search. In this sec on, we will try to do some
reﬁnements to it.
The ﬁrst thing is Length op miza on
In beam search we are trying to op mize:

And to do that we mul ply:
<1>

P(y

<2>

| x) * P(y

<1>

| x, y

<t>

) * … * P(y

<y(t‑1)>

| x, y

)

Each probability is a frac on, most of the me a small frac on.
Mul plying small frac ons will cause a numerical overﬂow. Meaning that it’s too small for the ﬂoa ng part
representa on in your computer to store accurately.
So in prac ce we use summing logs of probabili es instead of mul plying directly.

But there’s another problem. The two op miza on func ons we have men oned are preferring small
sequences rather than long ones. Because mul plying more frac ons gives a smaller value, so fewer frac ons
‑ bigger result.
So there’s another step ‑ dividing by the number of elements in the sequence.

alpha is a hyperparameter to tune.
If alpha = 0 ‑ no sequence length normaliza on.
If alpha = 1 ‑ full sequence length normaliza on.
In prac ce alpha = 0.7 is a good thing (somewhere in between two extremes).
The second thing is how can we choose best B ?
The larger B ‑ the larger possibili es, the be er are the results. But it will be more computa onally expensive.
In prac ce, you might see in the produc on se ng B=10
B=100 , B=1000 are uncommon (some mes used in research se ngs)
Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs
faster but is not guaranteed to ﬁnd the exact solu on.

Error analysis in beam search
We have talked before on Error analysis in “Structuring Machine Learning Projects” course. We will apply these
concepts to improve our beam search algorithm.
We will use error analysis to ﬁgure out if the B hyperparameter of the beam search is the problem (it doesn’t get
an op mal solu on) or in our RNN part.
Let’s take an example:
Ini al info:
x = “Jane visite l’Afrique en septembre.”
y* = “Jane visits Africa in September.” ‑ right answer
ŷ = “Jane visited Africa last September.” ‑ answer produced by model
Our model that has produced not a good result.
We now want to know who to blame ‑ the RNN or the beam search.
To do that, we calculate P(y* | X) and P(ŷ | X). There are two cases:
Case 1 (P(y* | X) > P(ŷ | X)):
Conclusion: Beam search is at fault.
Case 2 (P(y* | X) <= P(ŷ | X)):
Conclusion: RNN model is at fault.
The error analysis process is as following:
You choose N error examples and make the following table:

B for beam search, R is for the RNN.
Get counts and decide what to work on next.

BLEU Score
One of the challenges of machine transla on, is that given a sentence in a language there are one or more possible
good transla on in another language. So how do we evaluate our results?
The way we do this is by using BLEU score. BLEU stands for bilingual evalua on understudy.
The intui on is: as long as the machine‑generated transla on is pre y close to any of the references provided by
humans, then it will get a high BLEU score.
Let’s take an example:
X = “Le chat est sur le tapis.”
Y1 = “The cat is on the mat.” (human reference 1)
Y2 = “There is a cat on the mat.” (human reference 2)
Suppose that the machine outputs: “the the the the the the the.”
One way to evaluate the machine output is to look at each word in the output and check if it is in the
references. This is called precision:
precision = 7/7 because “the” appeared in Y1 or Y2
This is not a useful measure!

We can use a modiﬁed precision in which we are looking for the reference with the maximum number of a
par cular word and set the maximum appearing of this word to this number. So:
modiﬁed precision = 2/7 because the max is 2 in Y1
We clipped the 7 mes by the max which is 2.
Here we are looking at one word at a me ‑ unigrams, we may look at n‑grams too
BLEU score on bigrams
The n‑grams typically are collected from a text or speech corpus. When the items are words, n‑grams may
also be called shingles. An n‑gram of size 1 is referred to as a “unigram”; size 2 is a “bigram” (or, less
commonly, a “digram”); size 3 is a “trigram”.
X = “Le chat est sur le tapis.”
Y1 = “The cat is on the mat.”
Y2 = “There is a cat on the mat.”
Suppose that the machine outputs: “the cat the cat on the mat.”
The bigrams in the machine output:
Pairs

Count

Count clip

the cat

2

1 (Y1)

cat the

1

0

cat on

1

1 (Y2)

on the

1

1 (Y1)

the mat

1

1 (Y1)

Totals

6

4

Modiﬁed precision = sum(Count clip) / sum(Count) = 4/6
So here are the equa ons for modiﬁed presicion for the n‑grams case:

Let’s put this together to formalize the BLEU score:
Pn = Bleu score on one type of n‑gram
Combined BLEU score = BP * exp(1/n * sum(Pn))
For example if we want BLEU for 4, we compute P1, P2, P3, P4 and then average them and take the exp.
BP is called BP penalty which stands for brevity penalty. It turns out that if a machine outputs a small number
of words it will get a be er score so we need to handle that.

BLEU score has several open source implementa ons.
It is used in a variety of systems like machine transla on and image cap oning.

A en on Model Intui on

So far we were using sequence to sequence models with an encoder and decoders. There is a technique called

a en on which makes these models even be er.
The a en on idea has been one of the most inﬂuen al ideas in deep learning.
The problem of long sequences:
Given this model, inputs, and outputs.

The encoder should memorize this long sequence into one vector, and the decoder has to process this vector
to generate the transla on.
If a human would translate this sentence, he/she wouldn’t read the whole sentence and memorize it then try
to translate it. He/she translates a part at a me.
The performance of this model decreases if a sentence is long.
We will discuss the a en on model that works like a human that looks at parts at a me. That will
signiﬁcantly increase the accuracy even with longer sequence:

Blue is the normal model, while green is the model with a en on mechanism.
In this sec on we will give just some intui ons about the a en on model and in the next sec on we will discuss
it’s details.
At ﬁrst the a en on model was developed for machine transla on but then other applica ons used it like
computer vision and new architectures like Neural Turing machine.
The a en on model was descried in this paper:
Bahdanau et. al., 2014. Neural machine transla on by jointly learning to align and translate
Now for the intui on:
Suppose that our encoder is a bidirec onal RNN:

We give the French sentence to the encoder and it should generate a vector that represents the inputs.

Now to generate the ﬁrst word in English which is “Jane” we will make another RNN which is the decoder.
A en on weights are used to specify which words are needed when to generate a word. So to generate
“jane” we will look at “jane”, “visite”, “l’Afrique”

alpha<1,1>, alpha<1,2>, and alpha<1,3> are the a en on weights being used.
And so to generate any word there will be a set of a en on weights that controls which words we are
looking at right now.

A en on Model
Lets formalize the intui on from the last sec on into the exact details on how this can be implemented.
First we will have an bidirec onal RNN (most common is LSTMs) that encodes French language:

<t’>

For learning purposes, lets assume that a

will include the both direc ons ac va ons at me step t’.

We will have a unidirec onal RNN to produce the output using a context c which is computed using the
<t’>

a en on weights, which denote how much informa on does the output needs to look in a

Sum of the a en on weights for each element in the sequence should be 1:

The context c is calculated using this equa on:

Lets see how can we compute the a en on weights:
So alpha<t, t’> = amount of a en on y<t> should pay to a<t’>
Like for example we payed a en on to the ﬁrst three words through alpha<1,1>, alpha<1,2>, alpha<1,3>
We are going to so max the a en on weights so that their sum is 1:

Now we need to know how to calculate e<t, t’>. We will compute e using a small neural network (usually 1‑
layer, because we will need to compute this a lot):

s<t‑1> is the hidden state of the RNN s, and a<t’> is the ac va on of the other bidirec onal RNN.
One of the disadvantages of this algorithm is that it takes quadra c me or quadra c cost to run.

One fun way to see how a en on works is by visualizing the a en on weights:

Speech recogni on ‑ Audio data
Speech recogni on
One of the most exci ng developments using sequence‑to‑sequence models has been the rise of very accurate
speech recogni on.
Let’s deﬁne the speech recogni on problem:
X: audio clip
Y: transcript
If you plot an audio clip it will look like this:

The horizontal axis is me while the ver cal is changes in air pressure.
What really is an audio recording? A microphone records li le varia ons in air pressure over me, and it is
these li le varia ons in air pressure that your ear perceives as sound. You can think of an audio recording is a
long list of numbers measuring the li le air pressure changes detected by the microphone. We will use audio
sampled at 44100 Hz (or 44100 Hertz). This means the microphone gives us 44100 numbers per second.
Thus, a 10 second audio clip is represented by 441000 numbers (= 10 * 44100).
It is quite diﬃcult to work with “raw” representa on of audio.
Because even human ear doesn’t process raw wave forms, the human ear can process diﬀerent frequencies.
There’s a common preprocessing step for an audio ‑ generate a spectrogram which works similarly to human
ears.

The horizontal axis is me while the ver cal is frequencies. Intensity of diﬀerent colors shows the
amount of energy ‑ how loud is the sound for diﬀerent frequencies (a human ear does a very similar
preprocessing step).
A spectrogram is computed by sliding a window over the raw audio signal, and calculates the most ac ve
frequencies in each window using a Fourier transforma on.
In the past days, speech recogni on systems were built using phonemes that are a hand engineered basic
units of sound. Linguists used to hypothesize that wri ng down audio in terms of these basic units of sound
called phonemes would be the best way to do speech recogni on.
End‑to‑end deep learning found that phonemes was no longer needed. One of the things that made this
possible is the large audio datasets.
Research papers have around 300 ‑ 3000 hours of training data while the best commercial systems are now
trained on over 100,000 hours of audio.
You can build an accurate speech recogni on system using the a en on model that we have descried in the
previous sec on:

One of the methods that seem to work well is CTC cost which stands for “Connec onist temporal classiﬁca on”
To explain this let’s say that Y = “the quick brown fox”

We are going to use an RNN with input, output structure:

Note: this is a unidirec onal RNN, but it prac ce a bidirec onal RNN is used.
No ce, that the number of inputs and number of outputs are the same here, but in speech recogni on
problem input X tends to be a lot larger than output Y.
10 seconds of audio at 100Hz gives us X with shape (1000, ). These 10 seconds don’t contain 1000
character outputs.
The CTC cost func on allows the RNN to output something like this:
ttt_h_eee<SPC>___<SPC>qqq___ ‑ this covers “the q”.
The _ is a special character called “blank” and <SPC> is for the “space” character.
Basic rule for CTC: collapse repeated characters not separated by “blank”
So the 19 character in our Y can be generated into 1000 character output using CTC and it’s special blanks.
The ideas were taken from this paper:
Graves et al., 2006. Connec onist Temporal Classiﬁca on: Labeling unsegmented sequence data with
recurrent neural networks
This paper’s ideas were also used by Baidu’s DeepSpeech.
Using both a en on model and CTC cost can help you to build an accurate speech recogni on system.

Trigger Word Detec on
With the rise of deep learning speech recogni on, there are a lot of devices that can be waked up by saying some
words with your voice. These systems are called trigger word detec on systems.
For example, Alexa ‑ a smart device made by Amazon ‑ can answer your call “Alexa, what me is it?” and then
Alexa will respond to you.
Trigger word detec on systems include:

For now, the trigger word detec on literature is s ll evolving so there actually isn’t a single universally agreed on
the algorithm for trigger word detec on yet. But let’s discuss an algorithm that can be used.
Let’s now build a model that can solve this problem:
X: audio clip
X has been preprocessed and spectrogram features have been returned of X
X<1>, X<2>, … , X<t>

Y will be labels 0 or 1. 0 represents the non‑trigger word, while 1 is that trigger word that we need to detect.
The model architecture can be like this:

The ver cal lines in the audio clip represent moment just a er the trigger word. The corresponding to
this will be 1.
One disadvantage of this creates a very imbalanced training set. There will be a lot of zeros and few ones.
A hack to solve this is to make an output a few ones for several mes or for a ﬁxed period of me before
rever ng back to zero.

Extras

Machine transla on a en on model (from notebooks)
The model is built with keras layers.
The a en on model.

There are two separate LSTMs in this model. Because the one at the bo om of the picture is a Bi‑direc onal
LSTM and comes before the a en on mechanism, we will call it pre‑a en on Bi‑LSTM. The LSTM at the top
of the diagram comes a er the a en on mechanism, so we will call it the post‑a en on LSTM. The pre‑
a en on Bi‑LSTM goes through Tx me steps; the post‑a en on LSTM goes through Ty
The post‑a en on LSTM passes s

⟨t⟩

me steps.

, c from one me step to the next. In the lecture videos, we were using
⟨t⟩

only a basic RNN for the post‑ac va on sequence model, so the state captured by the RNN output
ac va ons s⟨t⟩ . But since we are using an LSTM here, the LSTM has both the output ac va on s⟨t⟩ and the
hidden cell state c⟨t⟩ . However, unlike previous text genera on examples (such as Dinosaurus in week 1), in
this model the post‑ac va on LSTM at me t does will not take the speciﬁc generated y ⟨t−1⟩ as input; it only
takes s⟨t⟩ and c⟨t⟩ as input. We have designed the model this way, because (unlike language genera on where
adjacent characters are highly correlated) there isn’t as strong a dependency between the previous character
and the next character in a YYYY‑MM‑DD date.
′

What one “A en on” step does to calculate the a en on variables α⟨t,t ⟩ , which are used to compute the context
variable context⟨t⟩ for each mestep in the output (t

= 1, … , Ty ).

The diagram uses a RepeatVector node to copy s⟨t−1⟩ 's value Tx mes, and then Concatenation to
′

′

concatenate s⟨t−1⟩ and a⟨t⟩ to compute e⟨t,t , which is then passed through a so max to compute α⟨t,t ⟩ .

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

01 and 02: Introduction, Regression Analysis, and Gradient
Descent
Next Index

Introduction to the course
We will learn about
State of the art
How to do the implementation
Applications of machine learning include
Search
Photo tagging
Spam filters
The AI dream of building machines as intelligent as humans
Many people believe best way to do that is mimic how humans learn
What the course covers
Learn about state of the art algorithms
But the algorithms and math alone are no good
Need to know how to get these to work in problems
Why is ML so prevalent?
Grew out of AI
Build intelligent machines
You can program a machine how to do some simple thing
For the most part hard-wiring AI is too difficult
Best way to do it is to have some way for machines to learn things themselves
A mechanism for learning - if a machine can learn from input then it does the hard work for you
Examples
Database mining
Machine learning has recently become so big party because of the huge amount of data being generated
Large datasets from growth of automation web
Sources of data include
Web data (click-stream or click through data)
Mine to understand users better
Huge segment of silicon valley
Medical records
Electronic records -> turn records in knowledges
Biological data
Gene sequences, ML algorithms give a better understanding of human genome
Engineering info
Data from sensors, log reports, photos etc
Applications that we cannot program by hand
Autonomous helicopter
Handwriting recognition
This is very inexpensive because when you write an envelope, algorithms can automatically route envelopes through
the post
Natural language processing (NLP)
AI pertaining to language
Computer vision
AI pertaining vision
Self customizing programs
Netflix
Amazon
iTunes genius
Take users info
Learn based on your behavior
Understand human learning and the brain
If we can build systems that mimic (or try to mimic) how the brain works, this may push our own understanding of the
associated neurobiology

What is machine learning?
Here we...
Define what it is
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

1/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

When to use it
Not a well defined definition
Couple of examples of how people have tried to define it
Arthur Samuel (1959)
Machine learning: "Field of study that gives computers the ability to learn without being explicitly
programmed"
Samuels wrote a checkers playing program
Had the program play 10000 games against itself
Work out which board positions were good and bad depending on wins/losses
Tom Michel (1999)
Well posed learning problem: "A computer program is said to learn from experience E with respect to
some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P,
improves with experience E."
The checkers example,
E = 10000s games
T is playing checkers
P if you win or not
Several types of learning algorithms
Supervised learning
Teach the computer how to do something, then let it use it;s new found knowledge to do it
Unsupervised learning
Let the computer learn how to do something, and use this to determine structure and patterns in data
Reinforcement learning
Recommender systems
This course
Look at practical advice for applying learning algorithms
Learning a set of tools and how to apply them

Supervised learning - introduction
Probably the most common problem type in machine learning
Starting with an example
How do we predict housing prices
Collect data regarding housing prices and how they relate to size in feet

Example problem: "Given this data, a friend has a house 750 square feet - how much can they be expected to get?"
What approaches can we use to solve this?
Straight line through data
Maybe $150 000
Second order polynomial
Maybe $200 000
One thing we discuss later - how to chose straight or curved line?
Each of these approaches represent a way of doing supervised learning
What does this mean?
We gave the algorithm a data set where a "right answer" was provided
So we know actual prices for houses
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

2/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

The idea is we can learn what makes the price a certain value from the training data
The algorithm should then produce more right answers based on new training data where we don't know the price
already
i.e. predict the price
We also call this a regression problem
Predict continuous valued output (price)
No real discrete delineation
Another example
Can we definer breast cancer as malignant or benign based on tumour size

Looking at data
Five of each
Can you estimate prognosis based on tumor size?
This is an example of a classification problem
Classify data into one of two discrete classes - no in between, either malignant or not
In classification problems, can have a discrete number of possible values for the output
e.g. maybe have four values
0 - benign
1 - type 1
2 - type 2
3 - type 4
In classification problems we can plot data in a different way

Use only one attribute (size)
In other problems may have multiple attributes
We may also, for example, know age and tumor size

Based on that data, you can try and define separate classes by
Drawing a straight line between the two groups
Using a more complex function to define the two groups (which we'll discuss later)

www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

3/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

Then, when you have an individual with a specific tumor size and who is a specific age, you can hopefully use that
information to place them into one of your classes
You might have many features to consider
Clump thickness
Uniformity of cell size
Uniformity of cell shape
The most exciting algorithms can deal with an infinite number of features
How do you deal with an infinite number of features?
Neat mathematical trick in support vector machine (which we discuss later)
If you have an infinitely long list - we can develop and algorithm to deal with that
Summary
Supervised learning lets you get the "right" data a
Regression problem
Classification problem

Unsupervised learning - introduction
Second major problem type
In unsupervised learning, we get unlabeled data
Just told - here is a data set, can you structure it
One way of doing this would be to cluster data into to groups
This is a clustering algorithm
Clustering algorithm
Example of clustering algorithm
Google news
Groups news stories into cohesive groups
Used in any other problems as well
Genomics
Microarray data
Have a group of individuals
On each measure expression of a gene
Run algorithm to cluster individuals into types of people

Organize computer clusters
Identify potential weak spots or distribute workload effectively
Social network analysis
Customer data
Astronomical data analysis
Algorithms give amazing results
Basically
Can you automatically generate structure
Because we don't give it the answer, it's unsupervised learning
Cocktail party algorithm
Cocktail party problem
Lots of overlapping voices - hard to hear what everyone is saying
Two people talking
Microphones at different distances from speakers
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

4/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

Record sightly different versions of the conversation depending on where your microphone is
But overlapping none the less
Have recordings of the conversation from each microphone
Give them to a cocktail party algorithm
Algorithm processes audio recordings
Determines there are two audio sources
Separates out the two sources
Is this a very complicated problem
Algorithm can be done with one line of code!
[W,s,v] = svd((repmat(sum(x.*x,1), size(x,1),1).*x)*x');
Not easy to identify
But, programs can be short!
Using octave (or MATLAB) for examples
Often prototype algorithms in octave/MATLAB to test as it's very fast
Only when you show it works migrate it to C++
Gives a much faster agile development
Understanding this algorithm
svd - linear algebra routine which is built into octave
In C++ this would be very complicated!
Shown that using MATLAB to prototype is a really good way to do this

Linear Regression
Housing price data example used earlier
Supervised learning regression problem
What do we start with?
Training set (this is your data set)
Notation (used throughout the course)
m = number of training examples
x's = input variables / features
y's = output variable "target" variables
(x,y) - single training example
(xi, yj) - specific example (ith training example)
i is an index to training set

With our training set defined - how do we used it?
Take training set
Pass into a learning algorithm
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

5/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

Algorithm outputs a function (denoted h ) (h = hypothesis)
This function takes an input (e.g. size of new house)
Tries to output the estimated value of Y
How do we represent hypothesis h ?
Going to present h as;
hθ(x) = θ0 + θ1x
h(x) (shorthand)

What does this mean?
Means Y is a linear function of x!
θi are parameters
θ0 is zero condition
θ1 is gradient
This kind of function is a linear regression with one variable
Also called univariate linear regression
So in summary
A hypothesis takes in some variable
Uses parameters determined by a learning system
Outputs a prediction based on that input

Linear regression - implementation (cost function)
A cost function lets us figure out how to fit the best straight line to our data
Choosing values for θi (parameters)
Different values give you different functions
If θ0 is 1.5 and θ1 is 0 then we get straight line parallel with X along 1.5 @ y
If θ1 is > 0 then we get a positive slope
Based on our training set we want to generate parameters which make the straight line
Chosen these parameters so hθ(x) is close to y for our training examples
Basically, uses xs in training set with hθ(x) to give output which is as close to the actual y value as possible
Think of hθ(x) as a "y imitator" - it tries to convert the x into y, and considering we already have y we can evaluate how
well hθ(x) does this
To formalize this;
We want to want to solve a minimization problem
Minimize (hθ(x) - y)2
i.e. minimize the difference between h(x) and y for each/any/every example
Sum this over the training set

Minimize squared different between predicted house price and actual house price
1/2m
1/m - means we determine the average
1/2m the 2 makes the math a bit easier, and doesn't change the constants we determine at all (i.e. half the smallest
value is still the smallest value!)
Minimizing θ0/θ1 means we get the values of θ0 and θ1 which find on average the minimal deviation of x from y when we use
those parameters in our hypothesis function
More cleanly, this is a cost function

www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

6/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

And we want to minimize this cost function
Our cost function is (because of the summartion term) inherently looking at ALL the data in the training set at any time
So to recap
Hypothesis - is like your prediction machine, throw in an x value, get a putative y value

Cost - is a way to, using your training data, determine values for your θ values which make the hypothesis as accurate as
possible

This cost function is also called the squared error cost function
This cost function is reasonable choice for most regression functions
Probably most commonly used function
In case J(θ0,θ1) is a bit abstract, going into what it does, why it works and how we use it in the coming sections
Cost function - a deeper look
Lets consider some intuition about the cost function and why we want to use it
The cost function determines parameters
The value associated with the parameters determines how your hypothesis behaves, with different values generate different
Simplified hypothesis
Assumes θ0 = 0

Cost function and goal here are very similar to when we have θ0, but with a simpler parameter
Simplified hypothesis makes visualizing cost function J() a bit easier
So hypothesis pass through 0,0
Two key functins we want to understand
hθ(x)
Hypothesis is a function of x - function of what the size of the house is
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

7/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

J(θ1)
Is a function of the parameter of θ1
So for example
θ1 = 1
J(θ1) = 0
Plot
θ1 vs J(θ1)
Data
1)
θ1 = 1
J(θ1) = 0
2)
θ1 = 0.5
J(θ1) = ~0.58
3)
θ1 = 0
J(θ1) = ~2.3
If we compute a range of values plot
J(θ1) vs θ1 we get a polynomial (looks like a quadratic)

The optimization objective for the learning algorithm is find the value of θ1 which minimizes J(θ1)
So, here θ1 = 1 is the best value for θ1

A deeper insight into the cost function - simplified cost function
Assume you're familiar with contour plots or contour figures
Using same cost function, hypothesis and goal as previously
It's OK to skip parts of this section if you don't understand cotour plots
Using our original complex hyothesis with two pariables,
So cost function is
J(θ0, θ1)
Example,
Say
θ0 = 50
θ1 = 0.06
Previously we plotted our cost function by plotting
θ1 vs J(θ1)
Now we have two parameters
Plot becomes a bit more complicated
Generates a 3D surface plot where axis are
X = θ1
Z = θ0
Y = J(θ0,θ1)

www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

8/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

We can see that the height (y) indicates the value of the cost function, so find where y is at a minimum
Instead of a surface plot we can use a contour figures/plots
Set of ellipses in different colors
Each colour is the same value of J(θ0, θ1), but obviously plot to different locations because θ1 and θ0 will vary
Imagine a bowl shape function coming out of the screen so the middle is the concentric circles

Each point (like the red one above) represents a pair of parameter values for Ɵ0 and Ɵ1
Our example here put the values at
θ0 = ~800
θ1 = ~-0.15
Not a good fit
i.e. these parameters give a value on our contour plot far from the center
If we have
θ0 = ~360
θ1 = 0
This gives a better hypothesis, but still not great - not in the center of the countour plot
Finally we find the minimum, which gives the best hypothesis
Doing this by eye/hand is a pain in the ass
What we really want is an efficient algorithm fro finding the minimum for θ0 and θ1

Gradient descent algorithm
Minimize cost function J
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

9/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

Gradient descent
Used all over machine learning for minimization
Start by looking at a general J() function
Problem
We have J(θ0, θ1)
We want to get min J(θ0, θ1)
Gradient descent applies to more general functions
J(θ0, θ1, θ2 .... θn)
min J(θ0, θ1, θ2 .... θn)
How does it work?
Start with initial guesses
Start at 0,0 (or any other value)
Keeping changing θ0 and θ1 a little bit to try and reduce J(θ0,θ1)
Each time you change the parameters, you select the gradient which reduces J(θ0,θ1) the most possible
Repeat
Do so until you converge to a local minimum
Has an interesting property
Where you start can determine which minimum you end up

Here we can see one initialization point led to one local minimum
The other led to a different one
A more formal definition
Do the following until covergence

What does this all mean?
Update θj by setting it to (θj - α) times the partial derivative of the cost function with respect to θj
Notation
:=
Denotes assignment
NB a = b is a truth assertion
α (alpha)
Is a number called the learning rate
Controls how big a step you take
If α is big have an aggressive gradient descent
If α is small take tiny steps
Derivative term

Not going to talk about it now, derive it later
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

10/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

There is a subtly about how this gradient descent algorithm is implemented
Do this for θ0 and θ1
For j = 0 and j = 1 means we simultaneously update both
How do we do this?
Compute the right hand side for both θ0 and θ1
So we need a temp value
Then, update θ0 and θ1 at the same time
We show this graphically below

If you implement the non-simultaneous update it's not gradient descent, and will behave weirdly
But it might look sort of right - so it's important to remember this!
Understanding the algorithm
To understand gradient descent, we'll return to a simpler function where we minimize one parameter to help explain the
algorithm in more detail
min θ1 J(θ1) where θ1 is a real number
Two key terms in the algorithm
Alpha
Derivative term
Notation nuances
Partial derivative vs. derivative
Use partial derivative when we have multiple variables but only derive with respect to one
Use derivative when we are deriving with respect to all the variables
Derivative term

Derivative says
Lets take the tangent at the point and look at the slope of the line
So moving towards the mimum (down) will greate a negative derivative, alpha is always positive, so will update j(θ1) to
a smaller value
Similarly, if we're moving up a slope we make j(θ1) a bigger numbers
Alpha term (α)
What happens if alpha is too small or too large
Too small
Take baby steps
Takes too long
Too large
Can overshoot the minimum and fail to converge
When you get to a local minimum
Gradient of tangent/derivative is 0
So derivative term = 0
alpha * 0 = 0
So θ1 = θ1- 0
So θ1 remains the same
As you approach the global minimum the derivative term gets smaller, so your update gets smaller, even with alpha is fixed
Means as the algorithm runs you take smaller steps as you approach the minimum
So no need to change alpha over time

Linear regression with gradient descent
Apply gradient descent to minimize the squared error cost function J(θ0, θ1)
Now we have a partial derivative

www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

11/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

So here we're just expanding out the first expression
J(θ0, θ1) = 1/2m....
hθ(x) = θ0 + θ1*x
So we need to determine the derivative for each parameter - i.e.
When j = 0
When j = 1
Figure out what this partial derivative is for the θ0 and θ1 case
When we derive this expression in terms of j = 0 and j = 1 we get the following

To check this you need to know multivariate calculus
So we can plug these values back into the gradient descent algorithm
How does it work
Risk of meeting different local optimum
The linear regression cost function is always a convex function - always has a single minimum
Bowl shaped
One global optima
So gradient descent will always converge to global optima
In action
Initialize values to
θ0 = 900
θ1 = -0.1

End up at a global minimum
This is actually Batch Gradient Descent
Refers to the fact that over each step you look at all the training data
Each step compute over m training examples
Sometimes non-batch versions exist, which look at small data subsets
www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

12/13

10/5/2020

01_02_Introduction_regression_analysis_and_gradient_descent

We'll look at other forms of gradient descent (to use when m is too large) later in the course
There exists a numerical solution for finding a solution for a minimum function
Normal equations method
Gradient descent scales better to large data sets though
Used in lots of contexts and machine learning
What's next - important extensions
Two extension to the algorithm
1) Normal equation for numeric solution
To solve the minimization problem we can solve it [ min J(θ0, θ1) ] exactly using a numeric method which avoids the
iterative approach used by gradient descent
Normal equations method
Has advantages and disadvantages
Advantage
No longer an alpha term
Can be much faster for some problems
Disadvantage
Much more complicated
We discuss the normal equation in the linear regression with multiple features section
2) We can learn with a larger number of features
So may have other parameters which contribute towards a prize
e.g. with houses
Size
Age
Number bedrooms
Number floors
x1, x2, x3, x4
With multiple features becomes hard to plot
Can't really plot in more than 3 dimensions
Notation becomes more complicated too
Best way to get around with this is the notation of linear algebra
Gives notation and set of things you can do with matrices and vectors
e.g. Matrix

We see here this matrix shows us
Size
Number of bedrooms
Number floors
Age of home
All in one variable
Block of numbers, take all data organized into one big block
Vector
Shown as y
Shows us the prices
Need linear algebra for more complex linear regression modles
Linear algebra is good for making computationally efficient models (as seen later too)
Provide a good way to work with large sets of data sets
Typically vectorization of a problem is a common optimization technique

www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html

13/13

10/5/2020

03_Linear_algebra_review

03: Linear Algebra - Review
Previous Next Index

Matrices - overview
Rectangular array of numbers written between square brackets
2D array
Named as capital letters (A,B,X,Y)
Dimension of a matrix are [Rows x Columns]
Start at top left
To bottom left
To bottom right
R[r x c] means a matrix which has r rows and c columns

Is a [4 x 2] matrix
Matrix elements
A(i,j) = entry in ith row and jth column

Provides a way to organize, index and access a lot of data

Vectors - overview
Is an n by 1 matrix
Usually referred to as a lower case letter
n rows
1 column
e.g.

Is a 4 dimensional vector
Refer to this as a vector R4
Vector elements
vi = ith element of the vector
Vectors can be 0-indexed (C++) or 1-indexed (MATLAB)
In math 1-indexed is most common
But in machine learning 0-index is useful
Normally assume using 1-index vectors, but be aware sometimes these will (explicitly) be 0 index ones

Matrix manipulation
Addition
Add up elements one at a time
Can only add matrices of the same dimensions
Creates a new matrix of the same dimensions of the ones added
www.holehouse.org/mlclass/03_Linear_algebra_review.html

1/6

10/5/2020

03_Linear_algebra_review

Multiplication by scalar
Scalar = real number
Multiply each element by the scalar
Generates a matrix of the same size as the original matrix

Division by a scalar
Same as multiplying a matrix by 1/4
Each element is divided by the scalar
Combination of operands
Evaluate multiplications first

Matrix by vector multiplication
[3 x 2] matrix * [2 x 1] vector
New matrix is [3 x 1]
More generally if [a x b] * [b x c]
Then new matrix is [a x c]
How do you do it?
Take the two vector numbers and multiply them with the first row of the matrix
Then add results together - this number is the first number in the new vector
The multiply second row by vector and add the results together
Then multiply final row by vector and add them together

Detailed explanation
A*x=y
A is m x n matrix
x is n x 1 matrix
www.holehouse.org/mlclass/03_Linear_algebra_review.html

2/6

10/5/2020

03_Linear_algebra_review

n must match between vector and matrix
i.e. inner dimensions must match
Result is an m-dimensional vector
To get yi - multiply A's ith row with all the elements of vector x and add them up
Neat trick
Say we have a data set with four values
Say we also have a hypothesis hθ(x) = -40 + 0.25x
Create your data as a matrix which can be multiplied by a vector
Have the parameters in a vector which your matrix can be multiplied by
Means we can do
Prediction = Data Matrix * Parameters

Here we add an extra column to the data with 1s - this means our θ0 values can be calculated and expressed
The diagram above shows how this works
This can be far more efficient computationally than lots of for loops
This is also easier and cleaner to code (assuming you have appropriate libraries to do matrix multiplication)
Matrix-matrix multiplication
General idea
Step through the second matrix one column at a time
Multiply each column vector from second matrix by the entire first matrix, each time generating a vector
The final product is these vectors combined (not added or summed, but literally just put together)
Details
AxB=C
A = [m x n]
B = [n x o]
C = [m x o]
With vector multiplications o = 1
Can only multiply matrix where columns in A match rows in B
Mechanism
Take column 1 of B, treat as a vector
Multiply A by that column - generates an [m x 1] vector
Repeat for each column in B
There are o columns in B, so we get o columns in C
Summary
The i th column of matrix C is obtained by multiplying A with the i th column of B
Start with an example
AxB

www.holehouse.org/mlclass/03_Linear_algebra_review.html

3/6

10/5/2020

03_Linear_algebra_review

Initially
Take matrix A and multiply by the first column vector from B
Take the matrix A and multiply by the second column vector from B

2 x 3 times 3 x 2 gives you a 2 x 2 matrix

Implementation/use
House prices, but now we have three hypothesis and the same data set
To apply all three hypothesis to all data we can do this efficiently using matrix-matrix multiplication
Have
Data matrix
Parameter matrix
Example
Four houses, where we want to predict the prize
Three competing hypotheses
Because our hypothesis are one variable, to make the matrices match up we make our data (houses sizes) vector into a 4x2
matrix by adding an extra column of 1s

What does this mean
Can quickly apply three hypotheses at once, making 12 predictions
Lots of good linear algebra libraries to do this kind of thing very efficiently

Matrix multiplication properties
Can pack a lot into one operation
However, should be careful of how you use those operations
www.holehouse.org/mlclass/03_Linear_algebra_review.html

4/6

10/5/2020

03_Linear_algebra_review

Some interesting properties
Commutativity
When working with raw numbers/scalars multiplication is commutative
3 * 5 == 5 * 3
This is not true for matrix
A x B != B x A
Matrix multiplication is not commutative
Associativity
3 x 5 x 2 == 3 x 10 = 15 x 2
Associative property
Matrix multiplications is associative
A x (B x C) == (A x B) x C
Identity matrix
1 is the identity for any scalar
i.e. 1 x z = z
for any real number
In matrices we have an identity matrix called I
Sometimes called I{n x n}

See some identity matrices above
Different identity matrix for each set of dimensions
Has
1s along the diagonals
0s everywhere else
1x1 matrix is just "1"
Has the property that any matrix A which can be multiplied by an identity matrix gives you matrix A back
So if A is [m x n] then
A*I
I=nxn
I*A
I=mxm
(To make inside dimensions match to allow multiplication)
Identity matrix dimensions are implicit
Remember that matrices are not commutative AB != BA
Except when B is the identity matrix
Then AB == BA

Inverse and transpose operations
Matrix inverse
How does the concept of "the inverse" relate to real numbers?
1 = "identity element" (as mentioned above)
Each number has an inverse
This is the number you multiply a number by to get the identify element
i.e. if you have x, x * 1/x = 1
e.g. given the number 3
3 * 3-1 = 1 (the identity number/matrix)
In the space of real numbers not everything has an inverse
e.g. 0 does not have an inverse
What is the inverse of a matrix
If A is an m x m matrix, then A inverse = A-1
So A*A-1 = I
Only matrices which are m x m have inverses
Square matrices only!
Example

www.holehouse.org/mlclass/03_Linear_algebra_review.html

5/6

10/5/2020

03_Linear_algebra_review

2 x 2 matrix

How did you find the inverse
Turns out that you can sometimes do it by hand, although this is very hard
Numerical software for computing a matrices inverse
Lots of open source libraries
If A is all zeros then there is no inverse matrix
Some others don't, intuition should be matrices that don't have an inverse are a singular matrix or a degenerate matrix (i.e.
when it's too close to 0)
So if all the values of a matrix reach zero, this can be described as reaching singularity
Matrix transpose
Have matrix A (which is [n x m]) how do you change it to become [m x n] while keeping the same values
i.e. swap rows and columns!
How you do it;
Take first row of A - becomes 1st column of AT
Second row of A - becomes 2nd column...
A is an m x n matrix
B is a transpose of A
Then B is an n x m matrix
A(i,j) = B(j,i)

www.holehouse.org/mlclass/03_Linear_algebra_review.html

6/6

10/5/2020

04_Linear_Regression_with_multiple_variables

04: Linear Regression with Multiple Variables
Previous Next Index

Linear regression with multiple features
New version of linear regression with multiple features
Multiple variables = multiple features
In original version we had
X = house size, use this to predict
y = house price
If in a new scheme we have more variables (such as number of bedrooms, number floors, age of the home)
x1, x2, x3, x4 are the four features
x1 - size (feet squared)
x2 - Number of bedrooms
x3 - Number of floors
x4 - Age of home (years)
y is the output variable (price)
More notation
n
number of features (n = 4)
m
number of examples (i.e. number of rows in a table)
xi
vector of the input for an example (so a vector of the four parameters for the ith input example)
i is an index into the training set
So
x is an n-dimensional feature vector
x3 is, for example, the 3rd house, and contains the four features associated with that house
xji
The value of feature j in the ith training example
So
x23 is, for example, the number of bedrooms in the third house
Now we have multiple features
What is the form of our hypothesis?
Previously our hypothesis took the form;
hθ(x) = θ0 + θ1x
Here we have two parameters (theta 1 and theta 2) determined by our cost function
One variable x
Now we have multiple features
hθ(x) = θ0 + θ1x1 + θ2x2 + θ3x3 + θ4x4
For example
hθ(x) = 80 + 0.1x1 + 0.01x2 + 3x3 - 2x4
An example of a hypothesis which is trying to predict the price of a house
Parameters are still determined through a cost function
For convenience of notation, x0 = 1
For every example i you have an additional 0th feature for each example
So now your feature vector is n + 1 dimensional feature vector indexed from 0
This is a column vector called x
Each example has a column vector associated with it
So let's say we have a new example called "X"
Parameters are also in a 0 indexed n+1 dimensional vector
This is also a column vector called θ
This vector is the same for each example
www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

1/10

10/5/2020

04_Linear_Regression_with_multiple_variables

Considering this, hypothesis can be written
hθ(x) = θ0x0 + θ1x1 + θ2x2 + θ3x3 + θ4x4
If we do
hθ(x) =θT X
θT is an [1 x n+1] matrix
In other words, because θ is a column vector, the transposition operation transforms it into a
row vector
So before
θ was a matrix [n + 1 x 1]
Now
θT is a matrix [1 x n+1]
Which means the inner dimensions of θT and X match, so they can be multiplied together as
[1 x n+1] * [n+1 x 1]
= hθ(x)
So, in other words, the transpose of our parameter vector * an input example X gives you a
predicted hypothesis which is [1 x 1] dimensions (i.e. a single value)
This x0 = 1 lets us write this like this
This is an example of multivariate linear regression

Gradient descent for multiple variables
Fitting parameters for the hypothesis with gradient descent
Parameters are θ0 to θn
Instead of thinking about this as n separate values, think about the parameters as a single vector (θ)
Where θ is n+1 dimensional
Our cost function is

Similarly, instead of thinking of J as a function of the n+1 numbers, J() is just a function of the parameter vector
J(θ)

Gradient descent

Once again, this is
θj = θj - learning rate (α) times the partial derivative of J(θ) with respect to θJ(...)
We do this through a simultaneous update of every θj value
Implementing this algorithm
When n = 1

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

2/10

10/5/2020

04_Linear_Regression_with_multiple_variables

Above, we have slightly different update rules for θ0 and θ1

Actually they're the same, except the end has a previously undefined x0(i) as 1, so wasn't shown
We now have an almost identical rule for multivariate gradient descent

What's going on here?
We're doing this for each j (0 until n) as a simultaneous update (like when n = 1)
So, we re-set θj to
θj minus the learning rate (α) times the partial derivative of of the θ vector with respect to θj
In non-calculus words, this means that we do
Learning rate
Times 1/m (makes the maths easier)
Times the sum of
The hypothesis taking in the variable vector, minus the actual value, times the j-th value in
that variable vector for EACH example
It's important to remember that

These algorithm are highly similar

Gradient Decent in practice: 1 Feature Scaling
Having covered the theory, we now move on to learn about some of the practical tricks
Feature scaling
If you have a problem with multiple features
You should make sure those features have a similar scale
Means gradient descent will converge more quickly
e.g.
x1 = size (0 - 2000 feet)
x2 = number of bedrooms (1-5)
Means the contours generated if we plot θ1 vs. θ2 give a very tall and thin shape due to the huge range
difference
Running gradient descent on this kind of cost function can take a long time to find the global minimum

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

3/10

10/5/2020

04_Linear_Regression_with_multiple_variables

Pathological input to gradient descent
So we need to rescale this input so it's more effective
So, if you define each value from x1 and x2 by dividing by the max for each feature
Contours become more like circles (as scaled between 0 and 1)
May want to get everything into -1 to +1 range (approximately)
Want to avoid large ranges, small ranges or very different ranges from one another
Rule a thumb regarding acceptable ranges
-3 to +3 is generally fine - any bigger bad
-1/3 to +1/3 is ok - any smaller bad
Can do mean normalization
Take a feature xi
Replace it by (xi - mean)/max
So your values all have an average of about 0

Instead of max can also use standard deviation

Learning Rate α
Focus on the learning rate (α)
Topics
Update rule
Debugging
How to chose α
Make sure gradient descent is working
Plot min J(θ) vs. no of iterations
(i.e. plotting J(θ) over the course of gradient descent
If gradient descent is working then J(θ) should decrease after every iteration
Can also show if you're not making huge gains after a certain number
Can apply heuristics to reduce number of iterations if need be
www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

4/10

10/5/2020

04_Linear_Regression_with_multiple_variables

If, for example, after 1000 iterations you reduce the parameters by nearly nothing you could chose to only
run 1000 iterations in the future
Make sure you don't accidentally hard-code thresholds like this in and then forget about why they're their
though!

Number of iterations varies a lot
30 iterations
3000 iterations
3000 000 iterations
Very hard to tel in advance how many iterations will be needed
Can often make a guess based a plot like this after the first 100 or so iterations
Automatic convergence tests
Check if J(θ) changes by a small threshold or less
Choosing this threshold is hard
So often easier to check for a straight line
Why? - Because we're seeing the straightness in the context of the whole algorithm
Could you design an automatic checker which calculates a threshold based on the systems
preceding progress?
Checking its working
If you plot J(θ) vs iterations and see the value is increasing - means you probably need a smaller α
Cause is because your minimizing a function which looks like this

But you overshoot, so reduce learning rate so you actually reach the minimum (green line)

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

5/10

10/5/2020

04_Linear_Regression_with_multiple_variables

So, use a smaller α
Another problem might be if J(θ) looks like a series of waves
Here again, you need a smaller α
However
If α is small enough, J(θ) will decrease on every iteration
BUT, if α is too small then rate is too slow
A less steep incline is indicative of a slow convergence, because we're decreasing by less on each
iteration than a steeper slope
Typically
Try a range of alpha values
Plot J(θ) vs number of iterations for each version of alpha
Go for roughly threefold increases
0.001, 0.003, 0.01, 0.03. 0.1, 0.3

Features and polynomial regression
Choice of features and how you can get different learning algorithms by choosing appropriate features
Polynomial regression for non-linear function
Example
House price prediction
Two features
Frontage - width of the plot of land along road (x1)
Depth - depth away from road (x2)
You don't have to use just two features
Can create new features
Might decide that an important feature is the land area
So, create a new feature = frontage * depth (x3)
h(x) = θ0 + θ1x3
Area is a better indicator
Often, by defining new features you may get a better model
Polynomial regression
May fit the data better
θ0 + θ1x + θ2x2 e.g. here we have a quadratic function
For housing data could use a quadratic function
But may not fit the data so well - inflection point means housing prices decrease when size gets really
big
So instead must use a cubic function

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

6/10

10/5/2020

04_Linear_Regression_with_multiple_variables

How do we fit the model to this data
To map our old linear hypothesis and cost functions to these polynomial descriptions the easy thing to do
is set
x1 = x
x2 = x2

x3 = x3
By selecting the features like this and applying the linear regression algorithms you can do polynomial
linear regression
Remember, feature scaling becomes even more important here
Instead of a conventional polynomial you could do variable ^(1/something) - i.e. square root, cubed root etc
Lots of features - later look at developing an algorithm to chose the best features

Normal equation
For some linear regression problems the normal equation provides a better solution
So far we've been using gradient descent
Iterative algorithm which takes steps to converse
Normal equation solves θ analytically
Solve for the optimum value of theta
Has some advantages and disadvantages
How does it work?
Simplified cost function
J(θ) = aθ2 + bθ + c
θ is just a real number, not a vector
Cost function is a quadratic function
How do you minimize this?
Do

Take derivative of J(θ) with respect to θ
Set that derivative equal to 0
Allows you to solve for the value of θ which minimizes J(θ)
In our more complex problems;
Here θ is an n+1 dimensional vector of real numbers
Cost function is a function of the vector value
How do we minimize this function
Take the partial derivative of J(θ) with respect θj and set to 0 for every j
Do that and solve for θ0 to θn
This would give the values of θ which minimize J(θ)
If you work through the calculus and the solution, the derivation is pretty complex
www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

7/10

10/5/2020

04_Linear_Regression_with_multiple_variables

Not going to go through here
Instead, what do you need to know to implement this process
Example of normal equation

Here
m=4
n=4
To implement the normal equation
Take examples
Add an extra column (x0 feature)
Construct a matrix (X - the design matrix) which contains all the training data features in an [m x n+1]
matrix
Do something similar for y
Construct a column vector y vector [m x 1] matrix
Using the following equation (X transpose * X) inverse times X transpose y

If you compute this, you get the value of theta which minimize the cost function
General case
Have m training examples and n features
The design matrix (X)
Each training example is a n+1 dimensional feature column vector
X is constructed by taking each training example, determining its transpose (i.e. column -> row) and
using it for a row in the design A

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

8/10

10/5/2020

04_Linear_Regression_with_multiple_variables

This creates an [m x (n+1)] matrix

Vector y
Used by taking all the y values into a column vector

What is this equation?!
(XT * X)-1
What is this --> the inverse of the matrix (XT * X)
i.e. A = XT X
A-1 = (XT X)-1
In octave and MATLAB you could do;
pinv(X'*x)*x'*y
X' is the notation for X transpose
pinv is a function for the inverse of a matrix
In a previous lecture discussed feature scaling
If you're using the normal equation then no need for feature scaling
When should you use gradient descent and when should you use feature scaling?
Gradient descent
Need to chose learning rate
Needs many iterations - could make it slower
Works well even when n is massive (millions)
Better suited to big data
What is a big n though
100 or even a 1000 is still (relativity) small
If n is 10 000 then look at using gradient descent
Normal equation
No need to chose a learning rate
No need to iterate, check for convergence etc.
Normal equation needs to compute (XT X)-1
This is the inverse of an n x n matrix
With most implementations computing a matrix inverse grows by O(n3 )
So not great
Slow of n is large
Can be much slower

Normal equation and non-invertibility
Advanced concept
Often asked about, but quite advanced, perhaps optional material
www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

9/10

10/5/2020

04_Linear_Regression_with_multiple_variables

Phenomenon worth understanding, but not probably necessary
When computing (XT X)-1 * XT * y)
What if (XT X) is non-invertible (singular/degenerate)
Only some matrices are invertible
This should be quite a rare problem
Octave can invert matrices using
pinv (pseudo inverse)
This gets the right value even if (XT X) is non-invertible
inv (inverse)
What does it mean for (XT X) to be non-invertible
Normally two common causes
Redundant features in learning model
e.g.
x1 = size in feet
x2 = size in meters squared
Too many features
e.g. m <= n (m is much larger than n)
m = 10
n = 100
Trying to fit 101 parameters from 10 training examples
Sometimes work, but not always a good idea
Not enough data
Later look at why this may be too little data
To solve this we
Delete features
Use regularization (let's you use lots of features for a small training set)
If you find (XT X) to be non-invertible
Look at features --> are features linearly dependent?
So just delete one, will solve problem

www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables.html

10/10

10/5/2020

06_Logistic_Regression

06: Logistic Regression
Previous Next Index

Classification
Where y is a discrete value
Develop the logistic regression algorithm to determine what class a new input
should fall into
Classification problems
Email -> spam/not spam?
Online transactions -> fraudulent?
Tumor -> Malignant/benign
Variable in these problems is Y
Y is either 0 or 1
0 = negative class (absence of something)
1 = positive class (presence of something)
Start with binary class problems
Later look at multiclass classification problem, although this is just an extension
of binary classification
How do we develop a classification algorithm?
Tumour size vs malignancy (0 or 1)
We could use linear regression
Then threshold the classifier output (i.e. anything over some value is yes,
else no)
In our example below linear regression with thresholding seems to work

We can see above this does a reasonable job of stratifying the data points into one of
two classes
But what if we had a single Yes with a very small tumour
This would lead to classifying all the existing yeses as nos
Another issues with linear regression
We know Y is 0 or 1
Hypothesis can give values large than 1 or less than 0
So, logistic regression generates a value where is always either 0 or 1
www.holehouse.org/mlclass/06_Logistic_Regression.html

1/16

10/5/2020

06_Logistic_Regression

Logistic regression is a classification algorithm - don't be confused

Hypothesis representation
What function is used to represent our hypothesis in classification
We want our classifier to output values between 0 and 1
When using linear regression we did hθ(x) = (θT x)
For classification hypothesis representation we do hθ(x) = g((θT x))
Where we define g(z)
z is a real number
g(z) = 1/(1 + e-z)
This is the sigmoid function, or the logistic function
If we combine these equations we can write out the hypothesis as

What does the sigmoid function look like
Crosses 0.5 at the origin, then flattens out]
Asymptotes at 0 and 1

Given this we need to fit θ to our data
Interpreting hypothesis output
When our hypothesis (hθ(x)) outputs a number, we treat that value as the estimated probability
that y=1 on input x
Example
If X is a feature vector with x0 = 1 (as always) and x1 = tumourSize
hθ(x) = 0.7
Tells a patient they have a 70% chance of a tumor being malignant
We can write this using the following notation
hθ(x) = P(y=1|x ; θ)
What does this mean?
www.holehouse.org/mlclass/06_Logistic_Regression.html

2/16

10/5/2020

06_Logistic_Regression

Probability that y=1, given x, parameterized by θ
Since this is a binary classification task we know y = 0 or 1
So the following must be true
P(y=1|x ; θ) + P(y=0|x ; θ) = 1
P(y=0|x ; θ) = 1 - P(y=1|x ; θ)

Decision boundary
Gives a better sense of what the hypothesis function is computing
Better understand of what the hypothesis function looks like
One way of using the sigmoid function is;
When the probability of y being 1 is greater than 0.5 then we can predict y =
1
Else we predict y = 0
When is it exactly that hθ(x) is greater than 0.5?
Look at sigmoid function
g(z) is greater than or equal to 0.5 when z is greater than or equal to 0

So if z is positive, g(z) is greater than 0.5
z = (θT x)
So when
θT x >= 0
Then hθ >= 0.5
So what we've shown is that the hypothesis predicts y = 1 when θT x >= 0
The corollary of that when θT x <= 0 then the hypothesis predicts y = 0
Let's use this to better understand how the hypothesis makes its predictions
Decision boundary
hθ(x) = g(θ0 + θ1x1 + θ2x2)

www.holehouse.org/mlclass/06_Logistic_Regression.html

3/16

10/5/2020

06_Logistic_Regression

So, for example
θ0 = -3
θ1 = 1
θ2 = 1
So our parameter vector is a column vector with the above values
So, θT is a row vector = [-3,1,1]
What does this mean?
The z here becomes θT x
We predict "y = 1" if
-3x0 + 1x1 + 1x2 >= 0
-3 + x1 + x2 >= 0
We can also re-write this as
If (x1 + x2 >= 3) then we predict y = 1
If we plot
x1 + x2 = 3 we graphically plot our decision boundary

Means we have these two regions on the graph
Blue = false
Magenta = true
www.holehouse.org/mlclass/06_Logistic_Regression.html

4/16

10/5/2020

06_Logistic_Regression

Line = decision boundary
Concretely, the straight line is the set of points where hθ(x) = 0.5 exactly
The decision boundary is a property of the hypothesis
Means we can create the boundary with the hypothesis and parameters
without any data
Later, we use the data to determine the parameter values
i.e. y = 1 if
5 - x1 > 0
5 > x1

Non-linear decision boundaries
Get logistic regression to fit a complex non-linear data set
Like polynomial regress add higher order terms
So say we have
hθ(x) = g(θ0 + θ1x1+ θ3x12 + θ4x22)
We take the transpose of the θ vector times the input vector
Say θT was [-1,0,0,1,1] then we say;
Predict that "y = 1" if
-1 + x12 + x22 >= 0
or
x12 + x22 >= 1
If we plot x12 + x22 = 1
This gives us a circle with a radius of 1 around 0

Mean we can build more complex decision boundaries by fitting complex parameters
to this (relatively) simple hypothesis
More complex decision boundaries?
By using higher order polynomial terms, we can get even more
complex decision boundaries

www.holehouse.org/mlclass/06_Logistic_Regression.html

5/16

10/5/2020

06_Logistic_Regression

Cost function for logistic regression
Fit θ parameters
Define the optimization object for the cost function we use the fit the parameters
Training set of m training examples
Each example has is n+1 length column vector

This is the situation
Set of m training examples
Each example is a feature vector which is n+1 dimensional
x0 = 1
y ∈ {0,1}
Hypothesis is based on parameters (θ)
Given the training set how to we chose/fit θ?
Linear regression uses the following function to determine θ

Instead of writing the squared error term, we can write
If we define "cost()" as;
www.holehouse.org/mlclass/06_Logistic_Regression.html

6/16

10/5/2020

06_Logistic_Regression

cost(hθ(xi), y) = 1/2(hθ(xi) - yi)2
Which evaluates to the cost for an individual example using the same
measure as used in linear regression
We can redefine J(θ) as

Which, appropriately, is the sum of all the individual costs over the training
data (i.e. the same as linear regression)
To further simplify it we can get rid of the superscripts
So

What does this actually mean?
This is the cost you want the learning algorithm to pay if the outcome
is hθ(x) and the actual outcome is y
If we use this function for logistic regression this is a non-convex function for
parameter optimization
Could work....
What do we mean by non convex?
We have some function - J(θ) - for determining the parameters
Our hypothesis function has a non-linearity (sigmoid function of hθ(x) )
This is a complicated non-linear function
If you take hθ(x) and plug it into the Cost() function, and them plug the Cost()
function into J(θ) and plot J(θ) we find many local optimum -> non convex
function
Why is this a problem
Lots of local minima mean gradient descent may not find
the global optimum - may get stuck in a global minimum
We would like a convex function so if you run gradient descent you converge to a
global minimum
A convex logistic regression cost function
To get around this we need a different, convex Cost() function which means we can
apply gradient descent

This is our logistic regression cost function
www.holehouse.org/mlclass/06_Logistic_Regression.html

7/16

10/5/2020

06_Logistic_Regression

This is the penalty the algorithm pays
Plot the function
Plot y = 1
So hθ(x) evaluates as -log(hθ(x))

So when we're right, cost function is 0
Else it slowly increases cost function as we become "more" wrong
X axis is what we predict
Y axis is the cost associated with that prediction
This cost functions has some interesting properties
If y = 1 and hθ(x) = 1
If hypothesis predicts exactly 1 and thats exactly correct then that
corresponds to 0 (exactly, not nearly 0)
As hθ(x) goes to 0
Cost goes to infinity
This captures the intuition that if hθ(x) = 0 (predict P (y=1|x; θ) = 0) but y =
1 this will penalize the learning algorithm with a massive cost
What about if y = 0
then cost is evaluated as -log(1- hθ( x ))
Just get inverse of the other function

www.holehouse.org/mlclass/06_Logistic_Regression.html

8/16

10/5/2020

06_Logistic_Regression

Now it goes to plus infinity as hθ(x) goes to 1
With our particular cost functions J(θ) is going to be convex and avoid local minimum

Simplified cost function and gradient descent
Define a simpler way to write the cost function and apply gradient descent to the
logistic regression
By the end should be able to implement a fully functional logistic regression
function
Logistic regression cost function is as follows

This is the cost for a single example
For binary classification problems y is always 0 or 1
Because of this, we can have a simpler way to write the cost function
Rather than writing cost function on two lines/two cases
Can compress them into one equation - more efficient
Can write cost function is
cost(hθ, (x),y) = -ylog( hθ(x) ) - (1-y)log( 1- hθ(x) )
This equation is a more compact of the two cases above
We know that there are only two possible cases
www.holehouse.org/mlclass/06_Logistic_Regression.html

9/16

10/5/2020

06_Logistic_Regression

y=1
Then our equation simplifies to
-log(hθ(x)) - (0)log(1 - hθ(x))
-log(hθ(x))
Which is what we had before when y = 1
y=0
Then our equation simplifies to
-(0)log(hθ(x)) - (1)log(1 - hθ(x))
= -log(1- hθ(x))
Which is what we had before when y = 0
Clever!
So, in summary, our cost function for the θ parameters can be defined as

Why do we chose this function when other cost functions exist?
This cost function can be derived from statistics using the principle of
maximum likelihood estimation
Note this does mean there's an underlying Gaussian assumption relating to
the distribution of features
Also has the nice property that it's convex
To fit parameters θ:
Find parameters θ which minimize J(θ)
This means we have a set of parameters to use in our model
for future predictions
Then, if we're given some new example with set of features x, we can take the θ which
we generated, and output our prediction using

This result is
p(y=1 | x ; θ)
Probability y = 1, given x, parameterized by θ
How to minimize the logistic regression cost function
Now we need to figure out how to minimize J(θ)
Use gradient descent as before
Repeatedly update each parameter using a learning rate

www.holehouse.org/mlclass/06_Logistic_Regression.html

10/16

10/5/2020

06_Logistic_Regression

If you had n features, you would have an n+1 column vector for θ
This equation is the same as the linear regression rule
The only difference is that our definition for the hypothesis has changed
Previously, we spoke about how to monitor gradient descent to check it's working
Can do the same thing here for logistic regression
When implementing logistic regression with gradient descent, we have to update all
the θ values (θ0 to θn) simultaneously
Could use a for loop
Better would be a vectorized implementation
Feature scaling for gradient descent for logistic regression also applies here

Advanced optimization
Previously we looked at gradient descent for minimizing the cost function
Here look at advanced concepts for minimizing the cost function for logistic
regression
Good for large machine learning problems (e.g. huge feature set)
What is gradient descent actually doing?
We have some cost function J(θ), and we want to minimize it
We need to write code which can take θ as input and compute the following
J(θ)
Partial derivative if J(θ) with respect to j (where j=0 to j = n)

Given code that can do these two things
Gradient descent repeatedly does the following update

So update each j in θ sequentially
So, we must;
Supply code to compute J(θ) and the derivatives
www.holehouse.org/mlclass/06_Logistic_Regression.html

11/16

10/5/2020

06_Logistic_Regression

Then plug these values into gradient descent
Alternatively, instead of gradient descent to minimize the cost function we could use
Conjugate gradient
BFGS (Broyden-Fletcher-Goldfarb-Shanno)
L-BFGS (Limited memory - BFGS)
These are more optimized algorithms which take that same input and minimize the
cost function
These are very complicated algorithms
Some properties
Advantages
No need to manually pick alpha (learning rate)
Have a clever inner loop (line search algorithm) which tries a bunch of
alpha values and picks a good one
Often faster than gradient descent
Do more than just pick a good learning rate
Can be used successfully without understanding their complexity
Disadvantages
Could make debugging more difficult
Should not be implemented themselves
Different libraries may use different implementations - may hit
performance
Using advanced cost minimization algorithms
How to use algorithms
Say we have the following example

Example above
θ1 and θ2 (two parameters)

Cost function here is J(θ) = (θ1 - 5)2 + ( θ2 - 5)2
The derivatives of the J(θ) with respect to either θ1 and θ2 turns out to be the
2(θi - 5)
First we need to define our cost function, which should have the following signature
www.holehouse.org/mlclass/06_Logistic_Regression.html

12/16

10/5/2020

06_Logistic_Regression

function [jval, gradent] = costFunction(THETA)
Input for the cost function is THETA, which is a vector of the θ parameters
Two return values from costFunction are
jval
How we compute the cost function θ (the underived cost function)
In this case = (θ1 - 5)2 + (θ2 - 5)2
gradient
2 by 1 vector
2 elements are the two partial derivative terms
i.e. this is an n-dimensional vector
Each indexed value gives the partial derivatives for the partial
derivative of J(θ) with respect to θi
Where i is the index position in the gradient vector
With the cost function implemented, we can call the advanced algorithm using
options= optimset('GradObj', 'on', 'MaxIter', '100'); % define the
options data structure
initialTheta= zeros(2,1); # set the initial dimensions for theta %
initialize the theta values
[optTheta, funtionVal, exitFlag]= fminunc(@costFunction,
initialTheta, options); % run the algorithm
Here
options is a data structure giving options for the algorithm
fminunc
function minimize the cost function (find minimum of
unconstrained multivariable function)
@costFunction is a pointer to the costFunction function to be used
For the octave implementation
initialTheta must be a matrix of at least two dimensions
How do we apply this to logistic regression?
Here we have a vector

www.holehouse.org/mlclass/06_Logistic_Regression.html

13/16

10/5/2020

06_Logistic_Regression

Here
theta is a n+1 dimensional column vector
Octave indexes from 1, not 0
Write a cost function which captures the cost function for logistic regression

Multiclass classification problems
Getting logistic regression for multiclass classification using one vs. all
Multiclass - more than yes or no (1 or 0)
Classification with multiple classes for assignment

www.holehouse.org/mlclass/06_Logistic_Regression.html

14/16

10/5/2020

06_Logistic_Regression

Given a dataset with three classes, how do we get a learning algorithm to work?
Use one vs. all classification make binary classification work for multiclass
classification
One vs. all classification
Split the training set into three separate binary classification problems
i.e. create a new fake training set
Triangle (1) vs crosses and squares (0) hθ1(x)
P(y=1 | x1; θ)
Crosses (1) vs triangle and square (0) hθ2(x)
P(y=1 | x2; θ)
Square (1) vs crosses and square (0) hθ3(x)
P(y=1 | x3; θ)

www.holehouse.org/mlclass/06_Logistic_Regression.html

15/16

10/5/2020

06_Logistic_Regression

Overall
Train a logistic regression classifier hθ(i)(x) for each class i to predict the
probability that y = i
On a new input, x to make a prediction, pick the class i that maximizes the
probability that hθ(i)(x) = 1

www.holehouse.org/mlclass/06_Logistic_Regression.html

16/16

10/5/2020

07_Regularization

07: Regularization
Previous Next Index

The problem of overfitting
So far we've seen a few algorithms - work well for many applications, but can suffer from the problem
of overfitting
What is overfitting?
What is regularization and how does it help
Overfitting with linear regression
Using our house pricing example again
Fit a linear function to the data - not a great model
This is underfitting - also known as high bias
Bias is a historic/technical one - if we're fitting a straight line to the data we have a strong
preconception that there should be a linear fit
In this case, this is not correct, but a straight line can't help being straight!
Fit a quadratic function
Works well
Fit a 4th order polynomial
Now curve fit's through all five examples
Seems to do a good job fitting the training set
But, despite fitting the data we've provided very well, this is actually not such a
good model
This is overfitting - also known as high variance
Algorithm has high variance
High variance - if fitting high order polynomial then the hypothesis can basically fit
any data
Space of hypothesis is too large

To recap, if we have too many features then the learned hypothesis may give a cost function of
exactly zero
But this tries too hard to fit the training set
www.holehouse.org/mlclass/07_Regularization.html

1/8

10/5/2020

07_Regularization

Fails to provide a general solution - unable to generalize (apply to new examples)
Overfitting with logistic regression
Same thing can happen to logistic regression
Sigmoidal function is an underfit
But a high order polynomial gives and overfitting (high variance hypothesis)

Addressing overfitting
Later we'll look at identifying when overfitting and underfitting is occurring
Earlier we just plotted a higher order function - saw that it looks "too curvy"
Plotting hypothesis is one way to decide, but doesn't always work
Often have lots of a features - here it's not just a case of selecting a degree polynomial,
but also harder to plot the data and visualize to decide what features to keep and which
to drop
If you have lots of features and little data - overfitting can be a problem
How do we deal with this?
1) Reduce number of features
Manually select which features to keep
Model selection algorithms are discussed later (good for reducing number of
features)
But, in reducing the number of features we lose some information
Ideally select those features which minimize data loss, but even so, some info
is lost
2) Regularization
Keep all features, but reduce magnitude of parameters θ
Works well when we have a lot of features, each of which contributes a bit to
predicting y

Cost function optimization for regularization
www.holehouse.org/mlclass/07_Regularization.html

2/8

10/5/2020

07_Regularization

Penalize and make some of the θ parameters really small
e.g. here θ3 and θ4

The addition in blue is a modification of our cost function to help penalize θ3 and θ4
So here we end up with θ3 and θ4 being close to zero (because the constants are massive)
So we're basically left with a quadratic function

In this example, we penalized two of the parameter values
More generally, regularization is as follows
Regularization
Small values for parameters corresponds to a simpler hypothesis (you effectively get rid
of some of the terms)
A simpler hypothesis is less prone to overfitting
Another example
Have 100 features x1, x2, ..., x100
Unlike the polynomial example, we don't know what are the high order terms
How do we pick the ones to pick to shrink?
With regularization, take cost function and modify it to shrink all the parameters
Add a term at the end
This regularization term shrinks every parameter
By convention you don't penalize θ0 - minimization is from θ1 onwards

In practice, if you include θ0 has little impact
λ is the regularization parameter
www.holehouse.org/mlclass/07_Regularization.html

3/8

10/5/2020

07_Regularization

Controls a trade off between our two goals
1) Want to fit the training set well
2) Want to keep parameters small
With our example, using the regularized objective (i.e. the cost function with the
regularization term) you get a much smoother curve which fits the data and gives a much
better hypothesis
If λ is very large we end up penalizing ALL the parameters (θ1, θ2 etc.) so all the
parameters end up being close to zero
If this happens, it's like we got rid of all the terms in the hypothesis
This results here is then underfitting
So this hypothesis is too biased because of the absence of any parameters
(effectively)
So, λ should be chosen carefully - not too big...
We look at some automatic ways to select λ later in the course

Regularized linear regression
Previously, we looked at two algorithms for linear regression
Gradient descent
Normal equation
Our linear regression with regularization is shown below

Previously, gradient descent would repeatedly update the parameters θj, where j = 0,1,2...n
simultaneously
Shown below

We've got the θ0 update here shown explicitly
This is because for regularization we don't penalize θ0 so treat it slightly differently
How do we regularize these two rules?
Take the term and add λ/m * θj
www.holehouse.org/mlclass/07_Regularization.html

4/8

10/5/2020

07_Regularization

Sum for every θ (i.e. j = 0 to n)
This gives regularization for gradient descent
We can show using calculus that the equation given below is the partial derivative of the regularized
J(θ)

The update for θj
θj gets updated to
θj - α * [a big term which also depends on θj]
So if you group the θj terms together

The term

Is going to be a number less than 1 usually
Usually learning rate is small and m is large
So this typically evaluates to (1 - a small number)
So the term is often around 0.99 to 0.95
This in effect means θj gets multiplied by 0.99
Means the squared norm of θj a little smaller
The second term is exactly the same as the original gradient descent

Regularization with the normal equation
Normal equation is the other linear regression model
Minimize the J(θ) using the normal equation
To use regularization we add a term (+ λ [n+1 x n+1]) to the equation
[n+1 x n+1] is the n+1 identity matrix

www.holehouse.org/mlclass/07_Regularization.html

5/8

10/5/2020

07_Regularization

Regularization for logistic regression
We saw earlier that logistic regression can be prone to overfitting with lots of features
Logistic regression cost function is as follows;

To modify it we have to add an extra term

This has the effect of penalizing the parameters θ1, θ2 up to θn
Means, like with linear regression, we can get what appears to be a better fitting lower order
hypothesis
How do we implement this?
Original logistic regression with gradient descent function was as follows

Again, to modify the algorithm we simply need to modify the update rule for θ1, onwards
Looks cosmetically the same as linear regression, except obviously the hypothesis is very
different

Advanced optimization of regularized linear regression
As before, define a costFunction which takes a θ parameter and gives jVal and gradient back

www.holehouse.org/mlclass/07_Regularization.html

6/8

10/5/2020

07_Regularization

use fminunc
Pass it an @costfunction argument
Minimizes in an optimized manner using the cost function
jVal
Need code to compute J(θ)
Need to include regularization term
Gradient
Needs to be the partial derivative of J(θ) with respect to θi
Adding the appropriate term here is also necessary

www.holehouse.org/mlclass/07_Regularization.html

7/8

10/5/2020

07_Regularization

Ensure summation doesn't extend to to the lambda term!
It doesn't, but, you know, don't be daft!

www.holehouse.org/mlclass/07_Regularization.html

8/8

10/5/2020

08_Neural_Networks_Representation

08: Neural Networks - Representation
Previous Next Index

Neural networks - Overview and summary
Why do we need neural networks?
Say we have a complex supervised learning classification problem
Can use logistic regression with many polynomial terms
Works well when you have 1-2 features
If you have 100 features

e.g. our housing example
100 house features, predict odds of a house being sold in the next 6 months
Here, if you included all the quadratic terms (second order)
There are lots of them (x12 ,x1x2, x1x4 ..., x1x100)
For the case of n = 100, you have about 5000 features
Number of features grows O(n2)
This would be computationally expensive to work with as a feature set
A way around this to only include a subset of features
However, if you don't have enough features, often a model won't let you fit a complex
dataset
If you include the cubic terms
e.g. (x12x2, x1x2x3, x1x4x23 etc)
There are even more features grows O(n3)
About 170 000 features for n = 100
Not a good way to build classifiers when n is large

Example: Problems where n is large - computer vision
Computer vision sees a matrix of pixel intensity values
Look at matrix - explain what those numbers represent
To build a car detector
Build a training set of
Not cars
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

1/14

10/5/2020

08_Neural_Networks_Representation

Cars
Then test against a car
How can we do this
Plot two pixels (two pixel locations)
Plot car or not car on the graph

Need a non-linear hypothesis to separate the classes
Feature space
If we used 50 x 50 pixels --> 2500 pixels, so n = 2500
If RGB then 7500
If 100 x 100 RB then --> 50 000 000 features
Too big - wayyy too big
So - simple logistic regression here is not appropriate for large complex systems
Neural networks are much better for a complex nonlinear hypothesis even when feature
space is huge
Neurons and the brain
Neural networks (NNs) were originally motivated by looking at machines which replicate
the brain's functionality
Looked at here as a machine learning technique
Origins
To build learning systems, why not mimic the brain?
Used a lot in the 80s and 90s
Popularity diminished in late 90s
Recent major resurgence
NNs are computationally expensive, so only recently large scale neural networks
became computationally feasible
Brain
Does loads of crazy things
Hypothesis is that the brain has a single learning algorithm
Evidence for hypothesis
Auditory cortex --> takes sound signals
If you cut the wiring from the ear to the auditory cortex
Re-route optic nerve to the auditory cortex
Auditory cortex learns to see
Somatosensory context (touch processing)
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

2/14

10/5/2020

08_Neural_Networks_Representation

If you rewrite optic nerve to somatosensory cortex then it learns to see
With different tissue learning to see, maybe they all learn in the same way
Brain learns by itself how to learn
Other examples
Seeing with your tongue
Brainport
Grayscale camera on head
Run wire to array of electrodes on tongue
Pulses onto tongue represent image signal
Lets people see with their tongue
Human echolocation
Blind people being trained in schools to interpret sound and echo
Lets them move around
Haptic belt direction sense
Belt which buzzes towards north
Gives you a sense of direction
Brain can process and learn from data from any source

Model representation 1
How do we represent neural networks (NNs)?
Neural networks were developed as a way to simulate networks of neurones
What does a neurone look like

Three things to notice
Cell body
Number of input wires (dendrites)
Output wire (axon)
Simple level
Neurone gets one or more inputs through dendrites
Does processing
Sends output down axon
Neurons communicate through electric spikes
Pulse of electricity via axon to another neurone
Artificial neural network - representation of a neurone
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

3/14

10/5/2020

08_Neural_Networks_Representation

In an artificial neural network, a neurone is a logistic unit
Feed input via input wires
Logistic unit does computation
Sends output down output wires
That logistic computation is just like our previous logistic regression hypothesis calculation

Very simple model of a neuron's computation
Often good to include an x0 input - the bias unit
This is equal to 1
This is an artificial neurone with a sigmoid (logistic) activation function
Ɵ vector may also be called the weights of a model
The above diagram is a single neurone
Below we have a group of neurones strung together

Here, input is x1, x2 and x3

We could also call input activation on the first layer - i.e. (a11, a21 and a31 )
Three neurones in layer 2 (a12, a22 and a32 )
Final fourth neurone which produces the output
Which again we *could* call a13

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

4/14

10/5/2020

08_Neural_Networks_Representation

First layer is the input layer
Final layer is the output layer - produces value computed by a hypothesis
Middle layer(s) are called the hidden layers
You don't observe the values processed in the hidden layer
Not a great name
Can have many hidden layers
Neural networks - notation
ai(j) - activation of unit i in layer j
So, a12 - is the activation of the 1st unit in the second layer
By activation, we mean the value which is computed and output by that node
(j)
Ɵ - matrix of parameters controlling the function mapping from layer j to layer j + 1
Parameters for controlling mapping from one layer to the next
If network has
sj units in layer j and
sj+1 units in layer j + 1

Then Ɵj will be of dimensions [sj+1 X sj + 1]
Because
sj+1 is equal to the number of units in layer (j + 1)
is equal to the number of units in layer j, plus an additional unit
Looking at the Ɵ matrix
Column length is the number of units in the following layer
Row length is the number of units in the current layer + 1 (because we have to map
the bias unit)
So, if we had two layers - 101 and 21 units in each
Then Ɵj would be = [21 x 102]
What are the computations which occur?
We have to calculate the activation for each node
That activation depends on
The input(s) to the node
The parameter associated with that node (from the Ɵ vector associated with that
layer)
Below we have an example of a network, with the associated calculations for the four nodes
below

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

5/14

10/5/2020

08_Neural_Networks_Representation

As you can see
We calculate each of the layer-2 activations based on the input values with the bias term
(which is equal to 1)
i.e. x0 to x3
We then calculate the final hypothesis (i.e. the single node in layer 3) using exactly the
same logic, except in input is not x values, but the activation values from the preceding
layer
The activation value on each hidden unit (e.g. a12 ) is equal to the sigmoid function applied to
the linear combination of inputs
Three input units
So Ɵ(1) is the matrix of parameters governing the mapping of the input units to
hidden units
Ɵ(1) here is a [3 x 4] dimensional matrix
Three hidden units
Then Ɵ(2) is the matrix of parameters governing the mapping of the hidden layer to
the output layer
Ɵ(2) here is a [1 x 4] dimensional matrix (i.e. a row vector)
One output unit
Something conceptually important (that I hadn't really grasped the first time) is that
Every input/activation goes to every node in following layer
Which means each "layer transition" uses a matrix of parameters with the following
significance
For the sake of consistency with later nomenclature, we're using j,i and l as our
variables here (although later in this section we use j to show the layer we're
on)
Ɵjil
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

6/14

10/5/2020

08_Neural_Networks_Representation

j (first of two subscript numbers)= ranges from 1 to the number of units in
layer l+1
i (second of two subscript numbers) = ranges from 0 to the number of
units in layer l
l is the layer you're moving FROM
This is perhaps more clearly shown in my slightly over the top example below

For example
Ɵ131 = means
1 - we're mapping to node 1 in layer l+1
3 - we're mapping from node 3 in layer l
1 - we're mapping from layer 1

Model representation II
Here we'll look at how to carry out the computation efficiently through a vectorized
implementation. We'll also consider
why NNs are good and how we can use them to learn complex non-linear things
Below is our original problem from before
Sequence of steps to compute output of hypothesis are the equations below

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

7/14

10/5/2020

08_Neural_Networks_Representation

Define some additional terms
z12 = Ɵ101x0 + Ɵ111x1 + Ɵ121x2 + Ɵ131x3
Which means that
a12 = g(z12)
NB, superscript numbers are the layer associated
Similarly, we define the others as
z22 and z32
These values are just a linear combination of the values
If we look at the block we just redefined
We can vectorize the neural network computation
So lets define
x as the feature vector x
z2 as the vector of z values from the second layer

z2 is a 3x1 vector
We can vectorize the computation of the neural network as as follows in two steps
z2 = Ɵ(1)x
i.e. Ɵ(1) is the matrix defined above
x is the feature vector
2
a = g(z(2))
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

8/14

10/5/2020

08_Neural_Networks_Representation

To be clear, z2 is a 3x1 vecor
a2 is also a 3x1 vector
g() applies the sigmoid (logistic) function element wise to each member of
the z2 vector
To make the notation with input layer make sense;
a1 = x
a1 is the activations in the input layer
Obviously the "activation" for the input layer is just the input!
So we define x as a1 for clarity
So
a1 is the vector of inputs
a2 is the vector of values calculated by the g(z2) function
Having calculated then z2 vector, we need to calculate a02 for the final hypothesis calculation

To take care of the extra bias unit add a02 = 1
So add a02 to a2 making it a 4x1 vector

So,

z3 = Ɵ2a2
This is the inner term of the above equation
hƟ(x) = a3 = g(z3)
This process is also called forward propagation
Start off with activations of input unit
i.e. the x vector as input
Forward propagate and calculate the activation of each layer sequentially
This is a vectorized version of this implementation
Neural networks learning its own features
Diagram below looks a lot like logistic regression

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

9/14

10/5/2020

08_Neural_Networks_Representation

Layer 3 is a logistic regression node
The hypothesis output = g(Ɵ102 a02 + Ɵ112 a12 + Ɵ122 a22 + Ɵ132 a32)
This is just logistic regression
The only difference is, instead of input a feature vector, the features are just
values calculated by the hidden layer
The features a12, a22, and a32 are calculated/learned - not original features

So the mapping from layer 1 to layer 2 (i.e. the calculations which generate the a2 features) is
determined by another set of parameters - Ɵ1
So instead of being constrained by the original input features, a neural network can learn
its own features to feed into logistic regression
Depending on the Ɵ1 parameters you can learn some interesting things
Flexibility to learn whatever features it wants to feed into the final logistic
regression calculation
So, if we compare this to previous logistic regression, you would have to
calculate your own exciting features to define the best way to classify or
describe something
Here, we're letting the hidden layers do that, so we feed the hidden layers our
input values, and let them learn whatever gives the best final result to feed into
the final output layer
As well as the networks already seen, other architectures (topology) are possible
More/less nodes per layer
More layers
Once again, layer 2 has three hidden units, layer 3 has 2 hidden units by the time you get
to the output layer you get very interesting non-linear hypothesis

Some of the intuitions here are complicated and hard to understand
In the following lectures we're going to go though a detailed example to understand how
to do non-linear analysis

Neural network example - computing a complex,
nonlinear function of the input
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

10/14

10/5/2020

08_Neural_Networks_Representation

Non-linear classification: XOR/XNOR
x1, x2 are binary

Example on the right shows a simplified version of the more complex problem we're dealing
with (on the left)
We want to learn a non-linear decision boundary to separate the positive and negative
examples
y = x1 XOR x2
x1 XNOR x2
Where XNOR = NOT (x1 XOR x2)
Positive examples when both are true and both are false
Let's start with something a little more straight forward...
Don't worry about how we're determining the weights (Ɵ values) for now - just get
a flavor of how NNs work
Neural Network example 1: AND function
Simple first example

Can we get a one-unit neural network to compute this logical AND function? (probably...)
Add a bias unit
Add some weights for the networks
What are weights?
Weights are the parameter values which multiply into the input nodes (i.e. Ɵ)

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

11/14

10/5/2020

08_Neural_Networks_Representation

Sometimes it's convenient to add the weights into the diagram
These values are in fact just the Ɵ parameters so
Ɵ101 = -30

Ɵ111 = 20
Ɵ121 = 20

To use our original notation
Look at the four input values

So, as we can see, when we evaluate each of the four possible input, only (1,1) gives a positive
output
Neural Network example 2: NOT function
How about negation?

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

12/14

10/5/2020

08_Neural_Networks_Representation

Negation is achieved by putting a large negative weight in front of the variable you want to
negative
Neural Network example 3: XNOR function
So how do we make the XNOR function work?
XNOR is short for NOT XOR
i.e. NOT an exclusive or, so either go big (1,1) or go home (0,0)
So we want to structure this so the input which produce a positive output are
AND (i.e. both true)
OR
Neither (which we can shortcut by saying not only one being true)
So we combine these into a neural network as shown below;

Simplez!
Neural network intuition - handwritten digit classification
Yann LeCun = machine learning pioneer
Early machine learning system was postcode reading
Hilarious music, impressive demonstration!

Multiclass classification
Multiclass classification is, unsurprisingly, when you distinguish between more than two
categories (i.e. more than 1 or 0)
With handwritten digital recognition problem - 10 possible categories (0-9)
How do you do that?
Done using an extension of one vs. all classification
Recognizing pedestrian, car, motorbike or truck
Build a neural network with four output units
www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

13/14

10/5/2020

08_Neural_Networks_Representation

Output a vector of four numbers
1 is 0/1 pedestrian
2 is 0/1 car
3 is 0/1 motorcycle
4 is 0/1 truck
When image is a pedestrian get [1,0,0,0] and so on
Just like one vs. all described earlier
Here we have four logistic regression classifiers

Training set here is images of our four classifications
While previously we'd written y as an integer {1,2,3,4}
Now represent y as

www.holehouse.org/mlclass/08_Neural_Networks_Representation.html

14/14

10/5/2020

09_Neural_Networks_Learning

09: Neural Networks - Learning
Previous Next Index

Neural network cost function
NNs - one of the most powerful learning algorithms
Is a learning algorithm for fitting the derived parameters given a training set
Let's have a first look at a neural network cost function
Focus on application of NNs for classification problems
Here's the set up
Training set is {(x1, y1), (x2, y2), (x3, y3) ... (xn, ym)
L = number of layers in the network
In our example below L = 4
sl = number of units (not counting bias unit) in layer l

So here
l =4
s1 = 3
s2 = 5
s3 = 5
s4 = 4
Types of classification problems with NNs
Two types of classification, as we've previously seen
Binary classification
1 output (0 or 1)
So single output node - value is going to be a real number
k=1
NB k is number of units in output layer
sL = 1
Multi-class classification
k distinct classifications
Typically k is greater than or equal to three
If only two just go for binary
sL = k
So y is a k-dimensional vector of real numbers

Cost function for neural networks
The (regularized) logistic regression cost function is as follows;

For neural networks our cost function is a generalization of this equation above, so instead of one output we generate k outputs

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

1/11

10/5/2020

09_Neural_Networks_Learning

Our cost function now outputs a k dimensional vector
hƟ(x) is a k dimensional vector, so hƟ(x)i refers to the ith value in that vector
Costfunction J(Ɵ) is
[-1/m] times a sum of a similar term to which we had for logic regression
But now this is also a sum from k = 1 through to K (K is number of output nodes)
Summation is a sum over the k output units - i.e. for each of the possible classes
So if we had 4 output units then the sum is k = 1 to 4 of the logistic regression over each of the four output units in turn
This looks really complicated, but it's not so difficult
We don't sum over the bias terms (hence starting at 1 for the summation)
Even if you do and end up regularizing the bias term this is not a big problem
Is just summation over the terms
Woah there - lets take a second to try and understand this!
There are basically two halves to the neural network logistic regression cost function
First half

This is just saying
For each training data example (i.e. 1 to m - the first summation)
Sum for each position in the output vector
This is an average sum of logistic regression
Second half

This is a massive regularization summation term, which I'm not going to walk through, but it's a
fairly straightforward triple nested summation
This is also called a weight decay term
As before, the lambda value determines the important of the two halves
The regularization term is similar to that in logistic regression
So, we have a cost function, but how do we minimize this bad boy?!

Summary of what's about to go down
The following section is, I think, the most complicated thing in the course, so I'm going to take a second to explain the general idea of
what we're going to do;
We've already described forward propagation
This is the algorithm which takes your neural network and the initial input into that network and pushes the input through the
network
It leads to the generation of an output hypothesis, which may be a single real number, but can also be a vector
We're now going to describe back propagation
Back propagation basically takes the output you got from your network, compares it to the real value (y) and calculates how
wrong the network was (i.e. how wrong the parameters were)
It then, using the error you've just calculated, back-calculates the error associated with each unit from the preceding layer (i.e.
layer L - 1)
This goes on until you reach the input layer (where obviously there is no error, as the activation is the input)
These "error" measurements for each unit can be used to calculate the partial derivatives
Partial derivatives are the bomb, because gradient descent needs them to minimize the cost function
We use the partial derivatives with gradient descent to try minimize the cost function and update all the Ɵ values
This repeats until gradient descent reports convergence
A few things which are good to realize from the get go
There is a Ɵ matrix for each layer in the network
www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

2/11

10/5/2020

09_Neural_Networks_Learning

This has each node in layer l as one dimension and each node in l+1 as the other dimension
Similarly, there is going to be a Δ matrix for each layer
This has each node as one dimension and each training data example as the other

Back propagation algorithm
We previously spoke about the neural network cost function
Now we're going to deal with back propagation
Algorithm used to minimize the cost function, as it allows us to calculate partial derivatives!

The cost function used is shown above
We want to find parameters Ɵ which minimize J(Ɵ)
To do so we can use one of the algorithms already described such as
Gradient descent
Advanced optimization algorithms
To minimize a cost function we just write code which computes the following
J(Ɵ)
i.e. the cost function itself!
Use the formula above to calculate this value, so we've done that
Partial derivative terms
So now we need some way to do that
This is not trivial! Ɵ is indexed in three dimensions because we have separate parameter values for each node in each
layer going to each node in the following layer
i.e. each layer has a Ɵ matrix associated with it!
We want to calculate the partial derivative Ɵ with respect to a single parameter

Remember that the partial derivative term we calculate above is a REAL number (not a vector or a matrix)
Ɵ is the input parameters
Ɵ1 is the matrix of weights which define the function mapping from layer 1 to layer 2
Ɵ101 is the real number parameter which you multiply the bias unit (i.e. 1) with for the bias unit input into the
first unit in the second layer
Ɵ111 is the real number parameter which you multiply the first (real) unit with for the first input into the first
unit in the second layer
Ɵ211 is the real number parameter which you multiply the first (real) unit with for the first input into the second
unit in the second layer
As discussed, Ɵijl i
i here represents the unit in layer l+1 you're mapping to (destination node)
j is the unit in layer l you're mapping from (origin node)
l is the layer your mapping from (to layer l+1) (origin layer)
NB
The terms destination node, origin node and origin layer are terms I've made up!
So - this partial derivative term is
The partial derivative of a 3-way indexed dataset with respect to a real number (which is one of the values in that
dataset)
Gradient computation
One training example
Imagine we just have a single pair (x,y) - entire training set
How would we deal with this example?
The forward propagation algorithm operates as follows
Layer 1
a1 = x
z2 = Ɵ1a1
Layer 2
a2 = g(z2) (add a02)
z3 = Ɵ2a2
Layer 3
a3 = g(z3) (add a03)
z4 = Ɵ3a3

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

3/11

10/5/2020

09_Neural_Networks_Learning

Output
a4 = hƟ(x) = g(z4)

This is the vectorized implementation of forward propagation
Lets compute activation values sequentially (below just re-iterates what we had above!)

What is back propagation?
Use it to compute the partial derivatives
Before we dive into the mechanics, let's get an idea regarding the intuition of the algorithm
For each node we can calculate (δjl) - this is the error of node j in layer l

If we remember, ajl is the activation of node j in layer l
Remember the activation is a totally calculated value, so we'd expect there to be some error compared to the "real" value
The delta term captures this error
But the problem here is, "what is this 'real' value, and how do we calculate it?!"
The NN is a totally artificial construct
The only "real" value we have is our actual classification (our y value) - so that's where we start
If we use our example and look at the fourth (output) layer, we can first calculate
δj4 = aj4 - yj
[Activation of the unit] - [the actual value observed in the training example]
We could also write aj4 as hƟ(x)j
Although I'm not sure why we would?
This is an individual example implementation
Instead of focussing on each node, let's think about this as a vectorized problem
δ4 = a4 - y
So here δ4 is the vector of errors for the 4th layer
a4 is the vector of activation values for the 4th layer
With δ4 calculated, we can determine the error terms for the other layers as follows;

Taking a second to break this down
Ɵ3 is the vector of parameters for the 3->4 layer mapping
δ4 is (as calculated) the error vector for the 4th layer
g'(z3) is the first derivative of the activation function g evaluated by the input values given by z3
You can do the calculus if you want (...), but when you calculate this derivative you get
g'(z3) = a3 . * (1 - a3)
So, more easily
www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

4/11

10/5/2020

09_Neural_Networks_Learning

δ3 =

(Ɵ3)T δ4 .

*(a3 .

a3))

* (1 . * is the element wise multiplication between the two vectors
Why element wise? Because this is essentially an extension of individual values in a vectorized implementation,
so element wise multiplication gives that effect
We highlighted it just in case you think it's a typo!
Analyzing the mathematics

And if we take a second to consider the vector dimensionality (with our example above [3-5-5-4])
Ɵ3 = is a matrix which is [4 X 5] (if we don't include the bias term, 4 X 6 if we do)
(Ɵ3)T = therefore, is a [5 X 4] matrix
4
δ = is a 4x1 vector
So when we multiply a [5 X 4] matrix with a [4 X 1] vector we get a [5 X 1] vector
Which, low and behold, is the same dimensionality as the a3 vector, meaning we can run our pairwise multiplication
For δ3 when you calculate the derivative terms you get
a3 . * (1 - a3)
Similarly For δ2 when you calculate the derivative terms you get
a2 . * (1 - a2)
So to calculate δ2 we do
δ2 = (Ɵ2)T δ3 . *(a2 . * (1 - a2))
There's no δ1 term
Because that was the input!
Why do we do this?
We do all this to get all the δ terms, and we want the δ terms because through a very complicated derivation you can use δ to get the
partial derivative of Ɵ with respect to individual parameters (if you ignore regularization, or regularization is 0, which we deal with
later)
= ajl δi(l+1)
By doing back propagation and computing the delta terms you can then compute the partial derivative terms
We need the partial derivatives to minimize the cost function!
Putting it all together to get the partial derivatives!
What is really happening - lets look at a more complex example
Training set of m examples
First, set the delta values
Set equal to 0 for all values
Eventually these Δ values will be used to compute the partial derivative
Will be used as accumulators for computing the partial derivatives
Next, loop through the training set
i.e. for each example in the training set (dealing with each example as (x,y)
Set a1 (activation of input layer) = xi
Perform forward propagation to compute al for each layer (l = 1,2, ... L)
i.e. run forward propagation
Then, use the output label for the specific example we're looking at to calculate δL where δL = aL - yi
So we initially calculate the delta value for the output layer
Then, using back propagation we move back through the network from layer L-1 down to layer
www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

5/11

10/5/2020

09_Neural_Networks_Learning

Finally, use Δ to accumulate the partial derivative terms
Note here
l = layer
j = node in that layer
i = the error of the affected node in the target layer
You can vectorize the Δ expression too, as

Finally
After executing the body of the loop, exit the for loop and compute

When j = 0 we have no regularization term
At the end of ALL this
You've calculated all the D terms above using Δ
NB - each D term above is a real number!
We can show that each D is equal to the following

We have calculated the partial derivative for each parameter
We can then use these in gradient descent or one of the advanced optimization algorithms
Phew!
What a load of hassle!

Back propagation intuition
Some additionally back propagation notes
In case you found the preceding unclear, which it shouldn't be as it's fairly heavily modified with my own explanatory notes
Back propagation is hard(ish...)
But don't let that discourage you
It's hard in as much as it's confusing - it's not difficult, just complex
Looking at mechanical steps of back propagation
Forward propagation with pictures!

Feeding input into the input layer (xi, yi)
Note that x and y here are vectors from 1 to n where n is the number of features
So above, our data has two features (hence x1 and x2)
With out input data present we use forward propagation

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

6/11

10/5/2020

09_Neural_Networks_Learning

The sigmoid function applied to the z values gives the activation values
Below we show exactly how the z value is calculated for an example

Back propagation
With forwardprop done we move on to do back propagation
Back propagation is doing something very similar to forward propagation, but backwards
Very similar though
Let's look at the cost function again...
Below we have the cost function if there is a single output (i.e. binary classification)

This function cycles over each example, so the cost for one example really boils down to this

Which, we can think of as a sigmoidal version of the squared difference (check out the derivation if you don't believe me)
So, basically saying, "how well is the network doing on example i "?
We can think about a δ term on a unit as the "error" of cost for the activation value associated with a unit
More formally (don't worry about this...), δ is

Where cost is as defined above
Cost function is a function of y value and the hypothesis function
So - for the output layer, back propagation sets the δ value as [a - y]
Difference between activation and actual value
We then propagate these values backwards;
www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

7/11

10/5/2020

09_Neural_Networks_Learning

Looking at another example to see how we actually calculate the delta value;

So, in effect,
Back propagation calculates the δ, and those δ values are the weighted sum of the next layer's delta values, weighted by
the parameter associated with the links
Forward propagation calculates the activation (a) values, which
Depending on how you implement you may compute the delta values of the bias values
However, these aren't actually used, so it's a bit inefficient, but not a lot more!

Implementation notes - unrolling parameters (matrices)
Needed for using advanced optimization routines

Is the MATLAB/octave code
But theta is going to be matrices
fminunc takes the costfunction and initial theta values
These routines assume theta is a parameter vector
Also assumes the gradient created by costFunction is a vector
For NNs, our parameters are matrices
e.g.

Example

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

8/11

10/5/2020

09_Neural_Networks_Learning

Use the thetaVec = [ Theta1(:); Theta2(:); Theta3(:)]; notation to unroll the matrices into a long vector
To go back you use
Theta1 = resape(thetaVec(1:110), 10, 11)

Gradient checking
Backpropagation has a lot of details, small bugs can be present and ruin it :-(
This may mean it looks like J(Ɵ) is decreasing, but in reality it may not be decreasing by as much as it should
So using a numeric method to check the gradient can help diagnose a bug
Gradient checking helps make sure an implementation is working correctly
Example
Have an function J(Ɵ)
Estimate derivative of function at point Ɵ (where Ɵ is a real number)
How?
Numerically
Compute Ɵ + ε
Compute Ɵ - ε
Join them by a straight line
Use the slope of that line as an approximation to the derivative

Usually, epsilon is pretty small (0.0001)
If epsilon becomes REALLY small then the term BECOMES the slopes derivative
The is the two sided difference (as opposed to one sided difference, which would be J(Ɵ + ε) - J(Ɵ) /ε
If Ɵ is a vector with n elements we can use a similar approach to look at the partial derivatives

So, in octave we use the following code the numerically compute the derivatives

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

9/11

10/5/2020

09_Neural_Networks_Learning

So on each loop thetaPlus = theta except for thetaPlus(i)
Resets thetaPlus on each loop
Create a vector of partial derivative approximations
Using the vector of gradients from backprop (DVec)
Check that gradApprox is basically equal to DVec
Gives confidence that the Backproc implementation is correc
Implementation note
Implement back propagation to compute DVec
Implement numerical gradient checking to compute gradApprox
Check they're basically the same (up to a few decimal places)
Before using the code for learning turn off gradient checking
Why?
GradAprox stuff is very computationally expensive
In contrast backprop is much more efficient (just more fiddly)

Random initialization
Pick random small initial values for all the theta values
If you start them on zero (which does work for linear regression) then the algorithm fails - all activation values for each layer
are the same
So chose random values!
Between 0 and 1, then scale by epsilon (where epsilon is a constant)

Putting it all together
1) - pick a network architecture
Number of
Input units - number of dimensions x (dimensions of feature vector)
Output units - number of classes in classification problem
Hidden units
Default might be
1 hidden layer
Should probably have
Same number of units in each layer
Or 1.5-2 x number of input features
Normally
More hidden units is better
But more is more computational expensive
We'll discuss architecture more later

2) - Training a neural network
2.1) Randomly initialize the weights
Small values near 0
2.2) Implement forward propagation to get hƟ(x)i for any xi
2.3) Implement code to compute the cost function J(Ɵ)
2.4) Implement back propagation to compute the partial derivatives
General implementation below
www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

10/11

10/5/2020

09_Neural_Networks_Learning

for i = 1:m {
Forward propagation on (xi, yi) --> get activation (a) terms
Back propagation on (xi, yi) --> get delta (δ) terms
Compute Δ := Δl + δl+1(al)T
}
With this done compute the partial derivative terms
Notes on implementation
Usually done with a for loop over training examples (for forward and back propagation)
Can be done without a for loop, but this is a much more complicated way of doing things
Be careful
2.5) Use gradient checking to compare the partial derivatives computed using the above algorithm and numerical estimation of
gradient of J(Ɵ)
Disable the gradient checking code for when you actually run it
2.6) Use gradient descent or an advanced optimization method with back propagation to try to minimize J(Ɵ) as a function of
parameters Ɵ
Here J(Ɵ) is non-convex
Can be susceptible to local minimum
In practice this is not usually a huge problem
Can't guarantee programs with find global optimum should find good local optimum at least

e.g. above pretending data only has two features to easily display what's going on
Our minimum here represents a hypothesis output which is pretty close to y
If you took one of the peaks hypothesis is far from y
Gradient descent will start from some random point and move downhill
Back propagation calculates gradient down that hill

www.holehouse.org/mlclass/09_Neural_Networks_Learning.html

11/11

10/5/2020

10_Advice_for_applying_machine_learning

10: Advice for applying Machine Learning
Previous Next Index

Deciding what to try next
We now know many techniques
But, there is a big difference between someone who knows an algorithm vs. someone less familiar and doesn't understand
how to apply them
Make sure you know how to chose the best avenues to explore the various techniques
Here we focus deciding what avenues to try
Debugging a learning algorithm
So, say you've implemented regularized linear regression to predict housing prices

Trained it
But, when you test on new data you find it makes unacceptably large errors in its predictions
:-(
What should you try next?
There are many things you can do;
Get more training data
Sometimes more data doesn't help
Often it does though, although you should always do some preliminary testing to make sure more data will
actually make a difference (discussed later)
Try a smaller set a features
Carefully select small subset
You can do this by hand, or use some dimensionality reduction technique (e.g. PCA - we'll get to this later)
Try getting additional features
Sometimes this isn't helpful
LOOK at the data
Can be very time consuming
Adding polynomial features
You're grasping at straws, aren't you...
Building your own, new, better features based on your knowledge of the problem
Can be risky if you accidentally over fit your data by creating new features which are inherently specific/relevant
to your training data
Try decreasing or increasing λ
Change how important the regularization term is in your calculations
These changes can become MAJOR projects/headaches (6 months +)
Sadly, most common method for choosing one of these examples is to go by gut feeling (randomly)
Many times, see people spend huge amounts of time only to discover that the avenue is fruitless
No apples, pears, or any other fruit. Nada.
There are some simple techniques which can let you rule out half the things on the list
Save you a lot of time!
Machine learning diagnostics
Tests you can run to see what is/what isn't working for an algorithm
See what you can change to improve an algorithm's performance
These can take time to implement and understand (week)
But, they can also save you spending months going down an avenue which will never work

Evaluating a hypothesis
When we fit parameters to training data, try and minimize the error
We might think a low error is good - doesn't necessarily mean a good parameter set
Could, in fact, be indicative of overfitting
This means you model will fail to generalize
How do you tell if a hypothesis is overfitting?
Could plot hθ(x)
But with lots of features may be impossible to plot
Standard way to evaluate a hypothesis is
Split data into two portions
www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

1/8

10/5/2020

10_Advice_for_applying_machine_learning

1st portion is training set
2nd portion is test set
Typical split might be 70:30 (training:test)

NB if data is ordered, send a random percentage
(Or randomly order, then send data)
Data is typically ordered in some way anyway
So a typical train and test scheme would be
1) Learn parameters θ from training data, minimizing J(θ) using 70% of the training data]
2) Compute the test error
Jtest(θ) = average square error as measured on the test set

This is the definition of the test set error
What about if we were using logistic regression
The same, learn using 70% of the data, test with the remaining 30%

Sometimes there a better way - misclassification error (0/1 misclassification)
We define the error as follows

Then the test error is

i.e. its the fraction in the test set the hypothesis mislabels
These are the standard techniques for evaluating a learned hypothesis

Model selection and training validation test sets
How to chose regularization parameter or degree of polynomial (model selection problems)
We've already seen the problem of overfitting
More generally, this is why training set error is a poor predictor of hypothesis accuracy for new data (generalization)
Model selection problem

www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

2/8

10/5/2020

10_Advice_for_applying_machine_learning

Try to chose the degree for a polynomial to fit data

d = what degree of polynomial do you want to pick
An additional parameter to try and determine your training set
d =1 (linear)
d=2 (quadratic)
...
d=10
Chose a model, fit that model and get an estimate of how well you hypothesis will generalize
You could
Take model 1, minimize with training data which generates a parameter vector θ1 (where d =1)
Take mode 2, do the same, get a different θ2 (where d = 2)
And so on
Take these parameters and look at the test set error for each using the previous formula
Jtest(θ1)

Jtest(θ2)
...
Jtest(θ10)
You could then
See which model has the lowest test set error
Say, for example, d=5 is the lowest
Now take the d=5 model and say, how well does it generalize?
You could use Jtest(θ5)
BUT, this is going to be an optimistic estimate of generalization error, because our parameter is fit to that test set
(i.e. specifically chose it because the test set error is small)
So not a good way to evaluate if it will generalize
To address this problem, we do something a bit different for model selection
Improved model selection
Given a training set instead split into three pieces
1 - Training set (60%) - m values
2 - Cross validation (CV) set (20%)mcv
3 - Test set (20%) mtest
As before, we can calculate
Training error
Cross validation error
Test error

So
Minimize cost function for each of the models as before
Test these hypothesis on the cross validation set to generate the cross validation error
Pick the hypothesis with the lowest cross validation error
e.g. pick θ5
Finally
Estimate generalization error of model using the test set
Final note
www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

3/8

10/5/2020

10_Advice_for_applying_machine_learning

In machine learning as practiced today - many people will select the model using the test set and then check the model is
OK for generalization using the test error (which we've said is bad because it gives a bias analysis)
With a MASSIVE test set this is maybe OK
But considered much better practice to have separate training and validation sets

Diagnosis - bias vs. variance
If you get bad results usually because of one of
High bias - under fitting problem
High variance - over fitting problem
Important to work out which is the problem
Knowing which will help let you improve the algorithm
Bias/variance shown graphically below

The degree of a model will increase as you move towards overfitting
Lets define training and cross validation error as before
Now plot
x = degree of polynomial d
y = error for both training and cross validation (two lines)
CV error and test set error will be very similar

This plot helps us understand the error
We want to minimize both errors
Which is why that d=2 model is the sweet spot
How do we apply this for diagnostics
If cv error is high we're either at the high or the low end of d

if d is too small --> this probably corresponds to a high bias problem
if d is too large --> this probably corresponds to a high variance problem
For the high bias case, we find both cross validation and training error are high
Doesn't fit training data well
www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

4/8

10/5/2020

10_Advice_for_applying_machine_learning

Doesn't generalize either
For high variance, we find the cross validation error is high but training error is low
So we suffer from overfitting (training is low, cross validation is high)
i.e. training set fits well
But generalizes poorly

Regularization and bias/variance
How is bias and variance effected by regularization?

The equation above describes fitting a high order polynomial with regularization (used to keep parameter values small)
Consider three cases
λ = large
All θ values are heavily penalized
So most parameters end up being close to zero
So hypothesis ends up being close to 0
So high bias -> under fitting data
λ = intermediate
Only this values gives the fitting which is reasonable
λ = small
Lambda = 0
So we make the regularization term 0
So high variance -> Get overfitting (minimal regularization means it obviously doesn't do what it's meant to)

How can we automatically chose a good value for λ?
To do this we define another function Jtrain(θ) which is the optimization function without the regularization term (average
squared errors)

Define cross validation error and test set errors as before (i.e. without regularization term)
So they are 1/2 average squared error of various sets
Choosing λ
Have a set or range of values to use
Often increment by factors of 2 so
model(1)= λ = 0
model(2)= λ = 0.01
model(3)= λ = 0.02
model(4) = λ = 0.04
model(5) = λ = 0.08
.

www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

5/8

10/5/2020

10_Advice_for_applying_machine_learning

.
.
model(p) = λ = 10
This gives a number of models which have different λ
With these models
Take each one (pth)
Minimize the cost function
This will generate some parameter vector
Call this θ(p)
So now we have a set of parameter vectors corresponding to models with different λ values
Take all of the hypothesis and use the cross validation set to validate them
Measure average squared error on cross validation set
Pick the model which gives the lowest error
Say we pick θ(5)
Finally, take the one we've selected (θ(5)) and test it with the test set
Bias/variance as a function of λ
Plot λ vs.
Jtrain
When λ is small you get a small value (regularization basically goes to 0)
When λ is large you get a large vale corresponding to high bias
Jcv
When λ is small we see high variance
Too small a value means we over fit the data
When λ is large we end up underfitting, so this is bias
So cross validation error is high
Such a plot can help show you you're picking a good value for λ

Learning curves
A learning curve is often useful to plot for algorithmic sanity checking or improving performance
What is a learning curve?
Plot Jtrain (average squared error on training set) or Jcv (average squared error on cross validation set)
Plot against m (number of training examples)
m is a constant
So artificially reduce m and recalculate errors with the smaller training set sizes
Jtrain
Error on smaller sample sizes is smaller (as less variance to accommodate)
So as m grows error grows
Jcv
Error on cross validation set
When you have a tiny training set your generalize badly
But as training set grows your hypothesis generalize better
So cv error will decrease as m increases

What do these curves look like if you have
High bias
e.g. setting straight line to data
Jtrain
Training error is small at first and grows
Training error becomes close to cross validation
So the performance of the cross validation and training set end up being similar (but very poor)
Jcv
Straight line fit is similar for a few vs. a lot of data
So it doesn't generalize any better with lots of data because the function just doesn't fit the data
No increase in data will help it fit
www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

6/8

10/5/2020

10_Advice_for_applying_machine_learning

The problem with high bias is because cross validation and training error are both high
Also implies that if a learning algorithm as high bias as we get more examples the cross validation
error doesn't decrease
So if an algorithm is already suffering from high bias, more data does not help
So knowing if you're suffering from high bias is good!
In other words, high bias is a problem with the underlying way you're modeling your data
So more data won't improve that model
It's too simplistic
High variance
e.g. high order polynomial
Jtrain
When set is small, training error is small too
As training set sizes increases, value is still small
But slowly increases (in a near linear fashion)
Error is still low
Jcv
Error remains high, even when you have a moderate number of examples
Because the problem with high variance (overfitting) is your model doesn't generalize
An indicative diagnostic that you have high variance is that there's a big gap between training error and cross
validation error
If a learning algorithm is suffering from high variance, more data is probably going to help

So if an algorithm is already suffering from high variance, more data will probably help
Maybe
These are clean curves
In reality the curves you get are far dirtier
But, learning curve plotting can help diagnose the problems your algorithm will be suffering from

What to do next (revisited)
How do these ideas help us chose how we approach a problem?
Original example
Trained a learning algorithm (regularized linear regression)
But, when you test on new data you find it makes unacceptably large errors in its predictions
What should try next?
How do we decide what to do?
Get more examples --> helps to fix high variance
Not good if you have high bias
Smaller set of features --> fixes high variance (overfitting)
Not good if you have high bias
Try adding additional features --> fixes high bias (because hypothesis is too simple, make hypothesis more
specific)
Add polynomial terms --> fixes high bias problem
Decreasing λ --> fixes high bias
Increases λ --> fixes high variance
Relating it all back to neural networks - selecting a network architecture
One option is to use a small neural network
Few (maybe one) hidden layer and few hidden units
Such networks are prone to under fitting
But they are computationally cheaper
Larger network
www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

7/8

10/5/2020

10_Advice_for_applying_machine_learning

More hidden layers
How do you decide that a larger network is good?
Using a single hidden layer is good default
Also try with 1, 2, 3, see which performs best on cross validation set
So like before, take three sets (training, cross validation)
More units
This is computational expensive
Prone to over-fitting
Use regularization to address over fitting

www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html

8/8

10/5/2020

11_Machine_Learning_System_Design

11: Machine Learning System Design
Previous Next Index

Machine learning systems design
In this section we'll touch on how to put together a system
Previous sections have looked at a wide range of different issues in significant focus
This section is less mathematical, but material will be very useful non-the-less
Consider the system approach
You can understand all the algorithms, but if you don't understand how to make them work
in a complete system that's no good!

Prioritizing what to work on - spam classification
example
The idea of prioritizing what to work on is perhaps the most important skill programmers
typically need to develop
It's so easy to have many ideas you want to work on, and as a result do none of them well,
because doing one well is harder than doing six superficially
So you need to make sure you complete projects
Get something "shipped" - even if it doesn't have all the bells and whistles, that final
20% getting it ready is often the toughest
If you only release when you're totally happy you rarely get practice doing that final
20%
So, back to machine learning...
Building a spam classifier
Spam is email advertising

What kind of features might we define
Spam (1)
Misspelled word
Not spam (0)
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

1/8

10/5/2020

11_Machine_Learning_System_Design

Real content
How do we build a classifier to distinguish between the two
Feature representation
How do represent x (features of the email)?
y = spam (1) or not spam (0)
One approach - choosing your own features
Chose 100 words which are indicative of an email being spam or not spam
Spam --> e.g. buy, discount, deal
Non spam --> Andrew, now
All these words go into one long vector
Encode this into a reference vector
See which words appear in a message
Define a feature vector x
Which is 0 or 1 if a word corresponding word in the reference vector is present or not
This is a bitmap of the word content of your email
i.e. don't recount if a word appears more than once

In practice its more common to have a training set and pick the most frequently n words,
where n is 10 000 to 50 000
So here you're not specifically choosing your own features, but you are choosing
how you select them from the training set data
What's the best use of your time to improve system accuracy?
Natural inclination is to collect lots of data
Honey pot anti-spam projects try and get fake email addresses into spammers' hands,
collect loads of spam
This doesn't always help though
Develop sophisticated features based on email routing information (contained in email header)
Spammers often try and obscure origins of email
Send through unusual routes
Develop sophisticated features for message body analysis
Discount == discounts?
DEAL == deal?
Develop sophisticated algorithm to detect misspelling
Spammers use misspelled word to get around detection systems
Often a research group randomly focus on one option
May not be the most fruitful way to spend your time
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

2/8

10/5/2020

11_Machine_Learning_System_Design

If you brainstorm a set of options this is really good
Very tempting to just try something
Error analysis
When faced with a ML problem lots of ideas of how to improve a problem
Talk about error analysis - how to better make decisions
If you're building a machine learning system often good to start by building a simple algorithm
which you can implement quickly
Spend at most 24 hours developing an initially bootstrapped algorithm
Implement and test on cross validation data
Plot learning curves to decide if more data, features etc will help algorithmic optimization
Hard to tell in advance what is important
Learning curves really help with this
Way of avoiding premature optimization
We should let evidence guide decision making regarding development trajectory
Error analysis
Manually examine the samples (in cross validation set) that your algorithm made
errors on
See if you can work out why
Systematic patterns - help design new features to avoid these shortcomings
e.g.
Built a spam classifier with 500 examples in CV set
Here, error rate is high - gets 100 wrong
Manually look at 100 and categorize them depending on features
e.g. type of email
Looking at those email
May find most common type of spam emails are pharmacy
emails, phishing emails
See which type is most common - focus your work on those ones
What features would have helped classify them correctly
e.g. deliberate misspelling
Unusual email routing
Unusual punctuation
May fine some "spammer technique" is causing a lot of your misses
Guide a way around it
Importance of numerical evaluation
Have a way of numerically evaluated the algorithm
If you're developing an algorithm, it's really good to have some performance
calculation which gives a single real number to tell you how well its doing
e.g.
Say were deciding if we should treat a set of similar words as the same word
This is done by stemming in NLP (e.g. "Porter stemmer" looks at
the etymological stem of a word)
This may make your algorithm better or worse
Also worth consider weighting error (false positive vs. false negative)
e.g. is a false positive really bad, or is it worth have a few of one to
improve performance a lot
Can use numerical evaluation to compare the changes
See if a change improves an algorithm or not
A single real number may be hard/complicated to compute
But makes it much easier to evaluate how changes impact your algorithm
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

3/8

10/5/2020

11_Machine_Learning_System_Design

You should do error analysis on the cross validation set instead of the test set

Error metrics for skewed analysis
Once case where it's hard to come up with good error metric - skewed classes
Example
Cancer classification
Train logistic regression model hθ(x) where
Cancer means y = 1
Otherwise y = 0
Test classifier on test set
Get 1% error
So this looks pretty good..
But only 0.5% have cancer
Now, 1% error looks very bad!
So when one number of examples is very small this is an example of skewed classes
LOTS more of one class than another
So standard error metrics aren't so good
Another example
Algorithm has 99.2% accuracy
Make a change, now get 99.5% accuracy
Does this really represent an improvement to the algorithm?
Did we do something useful, or did we just create something which predicts y = 0 more
often
Get very low error, but classifier is still not great
Precision and recall
Two new metrics - precision and recall
Both give a value between 0 and 1
Evaluating classifier on a test set
For a test set, the actual class is 1 or 0
Algorithm predicts some value for class, predicting a value for each example in the test set
Considering this, classification can be
True positive (we guessed 1, it was 1)
False positive (we guessed 1, it was 0)
True negative (we guessed 0, it was 0)
False negative (we guessed 0, it was 1)
Precision
How often does our algorithm cause a false alarm?
Of all patients we predicted have cancer, what fraction of them actually have cancer
= true positives / # predicted positive
= true positives / (true positive + false positive)
High precision is good (i.e. closer to 1)
You want a big number, because you want false positive to be as close to 0 as
possible
Recall
How sensitive is our algorithm?
Of all patients in set that actually have cancer, what fraction did we correctly detect
= true positives / # actual positives
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

4/8

10/5/2020

11_Machine_Learning_System_Design

= true positive / (true positive + false negative)
High recall is good (i.e. closer to 1)
You want a big number, because you want false negative to be as close to 0 as
possible
By computing precision and recall get a better sense of how an algorithm is doing
This can't really be gamed
Means we're much more sure that an algorithm is good
Typically we say the presence of a rare class is what we're trying to determine (e.g. positive
(1) is the existence of the rare thing)

Trading off precision and recall
For many applications we want to control the trade-off between precision and recall
Example
Trained a logistic regression classifier
Predict 1 if hθ(x) >= 0.5
Predict 0 if hθ(x) < 0.5
This classifier may give some value for precision and some value for recall
Predict 1 only if very confident
One way to do this modify the algorithm we could modify the prediction threshold
Predict 1 if hθ(x) >= 0.8
Predict 0 if hθ(x) < 0.2
Now we can be more confident a 1 is a true positive
But classifier has lower recall - predict y = 1 for a smaller number of patients
Risk of false negatives
Another example - avoid false negatives
This is probably worse for the cancer example
Now we may set to a lower threshold
Predict 1 if hθ(x) >= 0.3
Predict 0 if hθ(x) < 0.7
i.e. 30% chance they have cancer
So now we have have a higher recall, but lower precision
Risk of false positives, because we're less discriminating in deciding what
means the person has cancer
This threshold defines the trade-off
We can show this graphically by plotting precision vs. recall

This curve can take many different shapes depending on classifier details
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

5/8

10/5/2020

11_Machine_Learning_System_Design

Is there a way to automatically chose the threshold
Or, if we have a few algorithms, how do we compare different algorithms or parameter
sets?

How do we decide which of these algorithms is best?
We spoke previously about using a single real number evaluation metric
By switching to precision/recall we have two numbers
Now comparison becomes harder
Better to have just one number
How can we convert P & R into one number?
One option is the average - (P + R)/2
This is not such a good solution
Means if we have a classifier which predicts y = 1 all the time you get a
high recall and low precision
Similarly, if we predict Y rarely get high precision and low recall
So averages here would be 0.45, 0.4 and 0.51
0.51 is best, despite having a recall of 1 - i.e. predict y=1 for
everything
So average isn't great
F1Score (fscore)
= 2 * (PR/ [P + R])
Fscore is like taking the average of precision and recall giving a higher
weight to the lower value
Many formulas for computing comparable precision/accuracy values
If P = 0 or R = 0 the Fscore = 0
If P = 1 and R = 1 then Fscore = 1
The remaining values lie between 0 and 1
Threshold offers a way to control trade-off between precision and recall
Fscore gives a single real number evaluation metric
If you're trying to automatically set the threshold, one way is to try a range of threshold
values and evaluate them on your cross validation set
Then pick the threshold which gives the best fscore.

Data for machine learning
Now switch tracks and look at how much data to train on
On early videos caution on just blindly getting more data
Turns out under certain conditions getting more data is a very effective way to improve
performance
Designing a high accuracy learning system
There have been studies of using different algorithms on data
Data - confusing words (e.g. two, to or too)
Algorithms
Perceptron (logistic regression)
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

6/8

10/5/2020

11_Machine_Learning_System_Design

Winnow
Like logistic regression
Used less now
Memory based
Used less now
Talk about this later
Naive Bayes
Cover later
Varied training set size and tried algorithms on a range of sizes

What can we conclude
Algorithms give remarkably similar performance
As training set sizes increases accuracy increases
Take an algorithm, give it more data, should beat a "better" one with less data
Shows that
Algorithm choice is pretty similar
More data helps
When is this true and when is it not?
If we can correctly assume that features x have enough information to predict y accurately,
then more data will probably help
A useful test to determine if this is true can be, "given x, can a human expert predict
y?"
So lets say we use a learning algorithm with many parameters such as logistic regression or
linear regression with many features, or neural networks with many hidden features
These are powerful learning algorithms with many parameters which can fit complex
functions
Such algorithms are low bias algorithms
Little systemic bias in their description - flexible
Use a small training set
Training error should be small
Use a very large training set
If the training set error is close to the test set error
Unlikely to over fit with our complex algorithms
So the test set error should also be small
Another way to think about this is we want our algorithm to have low bias and low variance
Low bias --> use complex algorithm
Low variance --> use large training set
www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

7/8

10/5/2020

www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html

11_Machine_Learning_System_Design

8/8

10/5/2020

12_Support_Vector_Machines

12: Support Vector Machines (SVMs)
Previous Next Index

Support Vector Machine (SVM) - Optimization objective
So far, we've seen a range of different algorithms
With supervised learning algorithms - performance is pretty similar
What matters more often is;
The amount of training data
Skill of applying algorithms
One final supervised learning algorithm that is widely used - support vector machine (SVM)
Compared to both logistic regression and neural networks, a SVM sometimes gives a cleaner way of
learning non-linear functions
Later in the course we'll do a survey of different supervised learning algorithms
An alternative view of logistic regression
Start with logistic regression, see how we can modify it to get the SVM
As before, the logistic regression hypothesis is as follows

And the sigmoid activation function looks like this

In order to explain the math, we use z as defined above
What do we want logistic regression to do?
We have an example where y = 1
Then we hope hθ(x) is close to 1

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

1/20

10/5/2020

12_Support_Vector_Machines

With hθ(x) close to 1, (θT x) must be much larger than 0

Similarly, when y = 0
Then we hope hθ(x) is close to 0
With hθ(x) close to 0, (θT x) must be much less than 0
This is our classic view of logistic regression
Let's consider another way of thinking about the problem
Alternative view of logistic regression
If you look at cost function, each example contributes a term like the one below to the overall cost
function

For the overall cost function, we sum over all the training examples using the above function,
and have a 1/m term
If you then plug in the hypothesis definition (hθ(x)), you get an expanded cost function equation;

So each training example contributes that term to the cost function for logistic regression
If y = 1 then only the first term in the objective matters
If we plot the functions vs. z we get the following graph

This plot shows the cost contribution of an example when y = 1 given z
So if z is big, the cost is low - this is good!
But if z is 0 or negative the cost contribution is high
This is why, when logistic regression sees a positive example, it tries to set θT x to be a
very large term
If y = 0 then only the second term matters

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

2/20

10/5/2020

12_Support_Vector_Machines

We can again plot it and get a similar graph

Same deal, if z is small then the cost is low
But if s is large then the cost is massive
SVM cost functions from logistic regression cost functions
To build a SVM we must redefine our cost functions
When y = 1
Take the y = 1 function and create a new cost function
Instead of a curved line create two straight lines (magenta) which acts as an approximation to
the logistic regression y = 1 function

Take point (1) on the z axis
Flat from 1 onwards
Grows when we reach 1 or a lower number
This means we have two straight lines
Flat when cost is 0
Straight growing line after 1
So this is the new y=1 cost function
Gives the SVM a computational advantage and an easier optimization problem
We call this function cost1(z)
Similarly
When y = 0

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

3/20

10/5/2020

12_Support_Vector_Machines

Do the equivalent with the y=0 function plot

We call this function cost0(z)
So here we define the two cost function terms for our SVM graphically
How do we implement this?
The complete SVM cost function
As a comparison/reminder we have logistic regression below

If this looks unfamiliar its because we previously had the - sign outside the expression
For the SVM we take our two logistic regression y=1 and y=0 terms described previously and replace
with
cost1(θT x)
cost0(θT x)
So we get

SVM notation is slightly different
In convention with SVM notation we rename a few things here
1) Get rid of the 1/m terms
This is just a slightly different convention
By removing 1/m we should get the same optimal values for
1/m is a constant, so should get same optimization
e.g. say you have a minimization problem which minimizes to u = 5
If your cost function * by a constant, you still generates the minimal value
That minimal value is different, but that's irrelevant
2) For logistic regression we had two terms;
Training data set term (i.e. that we sum over m) = A
Regularization term (i.e. that we sum over n) = B
So we could describe it as A + λB
Need some way to deal with the trade-off between regularization and data set terms
Set different values for λ to parametrize this trade-off
Instead of parameterization this as A + λB
For SVMs the convention is to use a different parameter called C
So do CA + B
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

4/20

10/5/2020

12_Support_Vector_Machines

If C were equal to 1/λ then the two functions (CA + B and A + λB) would give the same value
So, our overall equation is

Unlike logistic, hθ(x) doesn't give us a probability, but instead we get a direct prediction of 1 or 0
So if θT x is equal to or greater than 0 --> hθ(x) = 1
Else --> hθ(x) = 0

Large margin intuition
Sometimes people refer to SVM as large margin classifiers
We'll consider what that means and what an SVM hypothesis looks like
The SVM cost function is as above, and we've drawn out the cost terms below

Left is cost1 and right is cost0
What does it take to make terms small
If y =1
cost1(z) = 0 only when z >= 1
If y = 0
cost0(z) = 0 only when z <= -1
Interesting property of SVM
If you have a positive example, you only really need z to be greater or equal to 0
If this is the case then you predict 1
SVM wants a bit more than that - doesn't want to *just* get it right, but have the value be quite
a bit bigger than zero
Throws in an extra safety margin factor
Logistic regression does something similar
What are the consequences of this?
Consider a case where we set C to be huge
C = 100,000
So considering we're minimizing CA + B
If C is huge we're going to pick an A value so that A is equal to zero
What is the optimization problem here - how do we make A = 0?
Making A = 0
If y = 1
Then to make our "A" term 0 need to find a value of θ so (θT x) is greater than or
equal to 1
Similarly, if y = 0
Then we want to make "A" = 0 then we need to find a value of θ so (θT x) is equal to
or less than -1

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

5/20

10/5/2020

12_Support_Vector_Machines

So - if we think of our optimization problem a way to ensure that this first "A" term is equal to
0, we re-factor our optimization problem into just minimizing the "B" (regularization) term,
because
When A = 0 --> A*C = 0
So we're minimizing B, under the constraints shown below

Turns out when you solve this problem you get interesting decision boundaries

The green and magenta lines are functional decision boundaries which could be chosen by logistic
regression
But they probably don't generalize too well
The black line, by contrast is the the chosen by the SVM because of this safety net imposed by the
optimization graph
More robust separator
Mathematically, that black line has a larger minimum distance (margin) from any of the training
examples

By separating with the largest margin you incorporate robustness into your decision making
process
We looked at this at when C is very large
SVM is more sophisticated than the large margin might look
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

6/20

10/5/2020

12_Support_Vector_Machines

If you were just using large margin then SVM would be very sensitive to outliers

You would risk making a ridiculous hugely impact your classification boundary
A single example might not represent a good reason to change an algorithm
If C is very large then we do use this quite naive maximize the margin approach

So we'd change the black to the magenta
But if C is reasonably small, or a not too large, then you stick with the black decision boundary
What about non-linearly separable data?
Then SVM still does the right thing if you use a normal size C
So the idea of SVM being a large margin classifier is only really relevant when you have no
outliers and you can easily linearly separable data
Means we ignore a few outliers

Large margin classification mathematics (optional)
Vector inner products
Have two (2D) vectors u and v - what is the inner product (uT v)?

Plot u on graph

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

7/20

10/5/2020

12_Support_Vector_Machines

i.e u1 vs. u2

One property which is good to have is the norm of a vector
Written as ||u||
This is the euclidean length of vector u
So ||u|| = SQRT(u12 + u22) = real number
i.e. length of the arrow above
Can show via Pythagoras
For the inner product, take v and orthogonally project down onto u
First we can plot v on the same axis in the same way (v1 vs v1)
Measure the length/magnitude of the projection

So here, the green line is the projection
p = length along u to the intersection
p is the magnitude of the projection of vector v onto vector u
Possible to show that
uT v = p * ||u||
So this is one way to compute the inner product
T
u v = u1v1+ u2v2
So therefore
p * ||u|| = u1v1+ u2v2
This is an important rule in linear algebra
We can reverse this too
So we could do
vT u = v1u1+ v2u2
Which would obviously give you the same number

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

8/20

10/5/2020

12_Support_Vector_Machines

p can be negative if the angle between them is 90 degrees or more

So here p is negative
Use the vector inner product theory to try and understand SVMs a little better
SVM decision boundary

For the following explanation - two simplification
Set θ0= 0 (i.e. ignore intercept terms)
Set n = 2 - (x1, x2)
i.e. each example has only 2 features
Given we only have two parameters we can simplify our function to

And, can be re-written as

Should give same thing
We may notice that

The term in red is the norm of θ
If we take θ as a 2x1 vector
If we assume θ0 = 0 its still true
So, finally, this means our optimization function can be re-defined as

So the SVM is minimizing the squared norm
Given this, what are the (θT x) parameters doing?
Given θ and given example x what is this equal to
We can look at this in a comparable manner to how we just looked at u and v
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

9/20

10/5/2020

12_Support_Vector_Machines

Say we have a single positive training example (red cross below)

Although we haven't been thinking about examples as vectors it can be described as such

Now, say we have our parameter vector θ and we plot that on the same axis

The next question is what is the inner product of these two vectors

p, is in fact pi, because it's the length of p for example i
Given our previous discussion we know
(θT xi ) = pi * ||θ||
= θ1xi1 + θ2xi2
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

10/20

10/5/2020

12_Support_Vector_Machines

So these are both equally valid ways of computing θT xi
What does this mean?
The constraints we defined earlier
(θT x) >= 1 if y = 1
(θT x) <= -1 if y = 0
Can be replaced/substituted with the constraints
pi * ||θ|| >= 1 if y = 1
pi * ||θ|| <= -1 if y = 0
Writing that into our optimization objective

So, given we've redefined these functions let us now consider the training example below

Given this data, what boundary will the SVM choose? Note that we're still assuming θ0 = 0, which
means the boundary has to pass through the origin (0,0)
Green line - small margins

SVM would not chose this line
Decision boundary comes very close to examples
Lets discuss why the SVM would not chose this decision boundary
Looking at this line
We can show that θ is at 90 degrees to the decision boundary

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

11/20

10/5/2020

12_Support_Vector_Machines

θ is always at 90 degrees to the decision boundary (can show with linear algebra,
although we're not going to!)
So now lets look at what this implies for the optimization objective
Look at first example (x1)

Project a line from x1 on to to the θ vector (so it hits at 90 degrees)
The distance between the intersection and the origin is (p1)
Similarly, look at second example (x2)
Project a line from x2 into to the θ vector
This is the magenta line, which will be negative (p2)
If we overview these two lines below we see a graphical representation of what's going on;

We find that both these p values are going to be pretty small
If we look back at our optimization objective
We know we need p1 * ||θ|| to be bigger than or equal to 1 for positive examples
If p is small
Means that ||θ|| must be pretty large
Similarly, for negative examples we need p2 * ||θ|| to be smaller than or equal to -1
We saw in this example p2 is a small negative number
So ||θ|| must be a large number
Why is this a problem?
The optimization objective is trying to find a set of parameters where the norm of theta is small
So this doesn't seem like a good direction for the parameter vector (because as p values
get smaller ||θ|| must get larger to compensate)
So we should make p values larger which allows ||θ|| to become smaller
So lets chose a different boundary

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

12/20

10/5/2020

12_Support_Vector_Machines

Now if you look at the projection of the examples to θ we find that p1 becomes large and ||θ|| can
become small
So with some values drawn in

This means that by choosing this second decision boundary we can make ||θ|| smaller
Which is why the SVM choses this hypothesis as better
This is how we generate the large margin effect

The magnitude of this margin is a function of the p values
So by maximizing these p values we minimize ||θ||
Finally, we did this derivation assuming θ0 = 0,
If this is the case we're entertaining only decision boundaries which pass through (0,0)
If you allow θ0 to be other values then this simply means you can have decision boundaries which
cross through the x and y values at points other than (0,0)
Can show with basically same logic that this works, and even when θ0 is non-zero when you have
optimization objective described above (when C is very large) that the SVM is looking for a large
margin separator between the classes

Kernels - 1: Adapting SVM to non-linear classifiers
What are kernels and how do we use them
We have a training set

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

13/20

10/5/2020

12_Support_Vector_Machines

We want to find a non-linear boundary

Come up with a complex set of polynomial features to fit the data
Have hθ(x) which
Returns 1 if the combined weighted sum of vectors (weighted by the parameter vector) is
less than or equal to 0
Else return 0
Another way of writing this (new notation) is
That a hypothesis computes a decision boundary by taking the sum of the parameter
vector multiplied by a new feature vector f, which simply contains the various high
order x terms
e.g.
hθ(x) = θ0+ θ1f1+ θ2f2 + θ3f3
Where
f1= x1
f2 = x1x2
f3 = ...
i.e. not specific values, but each of the terms from your complex polynomial
function
Is there a better choice of feature f than the high order polynomials?
As we saw with computer imaging, high order polynomials become computationally
expensive
New features
Define three features in this example (ignore x0)
Have a graph of x1 vs. x2 (don't plot the values, just define the space)
Pick three points in that space

These points l1, l2, and l3, were chosen manually and are called landmarks
Given x, define f1 as the similarity between (x, l1)

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

14/20

10/5/2020

12_Support_Vector_Machines

= exp(- (|| x - l1 ||2 ) / 2σ2)

=
|| x - l1 || is the euclidean distance between the point x and the landmark l1 squared
Disussed more later
If we remember our statistics, we know that
σ is the standard deviation
σ2 is commonly called the variance
Remember, that as discussed

So, f2 is defined as
f2 = similarity(x, l1) = exp(- (|| x - l2 ||2 ) / 2σ2)
And similarly
f3 = similarity(x, l2) = exp(- (|| x - l1 ||2 ) / 2σ2)
This similarity function is called a kernel
This function is a Gaussian Kernel
So, instead of writing similarity between x and l we might write
f1 = k(x, l1)
Diving deeper into the kernel
So lets see what these kernels do and why the functions defined make sense
Say x is close to a landmark
Then the squared distance will be ~0
So

Which is basically e-0
Which is close to 1
Say x is far from a landmark
Then the squared distance is big
Gives e-large number
Which is close to zero
Each landmark defines a new features
If we plot f1 vs the kernel function we get a plot like this
Notice that when x = [3,5] then f1 = 1
As x moves away from [3,5] then the feature takes on values close to zero
So this measures how close x is to this landmark

What does σ do?

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

15/20

10/5/2020

12_Support_Vector_Machines

σ2 is a parameter of the Gaussian kernel
Defines the steepness of the rise around the landmark
Above example σ2 = 1
Below σ2 = 0.5

We see here that as you move away from 3,5 the feature f1 falls to zero much more rapidly
The inverse can be seen if σ2 = 3

Given this definition, what kinds of hypotheses can we learn?
With training examples x we predict "1" when
θ0+ θ1f1+ θ2f2 + θ3f3 >= 0
For our example, lets say we've already run an algorithm and got the
θ0 = -0.5
θ1 = 1
θ2 = 1
θ3 = 0
Given our placement of three examples, what happens if we evaluate an example at the
magenta dot below?

Looking at our formula, we know f1 will be close to 1, but f2 and f3 will be close to 0
So if we look at the formula we have
θ0+ θ1f1+ θ2f2 + θ3f3 >= 0
-0.5 + 1 + 0 + 0 = 0.5
0.5 is greater than 1

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

16/20

10/5/2020

12_Support_Vector_Machines

If we had another point far away from all three

This equates to -0.5
So we predict 0
Considering our parameter, for points near l1 and l2 you predict 1, but for points near l3 you predict
0
Which means we create a non-linear decision boundary that goes a lil' something like this;

Inside we predict y = 1
Outside we predict y = 0
So this show how we can create a non-linear boundary with landmarks and the kernel function in the
support vector machine
But
How do we get/chose the landmarks
What other kernels can we use (other than the Gaussian kernel)

Kernels II
Filling in missing detail and practical implications regarding kernels
Spoke about picking landmarks manually, defining the kernel, and building a hypothesis function
Where do we get the landmarks from?
For complex problems we probably want lots of them
Choosing the landmarks
Take the training data
For each example place a landmark at exactly the same location
So end up with m landmarks
One landmark per location per training example
Means our features measure how close to a training set example something is
Given a new example, compute all the f values
Gives you a feature vector f (f0 to fm)
f0 = 1 always
A more detailed look at generating the f vector
If we had a training example - features we compute would be using (xi, yi)
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

17/20

10/5/2020

12_Support_Vector_Machines

So we just cycle through each landmark, calculating how close to that landmark actually xi is
f1i, = k(xi, l1)
f2i, = k(xi, l2)
...
fmi, = k(xi, lm)

Somewhere in the list we compare x to itself... (i.e. when we're at fii)
So because we're using the Gaussian Kernel this evalues to 1
Take these m features (f1, f2 ... fm) group them into an [m +1 x 1] dimensional vector called f

fi is the f feature vector for the ith example
And add a 0th term = 1
Given these kernels, how do we use a support vector machine
SVM hypothesis prediction with kernels

Predict y = 1 if (θT f) >= 0
Because θ = [m+1 x 1]
And f = [m +1 x 1]
So, this is how you make a prediction assuming you already have θ
How do you get θ?
SVM training with kernels
Use the SVM learning algorithm

Now, we minimize using f as the feature vector instead of x
By solving this minimization problem you get the parameters for your SVM
In this setup, m = n
Because number of features is the number of training data examples we have
One final mathematic detail (not crucial to understand)
If we ignore θ0 then the following is true

What many implementations do is
Where the matrix M depends on the kernel you use
Gives a slightly different minimization - means we determine a rescaled version of θ
Allows more efficient computation, and scale to much bigger training sets
If you have a training set with 10 000 values, means you get 10 000 features
Solving for all these parameters can become expensive
So by adding this in we avoid a for loop and use a matrix multiplication algorithm instead
You can apply kernels to other algorithms
But they tend to be very computationally expensive
But the SVM is far more efficient - so more practical
Lots of good off the shelf software to minimize this function
SVM parameters (C)
Bias and variance trade off
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

18/20

10/5/2020

12_Support_Vector_Machines

Must chose C
C plays a role similar to 1/LAMBDA (where LAMBDA is the regularization parameter)
Large C gives a hypothesis of low bias high variance --> overfitting
Small C gives a hypothesis of high bias low variance --> underfitting
SVM parameters (σ2)
Parameter for calculating f values
Large σ2 - f features vary more smoothly - higher bias, lower variance
Small σ2 - f features vary abruptly - low bias, high variance

SVM - implementation and use
So far spoken about SVM in a very abstract manner
What do you need to do this
Use SVM software packages (e.g. liblinear, libsvm) to solve parameters θ
Need to specify
Choice of parameter C
Choice of kernel
Choosing a kernel
We've looked at the Gaussian kernel
Need to define σ (σ2)
Discussed σ2
When would you chose a Gaussian?
If n is small and/or m is large
e.g. 2D training set that's large
If you're using a Gaussian kernel then you may need to implement the kernel function
e.g. a function
fi = kernel(x1,x2)
Returns a real number
Some SVM packages will expect you to define kernel
Although, some SVM implementations include the Gaussian and a few others
Gaussian is probably most popular kernel
NB - make sure you perform feature scaling before using a Gaussian kernel
If you don't features with a large value will dominate the f value
Could use no kernel - linear kernel
Predict y = 1 if (θT x) >= 0
So no f vector
Get a standard linear classifier
Why do this?
If n is large and m is small then
Lots of features, few examples
Not enough data - risk overfitting in a high dimensional feature-space
Other choice of kernel
Linear and Gaussian are most common
Not all similarity functions you develop are valid kernels
Must satisfy Merecer's Theorem
SVM use numerical optimization tricks
Mean certain optimizations can be made, but they must follow the theorem
Polynomial Kernel
We measure the similarity of x and l by doing one of
(xT l)2
(xT l)3
(xT l+1)3
www.holehouse.org/mlclass/12_Support_Vector_Machines.html

19/20

10/5/2020

12_Support_Vector_Machines

General form is
(xT l+Con)D
If they're similar then the inner product tends to be large
Not used that often
Two parameters
Degree of polynomial (D)
Number you add to l (Con)
Usually performs worse than the Gaussian kernel
Used when x and l are both non-negative
String kernel
Used if input is text strings
Use for text classification
Chi-squared kernel
Histogram intersection kernel
Multi-class classification for SVM
Many packages have built in multi-class classification packages
Otherwise use one-vs all method
Not a big issue
Logistic regression vs. SVM
When should you use SVM and when is logistic regression more applicable
If n (features) is large vs. m (training set)
e.g. text classification problem
Feature vector dimension is 10 000
Training set is 10 - 1000
Then use logistic regression or SVM with a linear kernel
If n is small and m is intermediate
n = 1 - 1000
m = 10 - 10 000
Gaussian kernel is good
If n is small and m is large
n = 1 - 1000
m = 50 000+
SVM will be slow to run with Gaussian kernel
In that case
Manually create or add more features
Use logistic regression of SVM with a linear kernel
Logistic regression and SVM with a linear kernel are pretty similar
Do similar things
Get similar performance
A lot of SVM's power is using diferent kernels to learn complex non-linear functions
For all these regimes a well designed NN should work
But, for some of these problems a NN might be slower - SVM well implemented would be faster
SVM has a convex optimization problem - so you get a global minimum
It's not always clear how to chose an algorithm
Often more important to get enough data
Designing new features
Debugging the algorithm
SVM is widely perceived a very powerful learning algorithm

www.holehouse.org/mlclass/12_Support_Vector_Machines.html

20/20

10/5/2020

13_Clustering

13: Clustering
Previous Next Index

Unsupervised learning - introduction
Talk about clustering
Learning from unlabeled data
Unsupervised learning
Useful to contras with supervised learning
Compare and contrast
Supervised learning
Given a set of labels, fit a hypothesis to it
Unsupervised learning
Try and determining structure in the data
Clustering algorithm groups data together based on data features
What is clustering good for
Market segmentation - group customers into different market segments
Social network analysis - Facebook "smartlists"
Organizing computer clusters and data centers for network layout and location
Astronomical data analysis - Understanding galaxy formation

K-means algorithm
Want an algorithm to automatically group the data into coherent clusters
K-means is by far the most widely used clustering algorithm
Overview
Take unlabeled data and group into two clusters

Algorithm overview
1) Randomly allocate two points as the cluster centroids
Have as many cluster centroids as clusters you want to do (K cluster centroids, in fact)
In our example we just have two clusters
2) Cluster assignment step
Go through each example and depending on if it's closer to the red or blue centroid assign each point to one of
the two clusters
To demonstrate this, we've gone through the data and "colour" each point red or blue

www.holehouse.org/mlclass/13_Clustering.html

1/6

10/5/2020

13_Clustering

3) Move centroid step
Take each centroid and move to the average of the correspondingly assigned data-points

Repeat 2) and 3) until convergence
More formal definition
Input:
K (number of clusters in the data)
Training set {x1, x2, x3 ..., xn)
Algorithm:
Randomly initialize K cluster centroids as {µ1, µ2, µ3 ... µK}

Loop 1
This inner loop repeatedly sets the c(i) variable to be the index of the closes variable of cluster
centroid closes to xi
i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the
cluster closest

Loop 2
Loops over each centroid calculate the average mean based on all the points associated with
each centroid from c(i)
What if there's a centroid with no data
Remove that centroid, so end up with K-1 classes
Or, randomly reinitialize it
Not sure when though...
K-means for non-separated clusters
So far looking at K-means where we have well defined clusters
But often K-means is applied to datasets where there aren't well defined clusters

www.holehouse.org/mlclass/13_Clustering.html

2/6

10/5/2020

13_Clustering

e.g. T-shirt sizing

Not obvious discrete groups
Say you want to have three sizes (S,M,L) how big do you make these?
One way would be to run K-means on this data
May do the following

So creates three clusters, even though they aren't really there
Look at first population of people
Try and design a small T-shirt which fits the 1st population
And so on for the other two
This is an example of market segmentation
Build products which suit the needs of your subpopulations

K means optimization objective
Supervised learning algorithms have an optimization objective (cost function)
K-means does too
K-means has an optimization objective like the supervised learning functions we've seen
Why is this good?
Knowing this is useful because it helps for debugging
Helps find better clusters
While K-means is running we keep track of two sets of variables
ci is the index of clusters {1,2, ..., K} to which xi is currently assigned
i.e. there are m ci values, as each example has a ci value, and that value is one the the clusters (i.e. can only be
one of K different values)
µk, is the cluster associated with centroid k
Locations of cluster centroid k
www.holehouse.org/mlclass/13_Clustering.html

3/6

10/5/2020

13_Clustering

So there are K
So these the centroids which exist in the training data space
µci, is the cluster centroid of the cluster to which example xi has been assigned to
This is more for convenience than anything else
You could look up that example i is indexed to cluster j (using the c vector), where j is between 1 and K
Then look up the value associated with cluster j in the µ vector (i.e. what are the features associated
with µj)
But instead, for easy description, we have this variable which gets exactly the same value
Lets say xi as been assigned to cluster 5
Means that
ci = 5
µci, = µ5
Using this notation we can write the optimization objective;

i.e. squared distances between training example xi and the cluster centroid to which xi has been assigned to
This is just what we've been doing, as the visual description below shows;

The red line here shows the distances between the example xi and the cluster to which that example has been
assigned
Means that when the example is very close to the cluster, this value is small
When the cluster is very far away from the example, the value is large
This is sometimes called the distortion (or distortion cost function)
So we are finding the values which minimizes this function;

If we consider the k-means algorithm
The cluster assigned step is minimizing J(...) with respect to c1, c2 ... ci
i.e. find the centroid closest to each example
Doesn't change the centroids themselves
The move centroid step
We can show this step is choosing the values of µ which minimizes J(...) with respect to µ
So, we're partitioning the algorithm into two parts
First part minimizes the c variables
Second part minimizes the J variables
We can use this knowledge to help debug our K-means algorithm
Random initialization
How we initialize K-means
And how avoid local optimum
Consider clustering algorithm
Never spoke about how we initialize the centroids
A few ways - one method is most recommended
Have number of centroids set to less than number of examples (K < m) (if K > m we have a problem)0
Randomly pick K training examples
Set µ1 up to µK to these example's values
K means can converge to different solutions depending on the initialization setup

www.holehouse.org/mlclass/13_Clustering.html

4/6

10/5/2020

13_Clustering

Risk of local optimum

The local optimum are valid convergence, but local optimum not global ones
If this is a concern
We can do multiple random initializations
See if we get the same result - many same results are likely to indicate a global optimum
Algorithmically we can do this as follows;

A typical number of times to initialize K-means is 50-1000
Randomly initialize K-means
For each 100 random initialization run K-means
Then compute the distortion on the set of cluster assignments and centroids at convergent
End with 100 ways of cluster the data
Pick the clustering which gave the lowest distortion
If you're running K means with 2-10 clusters can help find better global optimum
If K is larger than 10, then multiple random initializations are less likely to be necessary
First solution is probably good enough (better granularity of clustering)

How do we choose the number of clusters?
Choosing K?
Not a great way to do this automatically
Normally use visualizations to do it manually
What are the intuitions regarding the data?
Why is this hard
Sometimes very ambiguous
e.g. two clusters or four clusters
Not necessarily a correct answer
This is why doing it automatic this is hard
Elbow method
Vary K and compute cost function at a range of K values
As K increases J(...) minimum value should decrease (i.e. you decrease the granularity so centroids can better optimize)
www.holehouse.org/mlclass/13_Clustering.html

5/6

10/5/2020

13_Clustering

Plot this (K vs J())
Look for the "elbow" on the graph

Chose the "elbow" number of clusters
If you get a nice plot this is a reasonable way of choosing K
Risks
Normally you don't get a a nice line -> no clear elbow on curve
Not really that helpful
Another method for choosing K
Using K-means for market segmentation
Running K-means for a later/downstream purpose
See how well different number of clusters serve you later needs
e.g.
T-shirt size example
If you have three sizes (S,M,L)
Or five sizes (XS, S, M, L, XL)
Run K means where K = 3 and K = 5
How does this look

This gives a way to chose the number of clusters
Could consider the cost of making extra sizes vs. how well distributed the products are
How important are those sizes though? (e.g. more sizes might make the customers happier)
So applied problem may help guide the number of clusters

www.holehouse.org/mlclass/13_Clustering.html

6/6

10/5/2020

14_Dimensionality_Reduction

14: Dimensionality Reduction (PCA)
Previous Next Index

Motivation 1: Data compression
Start talking about a second type of unsupervised learning problem - dimensionality reduction
Why should we look at dimensionality reduction?
Compression
Speeds up algorithms
Reduces space used by data for them
What is dimensionality reduction?
So you've collected many features - maybe more than you need
Can you "simply" your data set in a rational and useful way?
Example
Redundant data set - different units for same attribute
Reduce data to 1D (2D->1D)

Example above isn't a perfect straight line because of round-off error
Data redundancy can happen when different teams are working independently
Often generates redundant data (especially if you don't control data collection)
Another example
Helicopter flying - do a survey of pilots (x1 = skill, x2 = pilot enjoyment)
These features may be highly correlated
This correlation can be combined into a single attribute called aptitude (for example)
What does dimensionality reduction mean?
In our example we plot a line
Take exact example and record position on that line

So before x1 was a 2D feature vector (X and Y dimensions)
www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

1/10

10/5/2020

14_Dimensionality_Reduction

Now we can represent x1 as a 1D number (Z dimension)
So we approximate original examples
Allows us to half the amount of storage
Gives lossy compression, but an acceptable loss (probably)
The loss above comes from the rounding error in the measurement, however
Another example 3D -> 2D
So here's our data

Maybe all the data lies in one plane
This is sort of hard to explain in 2D graphics, but that plane may be aligned with one of the axis
Or or may not...
Either way, the plane is a small, a constant 3D space
In the diagram below, imagine all our data points are sitting "inside" the blue tray (has a dark blue exterior
face and a light blue inside)

Because they're all in this relative shallow area, we can basically ignore one of the dimension, so we draw two
new lines (z1 and z2) along the x and y planes of the box, and plot the locations in that box
i.e. we loose the data in the z-dimension of our "shallow box" (NB "z-dimensions" here refers to the
dimension relative to the box (i.e it's depth) and NOT the z dimension of the axis we've got drawn above) but
because the box is shallow it's OK to lose this. Probably....

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

2/10

10/5/2020

14_Dimensionality_Reduction

Plot values along those projections

So we've now reduced our 3D vector to a 2D vector
In reality we'd normally try and do 1000D -> 100D

Motivation 2: Visualization
It's hard to visualize highly dimensional data
Dimensionality reduction can improve how we display information in a tractable manner for human consumption
Why do we care?
Often helps to develop algorithms if we can understand our data better
Dimensionality reduction helps us do this, see data in a helpful
Good for explaining something to someone if you can "show" it in the data
Example;
Collect a large data set about many facts of a country around the world

So
x1 = GDP
...
x6 = mean household
Say we have 50 features per country
How can we understand this data better?
Very hard to plot 50 dimensional data
Using dimensionality reduction, instead of each country being represented by a 50-dimensional feature vector

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

3/10

10/5/2020

14_Dimensionality_Reduction

Come up with a different feature representation (z values) which summarize these features

This gives us a 2-dimensional vector
Reduce 50D -> 2D
Plot as a 2D plot
Typically you don't generally ascribe meaning to the new features (so we have to determine what these summary
values mean)
e.g. may find horizontal axis corresponds to overall country size/economic activity
and y axis may be the per-person well being/economic activity
So despite having 50 features, there may be two "dimensions" of information, with features associated with each
of those dimensions
It's up to you to asses what of the features can be grouped to form summary features, and how best to do that
(feature scaling is probably important)
Helps show the two main dimensions of variation in a way that's easy to understand

Principle Component Analysis (PCA): Problem Formulation
For the problem of dimensionality reduction the most commonly used algorithm is PCA
Here, we'll start talking about how we formulate precisely what we want PCA to do
So
Say we have a 2D data set which we wish to reduce to 1D

In other words, find a single line onto which to project this data
How do we determine this line?
The distance between each point and the projected version should be small (blue lines below are short)
www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

4/10

10/5/2020

14_Dimensionality_Reduction

PCA tries to find a lower dimensional surface so the sum of squares onto that surface is minimized
The blue lines are sometimes called the projection error
PCA tries to find the surface (a straight line in this case) which has the minimum projection error

As an aside, you should normally do mean normalization and feature scaling on your data before
PCA
A more formal description is
For 2D-1D, we must find a vector u(1), which is of some dimensionality
Onto which you can project the data so as to minimize the projection error

u(1) can be positive or negative (-u(1)) which makes no difference
Each of the vectors define the same red line
In the more general case
To reduce from nD to kD we
Find k vectors (u(1), u(2), ... u(k)) onto which to project the data to minimize the projection error
So lots of vectors onto which we project the data
Find a set of vectors which we project the data onto the linear subspace spanned by that set of vectors
We can define a point in a plane with k vectors
e.g. 3D->2D
Find pair of vectors which define a 2D plane (surface) onto which you're going to project your data
Much like the "shallow box" example in compression, we're trying to create the shallowest box possible (by
defining two of it's three dimensions, so the box' depth is minimized)

How does PCA relate to linear regression?
PCA is not linear regression
www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

5/10

10/5/2020

14_Dimensionality_Reduction

Despite cosmetic similarities, very different
For linear regression, fitting a straight line to minimize the straight line between a point and a squared line
NB - VERTICAL distance between point
For PCA minimizing the magnitude of the shortest orthogonal distance
Gives very different effects
More generally
With linear regression we're trying to predict "y"
With PCA there is no "y" - instead we have a list of features and all features are treated equally
If we have 3D dimensional data 3D->2D
Have 3 features treated symmetrically

PCA Algorithm
Before applying PCA must do data preprocessing
Given a set of m unlabeled examples we must do
Mean normalization
Replace each xji with xj - µj,
In other words, determine the mean of each feature set, and then for each feature subtract the
mean from the value, so we re-scale the mean to be 0
Feature scaling (depending on data)
If features have very different scales then scale so they all have a comparable range of values
e.g. xji is set to (xj - µj) / sj
Where sj is some measure of the range, so could be
Biggest - smallest
Standard deviation (more commonly)
With preprocessing done, PCA finds the lower dimensional sub-space which minimizes the sum of the square
In summary, for 2D->1D we'd be doing something like this;

Need to compute two things;
Compute the u vectors
The new planes
Need to compute the z vectors
z vectors are the new, lower dimensionality feature vectors
A mathematical derivation for the u vectors is very complicated
But once you've done it, the procedure to find each u vector is not that hard
Algorithm description
Reducing data from n-dimensional to k-dimensional
Compute the covariance matrix

This is commonly denoted as Σ (greek upper case sigma) - NOT summation symbol
Σ = sigma
This is an [n x n] matrix
Remember than xi is a [n x 1] matrix

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

6/10

10/5/2020

14_Dimensionality_Reduction

In MATLAB or octave we can implement this as follows;

Compute eigenvectors of matrix Σ
[U,S,V] = svd(sigma)
svd = singular value decomposition
More numerically stable than eig
eig = also gives eigenvector
U,S and V are matrices
U matrix is also an [n x n] matrix
Turns out the columns of U are the u vectors we want!
So to reduce a system from n-dimensions to k-dimensions
Just take the first k-vectors from U (first k columns)

Next we need to find some way to change x (which is n dimensional) to z (which is k dimensional)
(reduce the dimensionality)
Take first k columns of the u matrix and stack in columns
n x k matrix - call this Ureduce
We calculate z as follows
z = (Ureduce)T * x
So [k x n] * [n x 1]
Generates a matrix which is
k*1
If that's not witchcraft I don't know what is!
Exactly the same as with supervised learning except we're now doing it with unlabeled data
So in summary
Preprocessing
Calculate sigma (covariance matrix)
Calculate eigenvectors with svd
Take k vectors from U (Ureduce= U(:,1:k);)
Calculate z (z =Ureduce' * x;)
No mathematical derivation
Very complicated
But it works

Reconstruction from Compressed Representation
Earlier spoke about PCA as a compression algorithm
If this is the case, is there a way to decompress the data from low dimensionality back to a higher dimensionality
format?
Reconstruction

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

7/10

10/5/2020

14_Dimensionality_Reduction

Say we have an example as follows

We have our examples (x1, x2 etc.)
Project onto z-surface
Given a point z1, how can we go back to the 2D space?
Considering
z (vector) = (Ureduce)T * x
To go in the opposite direction we must do
xapprox = Ureduce * z
To consider dimensions (and prove this really works)
Ureduce = [n x k]
z [k * 1]
So
xapprox = [n x 1]
So this creates the following representation

We lose some of the information (i.e. everything is now perfectly on that line) but it is now projected into 2D space

Choosing the number of Principle Components
How do we chose k ?
k = number of principle components
Guidelines about how to chose k for PCA
To chose k think about how PCA works

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

8/10

10/5/2020

14_Dimensionality_Reduction

PCA tries to minimize averaged squared projection error

Total variation in data can be defined as the average over data saying how far are the training examples from the
origin

When we're choosing k typical to use something like this

Ratio between averaged squared projection error with total variation in data
Want ratio to be small - means we retain 99% of the variance
If it's small (0) then this is because the numerator is small
The numerator is small when xi = xapproxi
i.e. we lose very little information in the dimensionality reduction, so when we decompress we
regenerate the same data
So we chose k in terms of this ratio
Often can significantly reduce data dimensionality while retaining the variance
How do you do this

Advice for Applying PCA
Can use PCA to speed up algorithm running time
Explain how
And give general advice
Speeding up supervised learning algorithms
Say you have a supervised learning problem
Input x and y
x is a 10 000 dimensional feature vector
e.g. 100 x 100 images = 10 000 pixels
Such a huge feature vector will make the algorithm slow
With PCA we can reduce the dimensionality and make it tractable
How
1) Extract xs
So we now have an unlabeled training set
2) Apply PCA to x vectors
So we now have a reduced dimensional feature vector z
3) This gives you a new training set
Each vector can be re-associated with the label
4) Take the reduced dimensionality data set and feed to a learning algorithm
Use y as labels and z as feature vector
www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

9/10

10/5/2020

14_Dimensionality_Reduction

5) If you have a new example map from higher dimensionality vector to lower dimensionality vector, then
feed into learning algorithm
PCA maps one vector to a lower dimensionality vector
x -> z
Defined by PCA only on the training set
The mapping computes a set of parameters
Feature scaling values
Ureduce
Parameter learned by PCA
Should be obtained only by determining PCA on your training set
So we use those learned parameters for our
Cross validation data
Test set
Typically you can reduce data dimensionality by 5-10x without a major hit to algorithm

Applications of PCA
Compression
Why
Reduce memory/disk needed to store data
Speed up learning algorithm
How do we chose k?
% of variance retained
Visualization
Typically chose k =2 or k = 3
Because we can plot these values!
One thing often done wrong regarding PCA
A bad use of PCA: Use it to prevent over-fitting
Reasoning
If we have xi we have n features, zi has k features which can be lower
If we only have k features then maybe we're less likely to over fit...
This doesn't work
BAD APPLICATION
Might work OK, but not a good way to address over fitting
Better to use regularization
PCA throws away some data without knowing what the values it's losing
Probably OK if you're keeping most of the data
But if you're throwing away some crucial data bad
So you have to go to like 95-99% variance retained
So here regularization will give you AT LEAST as good a way to solve over fitting
A second PCA myth
Used for compression or visualization - good
Sometimes used
Design ML system with PCA from the outset
But, what if you did the whole thing without PCA
See how a system performs without PCA
ONLY if you have a reason to believe PCA will help should you then add PCA
PCA is easy enough to add on as a processing step
Try without first!

www.holehouse.org/mlclass/14_Dimensionality_Reduction.html

10/10

10/5/2020

15_Anomaly_Detection

15: Anomaly Detection
Previous Next Index

Anomaly detection - problem motivation
Anomaly detection is a reasonably commonly used type of machine learning application
Can be thought of as a solution to an unsupervised learning problem
But, has aspects of supervised learning
What is anomaly detection?
Imagine you're an aircraft engine manufacturer
As engines roll off your assembly line you're doing QA
Measure some features from engines (e.g. heat generated and vibration)
You now have a dataset of x1 to xm (i.e. m engines were tested)
Say we plot that dataset

0
Next day you have a new engine
An anomaly detection method is used to see if the new engine is anomalous (when compared to the previous engines)
If the new engine looks like this;

Probably OK - looks like the ones we've seen before
But if the engine looks like this

Uh oh! - this looks like an anomalous data-point
More formally
We have a dataset which contains normal (data)
How we ensure they're normal is up to us
In reality it's OK if there are a few which aren't actually normal
Using that dataset as a reference point we can see if other examples are anomalous
How do we do this?
First, using our training dataset we build a model
www.holehouse.org/mlclass/15_Anomaly_Detection.html

1/13

10/5/2020

15_Anomaly_Detection

We can access this model using p(x)
This asks, "What is the probability that example x is normal"
Having built a model
if p(xtest) < ε --> flag this as an anomaly
if p(xtest) >= ε --> this is OK
ε is some threshold probability value which we define, depending on how sure we need/want to be
We expect our model to (graphically) look something like this;

i.e. this would be our model if we had 2D data
Applications
Fraud detection
Users have activity associated with them, such as
Length on time on-line
Location of login
Spending frequency
Using this data we can build a model of what normal users' activity is like
What is the probability of "normal" behavior?
Identify unusual users by sending their data through the model
Flag up anything that looks a bit weird
Automatically block cards/transactions
Manufacturing
Already spoke about aircraft engine example
Monitoring computers in data center
If you have many machines in a cluster
Computer features of machine
x1 = memory use
x2 = number of disk accesses/sec
x3 = CPU load
In addition to the measurable features you can also define your own complex features
x4 = CPU load/network traffic
If you see an anomalous machine
Maybe about to fail
Look at replacing bits from it

The Gaussian distribution (optional)
Also called the normal distribution
Example
Say x (data set) is made up of real numbers
Mean is µ
Variance is σ2
σ is also called the standard deviation - specifies the width of the Gaussian probability
The data has a Gaussian distribution
Then we can write this ~ N(µ,σ2 )
~ means = is distributed as
N (should really be "script" N (even curlier!) -> means normal distribution
µ, σ2 represent the mean and variance, respectively
These are the two parameters a Gaussian means

www.holehouse.org/mlclass/15_Anomaly_Detection.html

2/13

10/5/2020

15_Anomaly_Detection

Looks like this;

This specifies the probability of x taking a value
As you move away from µ
Gaussian equation is
P(x : µ , σ2) (probability of x, parameterized by the mean and squared variance)

Some examples of Gaussians below
Area is always the same (must = 1)
But width changes as standard deviation changes

Parameter estimation problem
What is it?
Say we have a data set of m examples
Give each example is a real number - we can plot the data on the x axis as shown below

Problem is - say you suspect these examples come from a Gaussian
Given the dataset can you estimate the distribution?

www.holehouse.org/mlclass/15_Anomaly_Detection.html

3/13

10/5/2020

15_Anomaly_Detection

Could be something like this

Seems like a reasonable fit - data seems like a higher probability of being in the central region, lower probability of
being further away
Estimating µ and σ2
µ = average of examples
σ2 = standard deviation squared

As a side comment
These parameters are the maximum likelihood estimation values for µ and σ2
You can also do 1/(m) or 1/(m-1) doesn't make too much difference
Slightly different mathematical problems, but in practice it makes little difference

Anomaly detection algorithm
Unlabeled training set of m examples
Data = {x1, x2, ..., xm }
Each example is an n-dimensional vector (i.e. a feature vector)
We have n features!
Model P(x) from the data set
What are high probability features and low probability features
x is a vector
So model p(x) as
= p(x1; µ1 , σ12) * p(x2; µ2 , σ22) * ... p(xn ; µn , σn2)
Multiply the probability of each features by each feature
We model each of the features by assuming each feature is distributed according to a Gaussian distribution
p(xi; µi , σi2)

The probability of feature xi given µi and σi2, using a Gaussian distribution
As a side comment
Turns out this equation makes an independence assumption for the features, although algorithm works if features are
independent or not
Don't worry too much about this, although if you're features are tightly linked you should be able to do some
dimensionality reduction anyway!
We can write this chain of multiplication more compactly as follows;

Capital PI (Π) is the product of a set of values
The problem of estimation this distribution is sometimes call the problem of density estimation
Algorithm

www.holehouse.org/mlclass/15_Anomaly_Detection.html

4/13

10/5/2020

15_Anomaly_Detection

1 - Chose features
Try to come up with features which might help identify something anomalous - may be unusually large or small values
More generally, chose features which describe the general properties
This is nothing unique to anomaly detection - it's just the idea of building a sensible feature vector
2 - Fit parameters
Determine parameters for each of your examples µi and σi2
Fit is a bit misleading, really should just be "Calculate parameters for 1 to n"
So you're calculating standard deviation and mean for each feature
You should of course used some vectorized implementation rather than a loop probably
3 - compute p(x)
You compute the formula shown (i.e. the formula for the Gaussian probability)
If the number is very small, very low chance of it being "normal"
Anomaly detection example
x1
Mean is about 5
Standard deviation looks to be about 2
x2
Mean is about 3
Standard deviation about 1
So we have the following system

If we plot the Gaussian for x1 and x2 we get something like this

www.holehouse.org/mlclass/15_Anomaly_Detection.html

5/13

10/5/2020

15_Anomaly_Detection

If you plot the product of these things you get a surface plot like this

With this surface plot, the height of the surface is the probability - p(x)
We can't always do surface plots, but for this example it's quite a nice way to show the probability of a 2D feature vector
Check if a value is anomalous
Set epsilon as some value
Say we have two new data points new data-point has the values
x1test

x2test
We compute
p(x1test) = 0.436 >= epsilon (~40% chance it's normal)
Normal
p(x2test) = 0.0021 < epsilon (~0.2% chance it's normal)
Anomalous
What this is saying is if you look at the surface plot, all values above a certain height are normal, all the values below that
threshold are probably anomalous

Developing and evaluating and anomaly detection system
Here talk about developing a system for anomaly detection
How to evaluate an algorithm
Previously we spoke about the importance of real-number evaluation
Often need to make a lot of choices (e.g. features to use)
Easier to evaluate your algorithm if it returns a single number to show if changes you made improved or worsened an
algorithm's performance
To develop an anomaly detection system quickly, would be helpful to have a way to evaluate your algorithm
Assume we have some labeled data
So far we've been treating anomalous detection with unlabeled data
If you have labeled data allows evaluation
i.e. if you think something iss anomalous you can be sure if it is or not
So, taking our engine example
You have some labeled data
Data for engines which were non-anomalous -> y = 0
Data for engines which were anomalous -> y = 1
Training set is the collection of normal examples
OK even if we have a few anomalous data examples
Next define
Cross validation set
Test set
For both assume you can include a few examples which have anomalous examples
Specific example
Engines
Have 10 000 good engines
OK even if a few bad ones are here...
LOTS of y = 0
20 flawed engines
Typically when y = 1 have 2-50
Split into
Training set: 6000 good engines (y = 0)
CV set: 2000 good engines, 10 anomalous
Test set: 2000 good engines, 10 anomalous
Ratio is 3:1:1
Sometimes we see a different way of splitting
Take 6000 good in training
Same CV and test set (4000 good in each) different 10 anomalous,
Or even 20 anomalous (same ones)
This is bad practice - should use different data in CV and test set
Algorithm evaluation
Take trainings set { x1, x2, ..., xm }
Fit model p(x)
www.holehouse.org/mlclass/15_Anomaly_Detection.html

6/13

10/5/2020

15_Anomaly_Detection

On cross validation and test set, test the example x
y = 1 if p(x) < epsilon (anomalous)
y = 0 if p(x) >= epsilon (normal)
Think of algorithm a trying to predict if something is anomalous
But you have a label so can check!
Makes it look like a supervised learning algorithm
What's a good metric to use for evaluation
y = 0 is very common
So classification would be bad
Compute fraction of true positives/false positive/false negative/true negative
Compute precision/recall
Compute F1-score
Earlier, also had epsilon (the threshold value)
Threshold to show when something is anomalous
If you have CV set you can see how varying epsilon effects various evaluation metrics
Then pick the value of epsilon which maximizes the score on your CV set
Evaluate algorithm using cross validation
Do final algorithm evaluation on the test set

Anomaly detection vs. supervised learning
If we have labeled data, we not use a supervised learning algorithm?
Here we'll try and understand when you should use supervised learning and when anomaly detection would be better
Anomaly detection
Very small number of positive examples
Save positive examples just for CV and test set
Consider using an anomaly detection algorithm
Not enough data to "learn" positive examples
Have a very large number of negative examples
Use these negative examples for p(x) fitting
Only need negative examples for this
Many "types" of anomalies
Hard for an algorithm to learn from positive examples when anomalies may look nothing like one another
So anomaly detection doesn't know what they look like, but knows what they don't look like
When we looked at SPAM email,
Many types of SPAM
For the spam problem, usually enough positive examples
So this is why we usually think of SPAM as supervised learning
Application and why they're anomaly detection
Fraud detection
Many ways you may do fraud
If you're a major on line retailer/very subject to attacks, sometimes might shift to supervised learning
Manufacturing
If you make HUGE volumes maybe have enough positive data -> make supervised
Means you make an assumption about the kinds of errors you're going to see
It's the unknown unknowns we don't like!
Monitoring machines in data
Supervised learning
Reasonably large number of positive and negative examples
Have enough positive examples to give your algorithm the opportunity to see what they look like
If you expect anomalies to look anomalous in the same way
Application
Email/SPAM classification
Weather prediction
Cancer classification

Choosing features to use
One of the things which has a huge effect is which features are used
Non-Gaussian features
Plot a histogram of data to check it has a Gaussian description - nice sanity check
Often still works if data is non-Gaussian
Use hist command to plot histogram

www.holehouse.org/mlclass/15_Anomaly_Detection.html

7/13

10/5/2020

15_Anomaly_Detection

Non-Gaussian data might look like this

Can play with different transformations of the data to make it look more Gaussian
Might take a log transformation of the data
i.e. if you have some feature x1, replace it with log(x1)

This looks much more Gaussian
Or do log(x1+c)
Play with c to make it look as Gaussian as possible
Or do x1/2
Or do x1/3
Error analysis for anomaly detection
Good way of coming up with features
Like supervised learning error analysis procedure
Run algorithm on CV set
See which one it got wrong
Develop new features based on trying to understand why the algorithm got those examples wrong
Example
p(x) large for normal, p(x) small for abnormal
e.g.

Here we have one dimension, and our anomalous value is sort of buried in it (in green - Gaussian superimposed in blue)
Look at data - see what went wrong
Can looking at that example help develop a new feature (x2) which can help distinguish further anomalous
Example - data center monitoring
Features
x1 = memory use
x2 = number of disk access/sec
x3 = CPU load
x4 = network traffic
We suspect CPU load and network traffic grow linearly with one another
If server is serving many users, CPU is high and network is high
Fail case is infinite loop, so CPU load grows but network traffic is low
New feature - CPU load/network traffic
May need to do feature scaling

Multivariate Gaussian distribution
Is a slightly different technique which can sometimes catch some anomalies which non-multivariate Gaussian distribution anomaly
detection fails to
www.holehouse.org/mlclass/15_Anomaly_Detection.html

8/13

10/5/2020

15_Anomaly_Detection

Unlabeled data looks like this

Say you can fit a Gaussian distribution to CPU load and memory use
Lets say in the test set we have an example which looks like an anomaly (e.g. x1 = 0.4, x2 = 1.5)
Looks like most of data lies in a region far away from this example
Here memory use is high and CPU load is low (if we plot x1 vs. x2 our green example looks miles away from the
others)
Problem is, if we look at each feature individually they may fall within acceptable limits - the issue is we know we shouldn't don't
get those kinds of values together
But individually, they're both acceptable

This is because our function makes probability prediction in concentric circles around the the means of both

www.holehouse.org/mlclass/15_Anomaly_Detection.html

9/13

10/5/2020

15_Anomaly_Detection

Probability of the two red circled examples is basically the same, even though we can clearly see the green one as an outlier
Doesn't understand the meaning
Multivariate Gaussian distribution model
To get around this we develop the multivariate Gaussian distribution
Model p(x) all in one go, instead of each feature separately
What are the parameters for this new model?
µ - which is an n dimensional vector (where n is number of features)
Σ - which is an [n x n] matrix - the covariance matrix
For the sake of completeness, the formula for the multivariate Gaussian distribution is as follows

NB don't memorize this - you can always look it up
What does this mean?
= absolute value of Σ (determinant of sigma)
This is a mathematic function of a matrix
You can compute it in MATLAB using det(sigma)
More importantly, what does this p(x) look like?
2D example

Sigma is sometimes call the identity matrix

p(x) looks like this
For inputs of x1 and x2 the height of the surface gives the value of p(x)
What happens if we change Sigma?

www.holehouse.org/mlclass/15_Anomaly_Detection.html

10/13

10/5/2020

15_Anomaly_Detection

So now we change the plot to

Now the width of the bump decreases and the height increases
If we set sigma to be different values this changes the identity matrix and we change the shape of our graph

Using these values we can, therefore, define the shape of this to better fit the data, rather than assuming symmetry in every dimension
One of the cool things is you can use it to model correlation between data
If you start to change the off-diagonal values in the covariance matrix you can control how well the various dimensions correlation

So we see here the final example gives a very tall thin distribution, shows a strong positive correlation
We can also make the off-diagonal values negative to show a negative correlation
Hopefully this shows an example of the kinds of distribution you can get by varying sigma
We can, of course, also move the mean (μ) which varies the peak of the distribution

Applying multivariate Gaussian distribution to anomaly detection
Saw some examples of the kinds of distributions you can model
Now let's take those ideas and look at applying them to different anomaly detection algorithms
www.holehouse.org/mlclass/15_Anomaly_Detection.html

11/13

10/5/2020

15_Anomaly_Detection

As mentioned, multivariate Gaussian modeling uses the following equation;

Which comes with the parameters µ and Σ
Where
µ - the mean (n-dimenisonal vector)
Σ - covariance matrix ([nxn] matrix)
Parameter fitting/estimation problem
If you have a set of examples
{x1, x2, ..., xm }
The formula for estimating the parameters is

Using these two formulas you get the parameters
Anomaly detection algorithm with multivariate Gaussian distribution
1) Fit model - take data set and calculate µ and Σ using the formula above
2) We're next given a new example (xtest) - see below

For it compute p(x) using the following formula for multivariate distribution

3) Compare the value with ε (threshold probability value)
if p(xtest) < ε --> flag this as an anomaly
if p(xtest) >= ε --> this is OK
If you fit a multivariate Gaussian model to our data we build something like this

Which means it's likely to identify the green value as anomalous
Finally, we should mention how multivariate Gaussian relates to our original simple Gaussian model (where each feature is looked
at individually)
Original model corresponds to multivariate Gaussian where the Gaussians' contours are axis aligned
i.e. the normal Gaussian model is a special case of multivariate Gaussian distribution
This can be shown mathematically

www.holehouse.org/mlclass/15_Anomaly_Detection.html

12/13

10/5/2020

15_Anomaly_Detection

Has this constraint that the covariance matrix sigma as ZEROs on the non-diagonal values

If you plug your variance values into the covariance matrix the models are actually identical
Original model vs. Multivariate Gaussian
Original Gaussian model
Probably used more often
There is a need to manually create features to capture anomalies where x1 and x2 take unusual combinations of values
So need to make extra features
Might not be obvious what they should be
This is always a risk - where you're using your own expectation of a problem to "predict" future anomalies
Typically, the things that catch you out aren't going to be the things you though of
If you thought of them they'd probably be avoided in the first place
Obviously this is a bigger issue, and one which may or may not be relevant depending on your problem space
Much cheaper computationally
Scales much better to very large feature vectors
Even if n = 100 000 the original model works fine
Works well even with a small training set
e.g. 50, 100
Because of these factors it's used more often because it really represents a optimized but axis-symmetric specialization of the general
model
Multivariate Gaussian model

Used less frequently
Can capture feature correlation
So no need to create extra values
Less computationally efficient
Must compute inverse of matrix which is [n x n]
So lots of features is bad - makes this calculation very expensive
So if n = 100 000 not very good
Needs for m > n
i.e. number of examples must be greater than number of features
If this is not true then we have a singular matrix (non-invertible)
So should be used only in m >> n
If you find the matrix is non-invertible, could be for one of two main reasons
m<n
So use original simple model
Redundant features (i.e. linearly dependent)
i.e. two features that are the same
If this is the case you could use PCA or sanity check your data

www.holehouse.org/mlclass/15_Anomaly_Detection.html

13/13

10/5/2020

16_Recommender_Systems

16: Recommender Systems
Previous Next Index

Recommender systems - introduction
Two motivations for talking about recommender systems
Important application of ML systems
Many technology companies find recommender systems to be absolutely key
Think about websites (amazon, Ebay, iTunes genius)
Try and recommend new content for you based on passed purchase
Substantial part of Amazon's revenue generation
Improvement in recommender system performance can bring in more income
Kind of a funny problem
In academic learning, recommender systems receives a small amount of attention
But in industry it's an absolutely crucial tool
Talk about the big ideas in machine learning
Not so much a technique, but an idea
As soon, features are really important
There's a big idea in machine learning that for some problems you can learn what a good set of
features are
So not select those features but learn them
Recommender systems do this - try and identify the crucial and relevant features
Example - predict movie ratings
You're a company who sells movies
You let users rate movies using a 1-5 star rating
To make the example nicer, allow 0-5 (makes math easier)
You have five movies
And you have four users
Admittedly, business isn't going well, but you're optimistic about the future as a result of your truly outstanding
(if limited) inventory

To introduce some notation
nu - Number of users (called ?nu occasionally as we can't subscript in superscript)
nm - Number of movies
r(i, j) - 1 if user j has rated movie i (i.e. bitmap)
y(i,j) - rating given by user j to move i (defined only if r(i,j) = 1
So for this example
nu = 4
nm = 5
Summary of scoring
Alice and Bob gave good ratings to rom coms, but low scores to action films
Carol and Dave game good ratings for action films but low ratings for rom coms
We have the data given above
www.holehouse.org/mlclass/16_Recommender_Systems.html

1/10

10/5/2020

16_Recommender_Systems

The problem is as follows
Given r(i,j) and y(i,j) - go through and try and predict missing values (?s)
Come up with a learning algorithm that can fill in these missing values

Content based recommendation
Using our example above, how do we predict?
For each movie we have a feature which measure degree to which each film is a
Romance (x1)
Action (x2)

If we have features like these, each film can be recommended by a feature vector
Add an extra feature which is x0 = 1 for each film
So for each film we have a [3 x 1] vector, which for film number 1 ("Love at Last") would be

i.e. for our dataset we have
{x1, x2, x3, x4, x5}
Where each of these is a [3x1] vector with an x0 = 1 and then a romance and an action score
To be consistent with our notation, n is going to be the number of features NOT counting the x0 term, so n =
2
We could treat each rating for each user as a separate linear regression problem
For each user j we could learn a parameter vector
Then predict that user j will rate movie i with
(θj)T xi = stars
inner product of parameter vector and features
So, lets take user 1 (Alice) and see what she makes of the modern classic Cute Puppies of Love (CPOL)
We have some parameter vector (θ1) associated with Alice
We'll explain later how we derived these values, but for now just take it that we have a vector

CPOL has a parameter vector (x3) associated with it

Our prediction will be equal to
(θ1)T x3 = (0 * 1) + (5 * 0.99) + (0 * 0)
= 4.95
Which may seem like a reasonable value
www.holehouse.org/mlclass/16_Recommender_Systems.html

2/10

10/5/2020

16_Recommender_Systems

All we're doing here is applying a linear regression method for each user
So we determine a future rating based on their interest in romance and action based on previous films
We should also add one final piece of notation
mj, - Number of movies rated by the user (j)
How do we learn (θj)
Create some parameters which give values as close as those seen in the data when applied

Sum over all values of i (all movies the user has used) when r(i,j) = 1 (i.e. all the films that the user has rated)
This is just like linear regression with least-squared error
We can also add a regularization term to make our equation look as follows

The regularization term goes from k=1 through to m, so (θj) ends up being an n+1 feature vector
Don't regularize over the bias terms (0)
If you do this you get a reasonable value
We're rushing through this a bit, but it's just a linear regression problem
To make this a little bit clearer you can get rid of the mj term (it's just a constant so shouldn't make any difference
to minimization)
So to learn (θj)

But for our recommender system we want to learn parameters for all users, so we add an extra summation
term to this which means we determine the minimum (θj) value for every user

When you do this as a function of each (θj) parameter vector you get the parameters for each user
So this is our optimization objective -> J(θ1, ..., θnu)
In order to do the minimization we have the following gradient descent

Slightly different to our previous gradient descent implementations
k = 0 and k != 0 versions

www.holehouse.org/mlclass/16_Recommender_Systems.html

3/10

10/5/2020

16_Recommender_Systems

We can define the middle term above as

Difference from linear regression
No 1/m terms (got rid of the 1/m term)
Otherwise very similar
This approach is called content-based approach because we assume we have features regarding the content which
will help us identify things that make them appealing to a user
However, often such features are not available - next we discuss a non-contents based approach!

Collaborative filtering - overview
The collaborative filtering algorithm has a very interesting property - does feature learning
i.e. it can learn for itself what features it needs to learn
Recall our original data set above for our five films and four raters
Here we assume someone had calculated the "romance" and "action" amounts of the films
This can be very hard to do in reality
Often want more features than just two
So - let's change the problem and pretend we have a data set where we don't know any of the features associated
with the films

Now let's make a different assumption
We've polled each user and found out how much each user likes
Romantic films
Action films
Which has generated the following parameter set

Alice and Bob like romance but hate action
Carol and Dave like action but hate romance
If we can get these parameters from the users we can infer the missing values from our table
Lets look at "Love at Last"
Alice and Bob loved it
Carol and Dave hated it
We know from the feature vectors Alice and Bob love romantic films, while Carol and Dave hate them
Based on the factor Alice and Bob liked "Love at Last" and Carol and Dave hated it we may be able to
(correctly) conclude that "Love at Last" is a romantic film
This is a bit of a simplification in terms of the maths, but what we're really asking is
"What feature vector should x1 be so that
www.holehouse.org/mlclass/16_Recommender_Systems.html

4/10

10/5/2020

16_Recommender_Systems

(θ1)T x1 is about 5
(θ2)T x1 is about 5
(θ3)T x1 is about 0
(θ4)T x1 is about 0
From this we can guess that x1 may be

Using that same approach we should then be able to determine the remaining feature vectors for the other
films
Formalizing the collaborative filtering problem
We can more formally describe the approach as follows
Given (θ1, ..., θnu) (i.e. given the parameter vectors for each users' preferences)
We must minimize an optimization function which tries to identify the best parameter vector associated with
a film

So we're summing over all the indices j for where we have data for movie i
We're minimizing this squared error
Like before, the above equation gives us a way to learn the features for one film
We want to learn all the features for all the films - so we need an additional summation term
How does this work with the previous recommendation system
Content based recommendation systems
Saw that if we have a set of features for movie rating you can learn a user's preferences
Now
If you have your users preferences you can therefore determine a film's features
This is a bit of a chicken & egg problem
What you can do is
Randomly guess values for θ
Then use collaborative filtering to generate x
Then use content based recommendation to improve θ
Use that to improve x
And so on
This actually works
Causes your algorithm to converge on a reasonable set of parameters
This is collaborative filtering
We call it collaborative filtering because in this example the users are collaborating together to help the algorithm
learn better features and help the system and the other users

Collaborative filtering Algorithm
Here we combine the ideas from before to build a collaborative filtering algorithm
Our starting point is as follows
If we're given the film's features we can use that to work out the users' preference

www.holehouse.org/mlclass/16_Recommender_Systems.html

5/10

10/5/2020

16_Recommender_Systems

If we're given the users' preferences we can use them to work out the film's features

One thing you could do is
Randomly initialize parameter
Go back and forward
But there's a more efficient algorithm which can solve θ and x simultaneously
Define a new optimization objective which is a function of x and θ

Understanding this optimization objective
The squared error term is the same as the squared error term in the two individual objectives above

So it's summing over every movie rated by every users
Note the ":" means, "for which"
Sum over all pairs (i,j) for which r(i,j) is equal to 1
The regularization terms
Are simply added to the end from the original two optimization functions
This newly defined function has the property that
If you held x constant and only solved θ then you solve the, "Given x, solve θ" objective above
Similarly, if you held θ constant you could solve x
In order to come up with just one optimization function we treat this function as a function of both film features x
and user parameters θ
Only difference between this in the back-and-forward approach is that we minimize with respect to both x
and θ simultaneously
When we're learning the features this way
Previously had a convention that we have an x0 = 1 term
When we're using this kind of approach we have no x0,
So now our vectors (both x and θ) are n-dimensional (not n+1)
We do this because we are now learning all the features so if the system needs a feature always = 1 then the
algorithm can learn one
Algorithm Structure
1) Initialize θ1, ..., θnu and x1, ..., xnm to small random values
A bit like neural networks - initialize all parameters to small random numbers
2) Minimize cost function (J(x1, ..., xnm, θ1, ...,θnu) using gradient descent
We find that the update rules look like this

www.holehouse.org/mlclass/16_Recommender_Systems.html

6/10

10/5/2020

16_Recommender_Systems

Where the top term is the partial derivative of the cost function with respect to xki while the bottom is the

partial derivative of the cost function with respect to θki
So here we regularize EVERY parameters (no longer x0 parameter) so no special case update rule
3) Having minimized the values, given a user (user j) with parameters θ and movie (movie i) with learned
features x, we predict a start rating of (θj)T xi
This is the collaborative filtering algorithm, which should give pretty good predictions for how users like new movies

Vectorization: Low rank matrix factorization
Having looked at collaborative filtering algorithm, how can we improve this?
Given one product, can we determine other relevant products?
We start by working out another way of writing out our predictions
So take all ratings by all users in our example above and group into a matrix Y

5 movies
4 users
Get a [5 x 4] matrix
Given [Y] there's another way of writing out all the predicted ratings

With this matrix of predictive ratings
We determine the (i,j) entry for EVERY movie
We can define another matrix X
Just like matrix we had for linear regression
Take all the features for each movie and stack them in rows

Think of each movie as one example

www.holehouse.org/mlclass/16_Recommender_Systems.html

7/10

10/5/2020

16_Recommender_Systems

Also define a matrix Θ

Take each per user parameter vector and stack in rows
Given our new matrices X and θ
We can have a vectorized way of computing the prediction range matrix by doing X * θT
We can given this algorithm another name - low rank matrix factorization
This comes from the property that the X * θT calculation has a property in linear algebra that we create a
low rank matrix
Don't worry about what a low rank matrix is
Recommending new movies to a user
Finally, having run the collaborative filtering algorithm, we can use the learned features to find related films
When you learn a set of features you don't know what the features will be - lets you identify the features
which define a film
Say we learn the following features
x1 - romance
x2 - action
x3 - comedy
x4 - ...
So we have n features all together
After you've learned features it's often very hard to come in and apply a human understandable metric to
what those features are
Usually learn features which are very meaning full for understanding what users like
Say you have movie i
Find movies j which is similar to i, which you can recommend
Our features allow a good way to measure movie similarity
If we have two movies xi and xj
We want to minimize ||xi - xj||
i.e. the distance between those two movies
Provides a good indicator of how similar two films are in the sense of user perception
NB - Maybe ONLY in terms of user perception

Implementation detail: Mean Normalization
Here we have one final implementation detail - make algorithm work a bit better
To show why we might need mean normalization let's consider an example where there's a user who hasn't rated
any movies

Lets see what the algorithm does for this user
Say n = 2
We now have to learn θ5 (which is an n-dimensional vector)
www.holehouse.org/mlclass/16_Recommender_Systems.html

8/10

10/5/2020

16_Recommender_Systems

Looking in the first term of the optimization objective
There are no films for which r(i,j) = 1
So this term places no role in determining θ5
So we're just minimizing the final regularization term

Of course, if the goal is to minimize this term then

Why - If there's no data to pull the values away from 0 this gives the min value
So this means we predict ANY movie to be zero
Presumably Eve doesn't hate all movies...
So if we're doing this we can't recommend any movies to her either
Mean normalization should let us fix this problem
How does mean normalization work?
Group all our ratings into matrix Y as before
We now have a column of ?s which corresponds to Eves rating

Now we compute the average rating each movie obtained and stored in an nm - dimensional column vector

If we look at all the movie ratings in [Y] we can subtract off the mean rating

Means we normalize each film to have an average rating of 0
Now, we take the new set of ratings and use it with the collaborative filtering algorithm
Learn θj and xi from the mean normalized ratings
For our prediction of user j on movie i, predict
www.holehouse.org/mlclass/16_Recommender_Systems.html

9/10

10/5/2020

16_Recommender_Systems

(θj)T xi + µi
Where these vectors are the mean normalized values
We have to add µ because we removed it from our θ values
So for user 5 the same argument applies, so

So on any movie i we're going to predict
(θ5)T xi + µi

Where (θ5)T xi = to 0 (still)
But we then add the mean (µi) which means Eve has an average rating assigned to each movie for
here
This makes sense
If Eve hasn't rated any films, predict the average rating of the films based on everyone
This is the best we can do
As an aside - we spoke here about mean normalization for users with no ratings
If you have some movies with no ratings you can also play with versions of the algorithm where you
normalize the columns
BUT this is probably less relevant - probably shouldn't recommend an unrated movie
To summarize, this shows how you do mean normalization preprocessing to allow your system to deal with users
who have not yet made any ratings
Means we recommend the user we know little about the best average rated products

www.holehouse.org/mlclass/16_Recommender_Systems.html

10/10

10/5/2020

17_Large_Scale_Machine_Learning

17: Large Scale Machine Learning
Previous Next Index

Learning with large datasets
This set of notes look at large scale machine learning - how do we deal with big datasets?
If you look back at 5-10 year history of machine learning, ML is much better now because we have much
more data
However, with this increase in data comes great responsibility? No, comes a much more significant
computational cost
New and exciting problems are emerging that need to be dealt with on both the algorithmic
and architectural level
Why large datasets?
One of best ways to get high performance is take a low bias algorithm and train it on a lot of data
e.g. Classification between confusable words
We saw that so long as you feed an algorithm lots of data they all perform pretty similarly

So it's good to learn with large datasets
But learning with large datasets comes with its own computational problems
Learning with large datasets
For example, say we have a data set where m = 100, 000, 000
This is pretty realistic for many datasets
Census data
Website traffic data
How do we train a logistic regression model on such a big system?

So you have to sum over 100,000,000 terms per step of gradient descent
Because of the computational cost of this massive summation, we'll look at more efficient ways around
this
- Either using a different approach
- Optimizing to avoid the summation
First thing to do is ask if we can train on 1000 examples instead of 100 000 000
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

1/14

10/5/2020

17_Large_Scale_Machine_Learning

Randomly pick a small selection
Can you develop a system which performs as well?
Sometimes yes - if this is the case you can avoid a lot of the headaches associated with big data
To see if taking a smaller sample works, you can sanity check by plotting error vs. training set size
If our plot looked like this

Looks like a high variance problem
More examples should improve performance
If plot looked like this

This looks like a high bias problem
More examples may not actually help - save a lot of time and effort if we know this before hand
One natural thing to do here might be to;
Add extra features
Add extra hidden units (if using neural networks)

Stochastic Gradient Descent
For many learning algorithms, we derived them by coming up with an optimization objective (cost
function) and using an algorithm to minimize that cost function
When you have a large dataset, gradient descent becomes very expensive
So here we'll define a different way to optimize for large data sets which will allow us to scale the
algorithms
Suppose you're training a linear regression model with gradient descent
Hypothesis

www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

2/14

10/5/2020

17_Large_Scale_Machine_Learning

Cost function

If we plot our two parameters vs. the cost function we get something like this

Looks like this bowl shape surface plot
Quick reminder - how does gradient descent work?

In the inner loop we repeatedly update the parameters θ
We will use linear regression for our algorithmic example here when talking about stochastic gradient
descent, although the ideas apply to other algorithms too, such as
Logistic regression
Neural networks
Below we have a contour plot for gradient descent showing iteration to a global minimum

As mentioned, if m is large gradient descent can be very expensive
Although so far we just referred to it as gradient descent, this kind of gradient descent is called batch
gradient descent
This just means we look at all the examples at the same time
Batch gradient descent is not great for huge datasets
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

3/14

10/5/2020

17_Large_Scale_Machine_Learning

If you have 300,000,000 records you need to read in all the records into memory from disk because
you can't store them all in memory
By reading all the records, you can move one step (iteration) through the algorithm
Then repeat for EVERY step
This means it take a LONG time to converge
Especially because disk I/O is typically a system bottleneck anyway, and this will inevitably
require a huge number of reads
What we're going to do here is come up with a different algorithm which only needs to look at single
example at a time
Stochastic gradient descent
Define our cost function slightly differently, as

So the function represents the cost of θ with respect to a specific example (xi, yi)
And we calculate this value as one half times the squared error on that example
Measures how well the hypothesis works on a single example
The overall cost function can now be re-written in the following form;

This is equivalent to the batch gradient descent cost function
With this slightly modified (but equivalent) view of linear regression we can write out how stochastic
gradient descent works
1) - Randomly shuffle

2) - Algorithm body

So what's going on here?
The term

Is the same as that found in the summation for batch gradient descent
It's possible to show that this term is equal to the partial derivative with respect to the
parameter θj of the cost(θ, (xi, yi))

www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

4/14

10/5/2020

17_Large_Scale_Machine_Learning

What stochastic gradient descent algorithm is doing is scanning through each example
The inner for loop does something like this...
Looking at example 1, take a step with respect to the cost of just the 1st training example
Having done this, we go on to the second training example
Now take a second step in parameter space to try and fit the second training example better
Now move onto the third training example
And so on...
Until it gets to the end of the data
We may now repeat this who procedure and take multiple passes over the data
The randomly shuffling at the start means we ensure the data is in a random order so we don't bias
the movement
Randomization should speed up convergence a little bit
Although stochastic gradient descent is a lot like batch gradient descent, rather than waiting to sum up
the gradient terms over all m examples, we take just one example and make progress in improving the
parameters already
Means we update the parameters on EVERY step through data, instead of at the end of each loop
through all the data
What does the algorithm do to the parameters?
As we saw, batch gradient descent does something like this to get to a global minimum

With stochastic gradient descent every iteration is much faster, but every iteration is flitting a single
example

What you find is that you "generally" move in the direction of the global minimum, but not
always
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

5/14

10/5/2020

17_Large_Scale_Machine_Learning

Never actually converges like batch gradient descent does, but ends up wandering around
some region close to the global minimum
In practice, this isn't a problem - as long as you're close to the minimum that's probably
OK
One final implementation note
May need to loop over the entire dataset 1-10 times
If you have a truly massive dataset it's possible that by the time you've taken a single pass through
the dataset you may already have a perfectly good hypothesis
In which case the inner loop might only need to happen 1 if m is very very large
If we contrast this to batch gradient descent
We have to make k passes through the entire dataset, where k is the number of steps needed to
move through the data

Mini Batch Gradient Descent
Mini-batch gradient descent is an additional approach which can work even faster than stochastic
gradient descent
To summarize our approaches so far
Batch gradient descent: Use all m examples in each iteration
Stochastic gradient descent: Use 1 example in each iteration
Mini-batch gradient descent: Use b examples in each iteration
b = mini-batch size
So just like batch, except we use tiny batches
Typical range for b = 2-100 (10 maybe)
For example
b = 10
Get 10 examples from training set
Perform gradient descent update using the ten examples
Mini-batch algorithm

We for-loop through b-size batches of m
Compared to batch gradient descent this allows us to get through data in a much more efficient way
After just b examples we begin to improve our parameters
Don't have to update parameters after every example, and don't have to wait until you cycled
through all the data
Mini-batch gradient descent vs. stochastic gradient descent
Why should we use mini-batch?
Allows you to have a vectorized implementation
Means implementation is much more efficient
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

6/14

10/5/2020

17_Large_Scale_Machine_Learning

Can partially parallelize your computation (i.e. do 10 at once)
A disadvantage of mini-batch gradient descent is the optimization of the parameter b
But this is often worth it!
To be honest, stochastic gradient descent and batch gradient descent are just specific forms of batchgradient descent
For mini-batch gradient descent, b is somewhere in between 1 and m and you can try to optimize for
it!

Stochastic gradient descent convergence
We now know about stochastic gradient descent
But how do you know when it's done!?
How do you tune learning rate alpha (α)?
Checking for convergence
With batch gradient descent, we could plot cost function vs number of iterations
Should decrease on every iteration
This works when the training set size was small because we could sum over all examples
Doesn't work when you have a massive dataset
With stochastic gradient descent
We don't want to have to pause the algorithm periodically to do a summation over all data
Moreover, the whole point of stochastic gradient descent is to avoid those whole-data
summations
For stochastic gradient descent, we have to do something different
Take cost function definition

One half the squared error on a single example
While the algorithm is looking at the example (xi, yi), but before it has updated θ we can compute
the cost of the example (cost(θ, (xi, yi))
i.e. we compute how well the hypothesis is working on the training example
Need to do this before we update θ because if we did it after θ was updated the algorithm
would be performing a bit better (because we'd have just used (xi, yi) to improve θ)
To check for the convergence, every 1000 iterations we can plot the costs averaged over the last
1000 examples
Gives a running estimate of how well we've done on the last 1000 estimates
By looking at the plots we should be able to check convergence is happening
What do these plots look like
In general
Might be a bit noisy (1000 examples isn't that much)
If you get a figure like this

That's a pretty decent run
Algorithm may have convergence
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

7/14

10/5/2020

17_Large_Scale_Machine_Learning

If you use a smaller learning rate you may get an even better final solution

This is because the parameter oscillate around the global minimum
A smaller learning rate means smaller oscillations
If you average over 1000 examples and 5000 examples you may get a smoother curve

This disadvantage of a larger average means you get less frequent feedback
Sometimes you may get a plot that looks like this

Looks like cost is not decreasing at all
But if you then increase to averaging over a larger number of examples you do see this general
trend
Means the blue line was too noisy, and that noise is ironed out by taking a greater number
of entires per average
Of course, it may not decrease, even with a large number
If you see a curve the looks like its increasing then the algorithm may be displaying divergence

Should use a smaller learning rate
Learning rate
We saw that with stochastic gradient descent we get this wandering around the minimum
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

8/14

10/5/2020

17_Large_Scale_Machine_Learning

In most implementations the learning rate is held constant
However, if you want to converge to a minimum you can slowly decrease the learning rate over time
A classic way of doing this is to calculate α as follows
α = const1/(iterationNumber + const2)
Which means you're guaranteed to converge somewhere
You also need to determine const1 and const2
BUT if you tune the parameters well, you can get something like this

Online learning
New setting
Allows us to model problems where you have a continuous stream of data you want an algorithm to
learn from
Similar idea of stochastic gradient descent, in that you do slow updates
Web companies use various types of online learning algorithms to learn from traffic
Can (for example) learn about user preferences and hence optimize your website
Example - Shipping service
Users come and tell you origin and destination
You offer to ship the package for some amount of money ($10 - $50)
Based on the price you offer, sometimes the user uses your service (y = 1), sometimes they don't (y =
0)
Build an algorithm to optimize what price we offer to the users
Capture
Information about user
Origin and destination
Work out
What the probability of a user selecting the service is
We want to optimize the price
To model this probability we have something like
p(y = 1|x; θ)
Probability that y =1, given x, parameterized by θ
Build this model with something like
Logistic regression
Neural network
If you have a website that runs continuously an online learning algorithm would do something like
this
User comes - is represented as an (x,y) pair where
x - feature vector including price we offer, origin, destination
y - if they chose to use our service or not

www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

9/14

10/5/2020

17_Large_Scale_Machine_Learning

The algorithm updates θ using just the (x,y) pair

So we basically update all the θ parameters every time we get some new data
While in previous examples we might have described the data example as (xi, yi) for an online
learning problem we discard this idea of a data "set" - instead we have a continuous stream of data
so indexing is largely irrelevant as you're not storing the data (although presumably you could store
it)
If you have a major website where you have a massive stream of data then this kind of algorithm is pretty
reasonable
You're free of the need to deal with all your training data
If you had a small number of users you could save their data and then run a normal algorithm on a
dataset
An online algorithm can adapt to changing user preferences
So over time users may become more price sensitive
The algorithm adapts and learns to this
So your system is dynamic
Another example - product search
Run an online store that sells cellphones
You have a UI where the user can type in a query like, "Android phone 1080p camera"
We want to offer the user 10 phones per query
How do we do this
For each phone and given a specific user query, we create a feature vector (x) which has data like
features of the phone, how many words in the user query match the name of the phone, how many
words in user query match description of phone
Basically how well does the phone match the user query
We want to estimate the probability of a user selecting a phone
So define
y = 1 if a user clicks on a link
y = 0 otherwise
So we want to learn
p(y = 1|x ; θ) <- this is the problem of learning the predicted click through rate (CTR)
If you can estimate the CTR for any phone we can use this to show the highest probability phones
first
If we display 10 phones per search, it means for each search we generate 10 training examples of
data
i.e. user can click through one or more, or none of them, which defines how well the prediction
performed
Other things you can do
Special offers to show the user
Show news articles - learn what users like
Product recommendation
These problems could have been formulated using standard techniques, but they are the kinds of
problems where you have so much data that this is a better way to do things

Map reduce and data parallelism
Previously spoke about stochastic gradient descent and other algorithms
These could be run on one machine
Some problems are just too big for one computer
Talk here about a different approach called Map Reduce
Map reduce example
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

10/14

10/5/2020

17_Large_Scale_Machine_Learning

We want to do batch gradient descent

Assume m = 400
Normally m would be more like 400 000 000
If m is large this is really expensive
Split training sets into different subsets
So split training set into 4 pieces
Machine 1 : use (x1, y1), ..., (x100, y100)
Uses first quarter of training set
Just does the summation for the first 100

So now we have these four temp values, and each machine does 1/4 of the work
Once we've got our temp variables
Send to to a centralized master server
Put them back together
Update θ using

This equation is doing the same as our original batch gradient descent algorithm

www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

11/14

10/5/2020

17_Large_Scale_Machine_Learning

More generally map reduce uses the following scheme (e.g. where you split into 4)

The bulk of the work in gradient descent is the summation
Now, because each of the computers does a quarter of the work at the same time, you get a 4x
speedup
Of course, in practice, because of network latency, combining results, it's slightly less than 4x, but
still good!
Important thing to ask is
"Can algorithm be expressed as computing sums of functions of the training set?"
Many algorithms can!
Another example
Using an advanced optimization algorithm with logistic regression

Need to calculate cost function - see we sum over training set
So split training set into x machines, have x machines compute the sum of the value over
1/xth of the data

These terms are also a sum over the training set
So use same approach
So with these results send temps to central server to deal with combining everything
More broadly, by taking algorithms which compute sums you can scale them to very large datasets
through parallelization
Parallelization can come from
Multiple machines
Multiple CPUs
Multiple cores in each CPU
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

12/14

10/5/2020

17_Large_Scale_Machine_Learning

So even on a single compute can implement parallelization
The advantage of thinking about Map Reduce here is because you don't need to worry about network
issues
It's all internal to the same machine
Finally caveat/thought
Depending on implementation detail, certain numerical linear algebra libraries can
automatically parallelize your calculations across multiple cores
So, if this is the case and you have a good vectorization implementation you can not worry about
local Parallelization and the local libraries sort optimization out for you
Hadoop
Hadoop is a good open source Map Reduce implementation
Represents a top-level Apache project develop by a global community of developers
Large developer community all over the world
Written in Java
Yahoo has been the biggest contributor
Pushed a lot early on
Support now from Cloudera

Interview with Cloudera CEO Mike Olson (2010)
Seeing a change in big data industry (Twitter, Facebook etc) - relational databases can't scale to the
volumes of data being generated
Q: Where the tech came from?
Early 2000s - Google had too much data to process (and index)
Designed and built Map Reduce
Buy and mount a load of rack servers
Spread the data out among these servers (with some duplication)
Now you've stored the data and you have this processing infrastructure spread
among the data
Use local CPU to look at local data
Massive data parallelism
Published as a paper in 2004
At the time wasn't obvious why it was necessary - didn't support queries,
transactions, SQL etc
When data was at "human" scale relational databases centralized around a single server
was fine
But now we're scaling by Moore's law in two ways
More data
Cheaper to store
Q: How do people approach the issues in big data?
People still use relational databases - great if you have predictable queries over structured data
Data warehousing still used - long term market
But the data people want to work with is becoming more complex and bigger
Free text, unstructured data doesn't fit will into tables
Do sentiment analysis in SQL isn't really that good
So to do new kinds of processing need a new type of architecture
Hadoop lets you do data processing - not transactional processing - on the big scale
Increasingly things like NoSQL is being used
Data centers are starting to chose technology which is aimed at a specific problem, rather than
trying to shoehorn problems into an ER issue
Open source technologies are taking over for developer facing infrastructures and platforms
Q: What is Hadoop?
Open source implementation of Map reduce (Apache software)
Yahoo invested a lot early on - developed a lot the early progress
www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

13/14

10/5/2020

17_Large_Scale_Machine_Learning

Is two things
HDFS
Disk on ever server
Software infrastructure to spread data
Map reduce
Lets you push code down to the data in parallel
As size increases you can just add more servers to scale up
Q: What is memcached?
Ubiquitous invisible infrastructure that makes the web run
You go to a website, see data being delivered out of a MySQL database
BUT, when infrastructure needs to scale querying a disk EVERY time is too much
Memcache is a memory layer between disk and web server
Cache reads
Push writes through incrementally
Is the glue that connects a website with a disk-backend
Northscale is commercializing this technology
New data delivery infrastructure which has pretty wide adoption
Q: What is Django?
Open source tool/language
Q: What are some of the tool sets being used in data management? What is MySQL
drizzle?
Drizzle is a re-implementation of MySQL
Team developing Drizzle feels they learned a lot of lessons when building MySQL
More modern architecture better targeted at web applications
NoSQL
Distributed hash tables
Idea is instead of a SQL query and fetching a record, go look up something from a store by
name
Go pull a record by name from a store
So now systems being developed to support that like
MongoDB
CouchDB
Hadoop companion projects
Hive
Lets you use SQL to talk to a Hadoop cluster
HBase
Sits on top of HDFS
Gives you key-value storage on top of HDFS - provides abstraction from distributed
system
Good time to be working in big data
Easy to set up small cluster through cloud system
Get a virtual cluster through Rackspace or Cloudera

www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html

14/14

