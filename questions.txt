1. In an A/B test, how can you check if assignment to the various buckets was truly random?
Plot the distributions of multiple features for both A and B and make sure that they have the same shape. More rigorously, we can conduct a permutation test to see if the distributions are the same.
MANOVA to compare different means
---------------------
2. What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?
Verify the sampling algorithm is random.
3. What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?
The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups.
4. What would be some issues if blogs decide to cover one of your experimental groups?
Same as the previous question. The above problem can happen in larger scale.
5. How would you conduct an A/B test on an opt-in feature? 
Ask someone for more details.
6. How would you run an A/B test for many variants, say 20 or more?
one control, 20 treatment, if the sample size for each group is big enough.
Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD).
7. How would you run an A/B test if the observations are extremely right-skewed?
lower the variability by modifying the KPI
cap values
percentile metrics
log transform
https://www.quora.com/How-would-you-run-an-A-B-test-if-the-observations-are-extremely-right-skewed
8. I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?
exclusive -> ok
9. What is a p-value? What is the difference between type-1 and type-2 error?
en.wikipedia.org/wiki/P-value
type-1 error: rejecting Ho when Ho is true
type-2 error: not rejecting Ho when Ha is true
10. You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?
For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups.
Ask someone for more details.
11. How would you design an experiment to determine the impact of latency on user engagement?
The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, i.e., add a delay in an A/B test.
12. What is maximum likelihood estimation? Could there be any case where it doesn’t exist?
A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model).
maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized.
for gaussian mixtures, non parametric models, it doesn’t exist
13. What’s the difference between a MAP, MOM, MLE estimator? In which cases would you want to use each?
MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution.
MOM sets moment values and solves for the parameters. MOM is not used much anymore because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.
14. What is a confidence interval and how do you interpret it?
For example, 95% confidence interval is an interval that when constructed for a set of samples each sampled in the same way, the constructed intervals include the true mean 95% of the time.
if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.
15. What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?
Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However, this is not always desirable for data analysis or predictive modeling as there is the bias variance tradeoff. We sometimes want to prioritize the generalizability and avoid overfitting by reducing variance and thus increasing bias.
1. What would be good metrics of success for an advertising-driven consumer product? (Buzzfeed, YouTube, Google Search, etc.) A service-driven consumer product? (Uber, Flickr, Venmo, etc.)
advertising-driven: Page-views and daily actives, CTR, CPC (cost per click)
click-ads
display-ads
service-driven: number of purchases, conversion rate
2. What would be good metrics of success for a productivity tool? (Evernote, Asana, Google Docs, etc.) A MOOC? (edX, Coursera, Udacity, etc.)
Productivity tool: same as premium subscriptions
MOOC: same as premium subscriptions, completion rate
3. What would be good metrics of success for an e-commerce product? (Etsy, Groupon, Birchbox, etc.) A subscription product? (Net ix, Birchbox, Hulu, etc.) Premium subscriptions? (OKCupid, LinkedIn, Spotify, etc.) 
e-commerce: number of purchases, conversion rate, Hourly, daily, weekly, monthly, quarterly, and annual sales, Cost of goods sold, Inventory levels, Site traffic, Unique visitors versus returning visitors, Customer service phone call count, Average resolution time
subscription
churn, CoCA, ARPU, MRR, LTV
premium subscriptions: 
subscription rate
4. What would be good metrics of success for a consumer product that relies heavily on engagement and interaction? (Snapchat, Pinterest, Facebook, etc.) A messaging product? (GroupMe, Hangouts, Snapchat, etc.)
heavily on engagement and interaction: uses AU ratios, email summary by type, and push notification summary by type, resurrection ratio
messaging product: 
daily, monthly active users
5. What would be good metrics of success for a product that offered in-app purchases? (Zynga, Angry Birds, other gaming apps)
Average Revenue Per Paid User
Average Revenue Per User
6. A certain metric is violating your expectations by going down or up more than you expect. How would you try to identify the cause of the change?
breakdown the KPI’s into what consists them and find where the change is
then further breakdown that basic KPI by channel, user cluster, etc. and relate them with any campaigns, changes in user behaviors in that segment
7. Growth for total number of tweets sent has been slow this month. What data would you look at to determine the cause of the problem?
Historical data, especially historical data at the same month
Outer data, such as economic data, political data, data about competitors
8. You’re a restaurant and are approached by Groupon to run a deal. What data would you ask from them in order to determine whether or not to do the deal?
for similar restaurants (they should define similarity), average increase in revenue gain per coupon, average increase in customers per coupon
9. You are tasked with improving the efficiency of a subway system. Where would you start?
define efficiency
10. Say you are working on Facebook News Feed. What would be some metrics that you think are important? How would you make the news each person gets more relevant?
rate for each action, duration users stay, CTR for sponsor feed posts
ref. News Feed Optimization
Affinity score: how close the content creator and the users are
Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
Time decay: the older the less important
11. How would you measure the impact that sponsored stories on Facebook News Feed have on user engagement? How would you determine the optimum balance between sponsored stories and organic content on a user’s News Feed?
AB test on different balance ratio and see 
12. You are on the data science team at Uber and you are asked to start thinking about surge pricing. What would be the objectives of such a product and how would you start looking into this?
 there is a gradual step-function type scaling mechanism until that imbalance of requests-to-drivers is alleviated and then vice versa as too many drivers come online enticed by the surge pricing structure. 
I would bet the algorithm is custom tailored and calibrated to each location as price elasticities almost certainly vary across different cities depending on a huge multitude of variables: income, distance/sprawl, traffic patterns, car ownership, etc. With the massive troves of user data that Uber probably has collected, they most likely have tweaked the algorithms for each city to adjust for these varying sensitivities to surge pricing. Throw in some machine learning and incredibly rich data and you've got yourself an incredible, constantly-evolving algorithm.
13. Say that you are Netflix. How would you determine what original series you should invest in and create?
Netflix uses data to estimate the potential market size for an original series before giving it the go-ahead.
14. What kind of services would find churn (metric that tracks how many customers leave the service) helpful? How would you calculate churn?
subscription based services
15. Let’s say that you’re are scheduling content for a content provider on television. How would you determine the best times to schedule content?
Based on similar product and the corresponding broadcast popularity
1. (Given a Dataset) Analyze this dataset and tell me what you can learn from it.
Typical data cleaning and visualization
2. What is R2? What are some other metrics that could be better than R2 and why?
goodness of fit measure. variance explained by the regression / total variance
the more predictors you add, the higher R^2 becomes.
hence use adjusted R^2 which adjusts for the degrees of freedom 
or train error metrics
3. What is the curse of dimensionality?
High dimensionality makes clustering hard, because having lots of dimensions means that everything is "far away" from each other.
For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases
All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.
The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data. 
We should conduct PCA to reduce dimensionality
4. Is more data always better?
Statistically
It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.
It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.
Practically
More data usually benefit the models
Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data.
5. What are advantages of plotting your data before performing analysis?
Data sets have errors. You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.
Variables can have skewness, outliers, etc. Then the arithmetic mean might not be useful, which means the standard deviation isn't useful.
Variables can be multimodal! If a variable is multimodal then anything based on its mean or median is going to be suspect. 
6. How can you make sure that you don’t analyze something that ends up meaningless?
Proper exploratory data analysis.
In every data analysis task, there's the exploratory phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.
Then there's the exploratory phase, where you look deeply into a set of hypotheses. 
The exploratory phase will generate lots of possible hypotheses, and the exploratory phase will let you really understand a few of them. Balance the two and you'll prevent yourself from wasting time on many things that end up meaningless, although not all.
7. What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?
data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.
The scientific method is eminently inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is.
8. How can you determine which features are the most important in your model?
Linear regression can use p-value
run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
Look at the variables added in forward variable selection 
9. How do you deal with some of your predictors being missing?
Remove rows with missing values - This works well if
the values are missing randomly (see Vinay Prabhu's answer for more details on this)
if you don't lose too much of the dataset after doing so.
Build another predictive model to predict the missing values
This could be a whole project in itself, so simple techniques are usually used here.
Use a model that can incorporate missing data 
Like a random forest, or any tree-based method.
10. You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?
Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. 
Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based.
principal component regression
11. Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?
PCA
12. Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?
ridge / lasso / elastic net regression
Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores
Recursive Feature Elimination:
First, train a model with all the feature and evaluate its performance on held out data.
Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining features.
Iterate until you observe a sharp drop in the predictive accuracy of the model.
13. Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?
p > n.
If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. 
14. You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?
The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.
15. What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?
The assumption is that a group of weak learners can be combined to form a strong learner.
Hence the combined model is expected to perform better than an individual model.
Assumptions:
average out biases
reduce variance
Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability leads to lower error. If you want to understand why, the original bagging paper( http://www.springerlink.com/) has a section called "why bagging works"
Boosting works because of the focus on better defining the "decision edge". By re-weighting examples near the margin (the positive and negative examples) you get a reduced error (see http://citeseerx.ist.psu.edu/vie...)
Use the outputs of your models as inputs to a meta-model. 
For example, if you're doing binary classification, you can use all the probability outputs of your individual models as inputs to a final logistic regression (or any model, really) that can combine the probability estimates.

One very important point is to make sure that the output of your models are out-of-sample predictions. This means that the predicted value for any row in your data-frame should NOT depend on the actual value for that row.

16. Given that you have wifi data in your office, how would you determine which rooms and areas are underutilized and over-utilized?
If the data is more used in one room, then that one is over utilized!
Maybe account for the room capacity and normalize the data.
17. How could you use GPS data from a car to determine the quality of a driver?
Speed
Driving paths
18. Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?
Historical data?
19. Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?
Evaluate his positions in the court.
20. How would you quantify the influence of a Twitter user?
like page rank with each user corresponding to the webpages and linking to the page equivalent to following.
21. Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?
winning probability for different positions
22. You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of difficulty?
One way you could do this is by storing a "skill level" for each user and a "difficulty level" for each problem.  We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem.*  Then we maximize the likelihood of the data to find the hidden skill and difficulty levels.
The Rasch model for dichotomous data takes the form:
{\displaystyle \Pr\{X_{ni}=1\}={\frac {\exp({\beta_{n}}-{\delta_{i}})}{1+\exp({\beta_{n}}-{\delta_{i}})}},}
where  is the ability of person  and  is the difficulty of item}.
23. You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?
Some people would take the mean rank of each sushi.  If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong).
24. Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?
collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative
for liberal and republican parties, find the mean vector and find the representative closest to the center point
25. How would you come up with an algorithm to detect plagiarism in online content?
reduce the text to a more compact form (e.g. fingerprinting, bag of words) then compare those with other texts by calculating the similarity
26. You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?
K-means
choose a small value of k that still has a low SSE (elbow method)
Elbow method
27. Let’s say you’re building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?
content-based filtering
collaborative filtering
Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 o spring, respectively. Each of Bobo’s descendants also have the same probabilities. What is the probability that Bobo’s lineage dies out?
p=1/4+1/4p+1/2p^2 => p=1/2
2. In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour?
1-(0.8)^4 = 0.5904
Or, we can use Poisson processes
3. How can you generate a random number between 1 - 7 with only a die?
Quora Answer
4. How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?
Flip twice:
HT --> H
TH --> T
If HH or TT, repeat.
5. You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?
more than two standard deviations
6. Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?
Plug in the value to the CDF of the same random variable
7. A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?
gg, gb, bg --> 1/3
8. You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?
Geometric distribution with p = 0.5
gender ratio is 1:1. Expected number of children is 2.
let X be the number of children until getting a female (happens with prob 1/2). this follows a geometric distribution with probability 1/2
9. How many ways can you split 12 people into 3 teams of 4?
the outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable
(12, 8) * (8, 4) * (4, 4) / (3, 3)
12! / (4!)^3 / 3!
10. Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.
the probability of a hash collision: 1-(10!/10^10)
the expected number of hash collisions: 10(1 - (1-1/10)^10)
Quora Reference
the expected number of hashes that are unused: 10*(9/10)^10
11. You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?
Lyfts arrive first: 2! * 3! / 5!
Ubers arrive first: same
12. I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?
100+60-20=140
13. On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?
24C5*(1+5(24-5))/24C5*24C5 = 4/1771
14. A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?
1
15. Let’s say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?
Shorter. Regression to the mean
16. What’s the expected number of coin flips until you get two heads in a row? What’s the expected number of coin flips until you get two tails in a row?
x = 0.25 * 2 + 0.25 * (x + 2) + 0.5 * (x + 1) --> x = 6
Quora Reference
17. Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?
less than $3
Quora reference
18. You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?
4/13
Bayesian method
19. You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What’s the chance that you picked up the fair coin, given the information that you observed?
Bayesian method
20. What is a P-Value ?
https://en.wikipedia.org/wiki/P-value

Q1: What’s the trade-off between bias and variance?
Answer: Bias is error due to erroneous or overly simplistic assumptions in the learning algorithm you’re using. This can lead to the model underfitting your data, making it hard for it to have high predictive accuracy and for you to generalize your knowledge from the training set to the test set.

Variance is error due to too much complexity in the learning algorithm you’re using. This leads to the algorithm being highly sensitive to high degrees of variation in your training data, which can lead your model to overfit the data. You’ll be carrying too much noise from your training data for your model to be very useful for your test data.

The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, the variance and a bit of irreducible error due to noise in the underlying dataset. Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance. You don’t want either high bias or high variance in your model.

More reading: Bias-Variance Tradeoff (Wikipedia)

Q2: What is the difference between supervised and unsupervised machine learning?
Answer: Supervised learning requires training labeled data. For example, in order to do classification (a supervised learning task), you’ll need to first label the data you’ll use to train the model to classify data into your labeled groups. Unsupervised learning, in contrast, does not require labeling data explicitly.

More reading: Classic examples of supervised vs. unsupervised learning (Springboard)

Q3: How is KNN different from k-means clustering?
Answer: K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.

The critical difference here is that KNN needs labeled points and is thus supervised learning, while k-means doesn’t—and is thus unsupervised learning.

More reading: How is the k-nearest neighbor algorithm different from k-means clustering? (Quora)

Q4: Explain how a ROC curve works.
Answer: The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).

More reading: Receiver operating characteristic (Wikipedia)

Q5: Define precision and recall.
Answer: Recall is also known as the true positive rate: the amount of positives your model claims compared to the actual number of positives there are throughout the data. Precision is also known as the positive predictive value, and it is a measure of the amount of accurate positives your model claims compared to the number of positives it actually claims. It can be easier to think of recall and precision in the context of a case where you’ve predicted that there were 10 apples and 5 oranges in a case of 10 apples. You’d have perfect recall (there are actually 10 apples, and you predicted there would be 10) but 66.7% precision because out of the 15 events you predicted, only 10 (the apples) are correct.

More reading: Precision and recall (Wikipedia)

Q6: What is Bayes’ Theorem? How is it useful in a machine learning context?
Answer: Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.

Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of the false positive rate of the population and the true positive rate of a condition. Say you had a 60% chance of actually having the flu after a flu test, but out of people who had the flu, the test will be false 50% of the time, and the overall population only has a 5% chance of having the flu. Would you actually have a 60% chance of having the flu after having a positive test?

Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Positive Rate of a Condition Sample) / (.6*0.05)(True Positive Rate of a Condition Sample) + (.5*0.95) (False Positive Rate of a Population)  = 0.0594 or 5.94% chance of getting a flu.

Bayes’ Theorem is the basis behind a branch of machine learning that most notably includes the Naive Bayes classifier. That’s something important to consider when you’re faced with machine learning interview questions.

More reading: An Intuitive (and Short) Explanation of Bayes’ Theorem (BetterExplained)

Q7: Why is “Naive” Bayes naive?
Answer: Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life.

As a Quora commenter put it whimsically, a Naive Bayes classifier that figured out that you liked pickles and ice cream would probably naively recommend you a pickle ice cream.

More reading: Why is “naive Bayes” naive? (Quora)

Q8: Explain the difference between L1 and L2 regularization.
Answer: L2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior on the terms, while L2 corresponds to a Gaussian prior.

More reading: What is the difference between L1 and L2 regularization? (Quora)

Q9: What’s your favorite algorithm, and can you explain it to me in less than a minute?
Answer: This type of question tests your understanding of how to communicate complex and technical nuances with poise and the ability to summarize quickly and efficiently. Make sure you have a choice and make sure you can explain different algorithms so simply and effectively that a five-year-old could grasp the basics!

Q10: What’s the difference between Type I and Type II error?
Answer: Don’t think that this is a trick question! Many machine learning interview questions will be an attempt to lob basic questions at you just to make sure you’re on top of your game and you’ve prepared all of your bases.

Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.

A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby.

More reading: Type I and type II errors (Wikipedia)

Q11: What’s a Fourier transform?
Answer: A Fourier transform is a generic method to decompose generic functions into a superposition of symmetric functions. Or as this more intuitive tutorial puts it, given a smoothie, it’s how we find the recipe. The Fourier transform finds the set of cycle speeds, amplitudes, and phases to match any time signal. A Fourier transform converts a signal from time to frequency domain—it’s a very common way to extract features from audio signals or other time series such as sensor data.

More reading: Fourier transform (Wikipedia)

Q12: What’s the difference between probability and likelihood?
More reading: What is the difference between “likelihood” and “probability”? (Cross Validated)

Q13: What is deep learning, and how does it contrast with other machine learning algorithms?
Answer: Deep learning is a subset of machine learning that is concerned with neural networks: how to use backpropagation and certain principles from neuroscience to more accurately model large sets of unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning algorithm that learns representations of data through the use of neural nets.

More reading: Deep learning (Wikipedia)

Q14: What’s the difference between a generative and discriminative model?
Answer: A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.

More reading: What is the difference between a Generative and Discriminative Algorithm? (Stack Overflow)

Q15: What cross-validation technique would you use on a time series dataset?
Answer: Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a time series is not randomly distributed data—it is inherently ordered by chronological order. If a pattern emerges in later time periods, for example, your model may still pick up on it even if that effect doesn’t hold in earlier years!

You’ll want to do something like forward chaining where you’ll be able to model on past data then look at forward-facing data.

Fold 1 : training [1], test [2]
Fold 2 : training [1 2], test [3]
Fold 3 : training [1 2 3], test [4]
Fold 4 : training [1 2 3 4], test [5]
Fold 5 : training [1 2 3 4 5], test [6]
More reading: Using k-fold cross-validation for time-series model selection (CrossValidated)

Q16: How is a decision tree pruned?
Answer: Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.

Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.

More reading: Pruning (decision trees)

Q17: Which is more important to you: model accuracy or model performance?
Answer: This question tests your grasp of the nuances of machine learning model performance! Machine learning interview questions often look towards the details. There are models with higher accuracy that can perform worse in predictive power—how does that make sense?

Well, it has everything to do with how model accuracy is only a subset of model performance, and at that, a sometimes misleading one. For example, if you wanted to detect fraud in a massive dataset with a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast minority of cases were fraud. However, this would be useless for a predictive model—a model designed to find fraud that asserted there was no fraud at all! Questions like this help you demonstrate that you understand model accuracy isn’t the be-all and end-all of model performance.

More reading: Accuracy paradox (Wikipedia)

Q18: What’s the F1 score? How would you use it?
Answer: The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much.

More reading: F1 score (Wikipedia)

Q19: How would you handle an imbalanced dataset?
Answer: An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:

Collect more data to even the imbalances in the dataset.
Resample the dataset to correct for imbalances.
Try a different algorithm altogether on your dataset.
What’s important here is that you have a keen sense for what damage an unbalanced dataset can cause, and how to balance that.

More reading: 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset (Machine Learning Mastery)

Q20: When should you use classification over regression?
Answer: Classification produces discrete values and dataset to strict categories, while regression gives you continuous results that allow you to better distinguish differences between individual points. You would use classification over regression if you wanted your results to reflect the belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to know whether a name was male or female rather than just how correlated they were with male and female names.)

More reading: Regression vs Classification (Math StackExchange)

Q21: Name an example where ensemble techniques might be useful.
Answer: Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by small changes in the training data). 

You could list some examples of ensemble methods (bagging, boosting, the “bucket of models” method) and demonstrate how they could increase predictive power.

More reading: Ensemble learning (Wikipedia)

Q22: How do you ensure you’re not overfitting with a model?
Answer: This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations.

There are three main methods to avoid overfitting:

Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.
Use cross-validation techniques such as k-folds cross-validation.
Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.
More reading: How can I avoid overfitting? (Quora)

Q23: What evaluation approaches would you work to gauge the effectiveness of a machine learning model?
Answer: You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion matrix. What’s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations.

More reading: How to Evaluate Machine Learning Algorithms (Machine Learning Mastery)

Q24: How would you evaluate a logistic regression model?
Answer: A subsection of the question above. You have to demonstrate an understanding of what the typical goals of a logistic regression are (classification, prediction, etc.) and bring up a few examples and use cases.

More reading: Evaluating a logistic regression (CrossValidated), Logistic Regression in Plain English

Q25: What’s the “kernel trick” and how is it useful?
Answer: The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension: instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data.

More reading: Kernel method (Wikipedia)



Machine Learning Interview Questions: Programming
These machine learning interview questions test your knowledge of programming principles you need to implement machine learning principles in practice. Machine learning interview questions tend to be technical questions that test your logic and programming skills: this section focuses more on the latter.

Q26: How do you handle missing or corrupted data in a dataset?
Answer: You could find missing/corrupted data in a dataset and either drop those rows or columns, or decide to replace them with another value.

In Pandas, there are two very useful methods: isnull() and dropna() that will help you find columns of data with missing or corrupted data and drop those values. If you want to fill the invalid values with a placeholder value (for example, 0), you could use the fillna() method.

More reading: Handling missing data (O’Reilly)

Q27: Do you have experience with Spark or big data tools for machine learning?
Answer: You’ll want to get familiar with the meaning of big data for different companies and the different tools they’ll want. Spark is the big data tool most in demand now, able to handle immense datasets with speed. Be honest if you don’t have experience with the tools demanded, but also take a look at job descriptions and see what tools pop up: you’ll want to invest in familiarizing yourself with them.

More reading: 50 Top Open Source Tools for Big Data (Datamation)

Q28: Pick an algorithm. Write the pseudo-code for a parallel implementation.
Answer: This kind of question demonstrates your ability to think in parallelism and how you could handle concurrency in programming implementations dealing with big data. Take a look at pseudocode frameworks such as Peril-L and visualization tools such as Web Sequence Diagrams to help you demonstrate your ability to write code that reflects parallelism.

More reading: Writing pseudocode for parallel programming (Stack Overflow)

Q29: What are some differences between a linked list and an array?
Answer: An array is an ordered collection of objects. A linked list is a series of objects with pointers that direct how to process them sequentially. An array assumes that every element has the same size, unlike the linked list. A linked list can more easily grow organically: an array has to be pre-defined or re-defined for organic growth. Shuffling a linked list involves changing which points direct where—meanwhile, shuffling an array is more complex and takes more memory.

More reading: Array versus linked list (Stack Overflow)

Q30: Describe a hash table.
Answer: A hash table is a data structure that produces an associative array. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing.

More reading: Hash table (Wikipedia)

Q31: Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?
Answer: What’s important here is to define your views on how to properly visualize data and your personal preferences when it comes to tools. Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and tools such as Plot.ly and Tableau.

More reading: 31 Free Data Visualization Tools (Springboard)

Related: 20 Python Interview Questions

Q32: Given two strings, A and B, of the same length n, find whether it is possible to cut both strings at a common point such that the first part of A and the second part of B form a palindrome.
Answer: You’ll often get standard algorithms and data structures questions as part of your interview process as a machine learning engineer that might feel akin to a software engineering interview. In this case, this comes from Google’s interview process. There are multiple ways to check for palindromes—one way of doing so if you’re using a programming language such as Python is to reverse the string and check to see if it still equals the original string, for example. The thing to look out for here is the category of questions you can expect, which will be akin to software engineering questions that drill down to your knowledge of algorithms and data structures. Make sure that you’re totally comfortable with the language of your choice to express that logic.

More reading: Glassdoor machine learning interview questions

Q33: How are primary and foreign keys related in SQL?
Answer: Most machine learning engineers are going to have to be conversant with a lot of different data formats. SQL is still one of the key ones used. Your ability to understand how to manipulate SQL databases will be something you’ll most likely need to demonstrate. In this example, you can talk about how foreign keys allow you to match up and join tables together on the primary key of the corresponding table—but just as useful is to talk through how you would think about setting up SQL tables and querying them. 

More reading: What is the difference between a primary and foreign key in SQL?

Q34: How does XML and CSVs compare in terms of size?
Answer: In practice, XML is much more verbose than CSVs are and takes up a lot more space. CSVs use some separators to categorize and organize data into neat columns. XML uses tags to delineate a tree-like structure for key-value pairs. You’ll often get XML back as a way to semi-structure data from APIs or HTTP responses. In practice, you’ll want to ingest XML data and try to process it into a usable CSV. This sort of question tests your familiarity with data wrangling sometimes messy data formats.  

More reading: How Can XML Be Used?

Q35: What are the data types supported by JSON? 
Answer: This tests your knowledge of JSON, another popular file format that wraps with JavaScript. There are six basic JSON datatypes you can manipulate: strings, numbers, objects, arrays, booleans, and null values. 

More reading: JSON datatypes

Q36: How would you build a data pipeline?
Answer: Data pipelines are the bread and butter of machine learning engineers, who take data science models and find ways to automate and scale them. Make sure you’re familiar with the tools to build data pipelines (such as Apache Airflow) and the platforms where you can host models and pipelines (such as Google Cloud or AWS or Azure). Explain the steps required in a functioning data pipeline and talk through your actual experience building and scaling them in production. 

More reading: 10 Minutes to Building A Machine Learning Pipeline With Apache Airflow

Machine Learning Interview Questions: Company/Industry Specific
These machine learning interview questions deal with how to implement your general machine learning knowledge to a specific company’s requirements. You’ll be asked to create case studies and extend your knowledge of the company and industry you’re applying for with your machine learning skills.

Q37: What do you think is the most valuable data in our business? 
Answer: This question or questions like it really try to test you on two dimensions. The first is your knowledge of the business and the industry itself, as well as your understanding of the business model. The second is whether you can pick how correlated data is to business outcomes in general, and then how you apply that thinking to your context about the company. You’ll want to research the business model and ask good questions to your recruiter—and start thinking about what business problems they probably want to solve most with their data. 

More reading: Three Recommendations For Making The Most Of Valuable Data

Q38: How would you implement a recommendation system for our company’s users?
Answer: A lot of machine learning interview questions of this type will involve the implementation of machine learning models to a company’s problems. You’ll have to research the company and its industry in-depth, especially the revenue drivers the company has, and the types of users the company takes on in the context of the industry it’s in.

More reading: How to Implement A Recommendation System? (Stack Overflow)

Q39: How can we use your machine learning skills to generate revenue?
Answer: This is a tricky question. The ideal answer would demonstrate knowledge of what drives the business and how your skills could relate. For example, if you were interviewing for music-streaming startup Spotify, you could remark that your skills at developing a better recommendation model would increase user retention, which would then increase revenue in the long run.

The startup metrics Slideshare linked above will help you understand exactly what performance indicators are important for startups and tech companies as they think about revenue and growth.

More reading: Startup Metrics for Startups (500 Startups)

Q40: What do you think of our current data process?
machine learning interview questions

Answer: This kind of question requires you to listen carefully and impart feedback in a manner that is constructive and insightful. Your interviewer is trying to gauge if you’d be a valuable member of their team and whether you grasp the nuances of why certain things are set the way they are in the company’s data process based on company or industry-specific conditions. They’re trying to see if you can be an intellectual peer. Act accordingly.

More reading: The Data Science Process Email Course (Springboard)

Machine Learning Interview Questions: General Machine Learning Interest
This series of machine learning interview questions attempts to gauge your passion and interest in machine learning. The right answers will serve as a testament to your commitment to being a lifelong learner in machine learning.

Q41: What are the last machine learning papers you’ve read?
Answer: Keeping up with the latest scientific literature on machine learning is a must if you want to demonstrate an interest in a machine learning position. This overview of deep learning in Nature by the scions of deep learning themselves (from Hinton to Bengio to LeCun) can be a good reference paper and an overview of what’s happening in deep learning — and the kind of paper you might want to cite.

More reading: What are some of the best research papers/books for machine learning?

Q42: Do you have research experience in machine learning?
Answer: Related to the last point, most organizations hiring for machine learning positions will look for your formal experience in the field. Research papers, co-authored or supervised by leaders in the field, can make the difference between you being hired and not. Make sure you have a summary of your research experience and papers ready—and an explanation for your background and lack of formal research experience if you don’t.

Q43: What are your favorite use cases of machine learning models?
Answer: The Quora thread below contains some examples, such as decision trees that categorize people into different tiers of intelligence based on IQ scores. Make sure that you have a few examples in mind and describe what resonated with you. It’s important that you demonstrate an interest in how machine learning is implemented.

More reading: What are the typical use cases for different machine learning algorithms? (Quora)

Q44: How would you approach the “Netflix Prize” competition?
Answer: The Netflix Prize was a famed competition where Netflix offered $1,000,000 for a better collaborative filtering algorithm. The team that won called BellKor had a 10% improvement and used an ensemble of different methods to win. Some familiarity with the case and its solution will help demonstrate you’ve paid attention to machine learning for a while.

More reading: Netflix Prize (Wikipedia)

Q45: Where do you usually source datasets?
Answer: Machine learning interview questions like these try to get at the heart of your machine learning interest. Somebody who is truly passionate about machine learning will have gone off and done side projects on their own, and have a good idea of what great datasets are out there. If you’re missing any, check out Quandl for economic and financial data, and Kaggle’s Datasets collection for another great list.

More reading: 19 Free Public Data Sets For Your First Data Science Project (Springboard)

Q46: How do you think Google is training data for self-driving cars?
Answer: Machine learning interview questions like this one really test your knowledge of different machine learning methods, and your inventiveness if you don’t know the answer. Google is currently using recaptcha to source labeled data on storefronts and traffic signs. They are also building on training data collected by Sebastian Thrun at GoogleX—some of which was obtained by his grad students driving buggies on desert dunes!

More reading: Waymo Tech

Q47: How would you simulate the approach AlphaGo took to beat Lee Sedol at Go?
Answer: AlphaGo beating Lee Sedol, the best human player at Go, in a best-of-five series was a truly seminal event in the history of machine learning and deep learning. The Nature paper above describes how this was accomplished with “Monte-Carlo tree search with deep neural networks that have been trained by supervised learning, from human expert games, and by reinforcement learning from games of self-play.”

More reading: Mastering the game of Go with deep neural networks and tree search (Nature)

Q48: What are your thoughts on GPT-3 and OpenAI’s model?
Answer: GPT-3 is a new language generation model developed by OpenAI. It was marked as exciting because with very little change in architecture, and a ton more data, GPT-3 could generate what seemed to be human-like conversational pieces, up to and including novel-size works and the ability to create code from natural language. There are many perspectives on GPT-3 throughout the Internet — if it comes up in an interview setting, be prepared to address this topic (and trending topics like it) intelligently to demonstrate that you follow the latest advances in machine learning. 

More reading: Language Models are Few-Shot Learners

Q49: What models do you train for fun, and what GPU/hardware do you use?
Answer: This question tests whether you’ve worked on machine learning projects outside of a corporate role and whether you understand the basics of how to resource projects and allocate GPU-time efficiently. Expect questions like this to come from hiring managers that are interested in getting a greater sense behind your portfolio, and what you’ve done independently.

More reading: Where to get free GPU cloud hours for machine learning

Q50: What are some of your favorite APIs to explore? 
Answer: If you’ve worked with external data sources, it’s likely you’ll have a few favorite APIs that you’ve gone through. You can be thoughtful here about the kinds of experiments and pipelines you’ve run in the past, along with how you think about the APIs you’ve used before. 

More reading: Awesome APIs

Q51: How do you think quantum computing will affect machine learning?
Answer: With the recent announcement of more breakthroughs in quantum computing, the question of how this new format and way of thinking through hardware serves as a useful proxy to explain classical computing and machine learning, and some of the hardware nuances that might make some algorithms much easier to do on a quantum machine. Demonstrating some knowledge in this area helps show that you’re interested in machine learning at a much higher level than just implementation details. 

Q1. You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)

Answer: Processing a high dimensional data on a limited memory machine is a strenuous task, your interviewer would be fully aware of that. Following are the methods you can use to tackle such situation:

Since we have lower RAM, we should close all other applications in our machine, including the web browser, so that most of the memory can be put to use.
We can randomly sample the data set. This means, we can create a smaller data set, let’s say, having 1000 variables and 300000 rows and do the computations.
To reduce dimensionality, we can separate the numerical and categorical variables and remove the correlated variables. For numerical variables, we’ll use correlation. For categorical variables, we’ll use chi-square test.
Also, we can use PCA and pick the components which can explain the maximum variance in the data set.
Using online learning algorithms like Vowpal Wabbit (available in Python) is a possible option.
Building a linear model using Stochastic Gradient Descent is also helpful.
We can also apply our business understanding to estimate which all predictors can impact the response variable. But, this is an intuitive approach, failing to identify useful predictors might result in significant loss of information.
Note: For point 4 & 5, make sure you read about online learning algorithms & Stochastic Gradient Descent. These are advanced methods.

 

Q2. Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the components?

Answer: Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, that’s the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesn’t change, it only changes the actual coordinates of the points.
If we don’t rotate the components, the effect of PCA will diminish and we’ll have to select more number of components to explain variance in the data set.

Know more: PCA

 

Q3. You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?

Answer: This question has enough hints for you to start thinking! Since, the data is spread across median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values.

 

Q4. You are given a data set on cancer detection. You’ve build a classification model and achieved an accuracy of 96%. Why shouldn’t you be happy with your model performance? What can you do about it?

Answer: If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:

We can use undersampling, oversampling or SMOTE to make the data balanced.
We can alter the prediction threshold value by doing probability caliberation and finding a optimal threshold using AUC-ROC curve.
We can assign weight to classes such that the minority classes gets larger weight.
We can also use anomaly detection.
Know more: Imbalanced Classification

 

Q5. Why is naive Bayes so ‘naive’ ?

Answer: naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are equally important and independent. As we know, these assumption are rarely true in real world scenario.

 

Q6. Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?

Answer: Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new email would be classified as spam.

Likelihood is the probability of classifying a given observation as 1 in presence of some other variable. For example: The probability that the word ‘FREE’ is used in previous spam message is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in any message.

 

Q7. You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?

Answer: Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.

 

Q8. You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?

Answer: You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals.

This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:

There exist a pattern.
You cannot solve it mathematically (even by writing exponential equations).
You have data on it.
Always look for these three factors to decide if machine learning is a tool to solve a particular problem.

 

Q9. You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?

Answer:  Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.

In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate  a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).

Also, to combat high variance, we can:

Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity.
Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal.
 

Q10. You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?

Answer: Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.

For example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on this data set, the first principal component would exhibit twice the variance than it would exhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more importance on those variable, which is misleading.

 

Q11. After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?

Answer: As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior results when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.

For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions.

 
Q12. How is kNN different from kmeans clustering?

Answer: Don’t get mislead by ‘k’ in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.

kmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels.

kNN algorithm tries to classify an unlabeled observation based on its k (can be any number ) surrounding neighbors. It is also known as lazy learner because it involves minimal training of model. Hence, it doesn’t use training data to make generalization on unseen data set.

 

Q13. How is True Positive Rate and Recall related? Write the equation.

Answer: True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).

Know more: Evaluation Metrics

 

Q14. You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?

Answer: Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is predicted value.   

When intercept term is present, R² value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in higher R².

 

Q15. After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?

Answer: To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.

But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.

Know more: Regression

 

Q16. When is Ridge regression favorable over Lasso regression?

Answer: You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.

Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective.

Know more: Ridge and Lasso Regression

 

Q17. Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?

Answer: After reading this question, you should have understood that this is a classic case of “causation and correlation”. No, we can’t conclude that decrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon.

Therefore, there might be a correlation between global average temperature and number of pirates, but based on this information we can’t say that pirated died because of rise in global average temperature.

Know more: Causation and Correlation

 

Q18. While working on a data set, how do you select important variables? Explain your methods.

Answer: Following are the methods of variable selection you can use:

Remove the correlated variables prior to selecting important variables
Use linear regression and select variables based on p values
Use Forward Selection, Backward Selection, Stepwise Selection
Use Random Forest, Xgboost and plot variable importance chart
Use Lasso Regression
Measure information gain for the available set of features and select top n features accordingly.
 

Q19. What is the difference between covariance and correlation?

Answer: Correlation is the standardized form of covariance.

Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we’ll get different covariances which can’t be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale.

 

Q20. Is it possible capture the correlation between continuous and categorical variable? If yes, how?

Answer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.

 

Q21. Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?

Answer: The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.

In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.

Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.

Know more: Tree based modeling

 

Q22. Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?

Answer: A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data set into purest possible children nodes.

Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. We can calculate Gini as following:

Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).
Calculate Gini for split using weighted Gini score of each node of that split
Entropy is the measure of impurity as given by (for binary class):

Entropy, Decision Tree

Here p and q is probability of success and failure respectively in that node. Entropy is zero when a node is homogeneous. It is maximum when a both the classes are present in a node at 50% – 50%.  Lower entropy is desirable.

 

Q23. You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?

Answer: The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldn’t find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation.

 

Q24. You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?

Answer: In such high dimensional data sets, we can’t use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.

To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.

Among other methods include subset regression, forward stepwise regression.

 

11222Q25. What is convex hull ? (Hint: Think SVM)

Answer: In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.

 

Q26. We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?

Answer: Don’t get baffled at this question. It’s a simple question asking the difference between the two.

Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: let’s say we have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot encoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.

In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables.

 

Q27. What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?

Answer: Neither.

In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we might end up validation on past years, which is incorrect. Instead, we can use forward chaining strategy with 5 fold as shown below:

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]
where 1,2,3,4,5,6 represents “year”.

 

Q28. You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?

Answer: We can deal with them in the following ways:

Assign a unique category to missing values, who knows the missing values might decipher some trend
We can remove them blatantly.
Or, we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.
 

29. ‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?

Answer: The basic idea for this kind of recommendation engine comes from collaborative filtering.

Collaborative Filtering algorithm considers “User Behavior” for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.

Know more: Recommender System

 

Q30. What do you understand by Type I vs Type II error ?

Answer: Type I error is committed when the null hypothesis is true and we reject it, also known as a ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’.

In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).

 

Q31. You are working on a classification problem. For validation purposes, you’ve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?

Answer: In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesn’t takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also.

 

Q32. You have been asked to evaluate a regression model based on R², adjusted R² and tolerance. What will be your criteria?

Answer: Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.

We will consider adjusted R² as opposed to R² to evaluate model fit because R² increases irrespective of improvement in prediction accuracy as we add more variables. But, adjusted R² would only increase if an additional variable improves the accuracy of model, otherwise stays same. It is difficult to commit a general threshold value for adjusted R² because it varies between data sets. For example: a gene mutation data set might result in lower adjusted R² and still provide fairly good predictions, as compared to a stock market data where lower adjusted R² implies that model is not good.

 

Q33. In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?

Answer: We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option.

Example: Think of a chess board, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements.

 

Q34. Explain machine learning to me like a 5 year old.

Answer: It’s simple. It’s just like how babies learn to walk. Every time they fall down, they learn (unconsciously) & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that again’. In order to avoid that pain, they try harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm.

This is how a machine works & develops intuition from its environment.

Note: The interview is only trying to test if have the ability of explain complex concepts in simple terms.

 

Q35. I know that a linear regression model is generally evaluated using Adjusted R² or F value. How would you evaluate a logistic regression model?

Answer: We can use the following methods:

Since logistic regression is used to predict probabilities, we can use AUC-ROC curve along with confusion matrix to determine its performance.
Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.
Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.
Know more: Logistic Regression

 

Q36. Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?

Answer: You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model.

If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then we’ll use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc.

In short, there is no one master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use.

 

Q37. Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?

Answer: For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.

 

Q38. When does regularization becomes necessary in Machine Learning?

Answer: Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing).

 

Q39. What do you understand by Bias Variance trade off?

Answer:  The error emerging from any model can be broken down into three components mathematically. Following are these component :

error of a model

Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends. Variance on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training.

 

Q40. OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.

Answer: OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words,

Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting in minimum distance between actual and predicted values. Maximum Likelihood helps in choosing the the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data.

1) Let A and B be events on the same sample space, with P (A) = 0.6 and P (B) = 0.7. Can these two events be disjoint?

A) Yes

B) No

Solution: (B)
These two events cannot be disjoint because P(A)+P(B) >1.

P(AꓴB) = P(A)+P(B)-P(AꓵB).

An event is disjoint if P(AꓵB) = 0. If A and B are disjoint P(AꓴB) = 0.6+0.7 = 1.3

And Since probability cannot be greater than 1, these two mentioned events cannot be disjoint.

 

2) Alice has 2 kids and one of them is a girl. What is the probability that the other child is also a girl? 

You can assume that there are an equal number of males and females in the world.

A) 0.5
B) 0.25
C) 0.333
D) 0.75
Solution: (C)
The outcomes for two kids can be {BB, BG, GB, GG}

Since it is mentioned that one of them is a girl, we can remove the BB option from the sample space. Therefore the sample space has 3 options while only one fits the second condition. Therefore the probability the second child will be a girl too is 1/3.

 

3) A fair six-sided die is rolled twice. What is the probability of getting 2 on the first roll and not getting 4 on the second roll?

A) 1/36
B) 1/18
C) 5/36
D) 1/6
E) 1/3
Solution: (C)
The two events mentioned are independent. The first roll of the die is independent of the second roll. Therefore the probabilities can be directly multiplied.

P(getting first 2) = 1/6

P(no second 4) = 5/6

Therefore P(getting first 2 and no second 4) = 1/6* 5/6 = 5/36

 
4) 

A) True

B) False

Solution: (A)
P(AꓵCc) will be only P(A). P(only A)+P(C) will make it P(AꓴC). P(BꓵAcꓵCc) is P(only B) Therefore P(AꓴC) and P(only B) will make P(AꓴBꓴC)

 

5) Consider a tetrahedral die and roll it twice. What is the probability that the number on the first roll is strictly higher than the number on the second roll?

Note: A tetrahedral die has only four sides (1, 2, 3 and 4). 

A) 1/2
B) 3/8
C) 7/16
D) 9/16
Solution: (B)
(1,1)	(2,1)	(3,1)	(4,1)
(1,2)	(2,2)	(3,2)	(4,2)
(1,3)	(2,3)	(3,3)	(4,3)
(1,4)	(2,4)	(3,4)	(4,4)
There are 6 out of 16 possibilities where the first roll is strictly higher than the second roll.

 

6) Which of the following options cannot be the probability of any event? 

A) -0.00001
B) 0.5
C) 1.001

A) Only A
B) Only B
C) Only C
D) A and B
E) B and C
F) A and C
Solution: (F)

Probability always lie within 0 to 1. 

 

7) Anita randomly picks 4 cards from a deck of 52-cards and places them back into the deck ( Any set of 4 cards is equally likely ). Then, Babita randomly chooses 8 cards out of the same deck ( Any set of 8 cards is equally likely). Assume that the choice of 4 cards by Anita and the choice of 8 cards by Babita are independent. What is the probability that all 4 cards chosen by Anita are in the set of 8 cards chosen by Babita?

A)48C4 x 52C4
B)48C4 x 52C8

C)48C8 x 52C8

D) None of the above
Solution: (A)

The total number of possible combination would be 52C4 (For selecting 4 cards by Anita) * 52C8 (For selecting 8 cards by Babita).

Since, the 4 cards that Anita chooses is among the 8 cards which Babita has chosen, thus the number of combinations possible is 52C4 (For selecting the 4 cards selected by Anita) * 48C4 (For selecting any other 4 cards by Babita, since the 4 cards selected by Anita are common)

 

Question Context 8:

A player is randomly dealt a sequence of 13 cards from a deck of 52-cards. All sequences of 13 cards are equally likely. In an equivalent model, the cards are chosen and dealt one at a time. When choosing a card, the dealer is equally likely to pick any of the cards that remain in the deck.

8) If you dealt 13 cards, what is the probability that the 13th card is a King?

A) 1/52
B) 1/13

C) 1/26

D) 1/12
Solution: (B)

Since we are not told anything about the first 12 cards that are dealt, the probability that the 13th card dealt is a King, is the same as the probability that the first card dealt, or in fact any particular card dealt is a King, and this equals: 4/52

 

9) A fair six-sided die is rolled 6 times. What is the probability of getting all outcomes as unique?

A) 0.01543
B) 0.01993
C) 0.23148
D) 0.03333
Solution: (A)

For all the outcomes to be unique, we have 6 choices for the first turn, 5 for the second turn, 4 for the third turn and so on

Therefore the probability if getting all unique outcomes will be equal to 0.01543

 

10) A group of 60 students is randomly split into 3 classes of equal size. All partitions are equally likely. Jack and Jill are two students belonging to that group. What is the probability that Jack and Jill will end up in the same class? 

A) 1/3
B) 19/59
C) 18/58
D) 1/2
Solution: (B)

Assign a different number to each student from 1 to 60. Numbers 1 to 20 go in group 1, 21 to 40 go to group 2, 41 to 60 go to group 3.

All possible partitions are obtained with equal probability by a random assignment if these numbers, it doesn’t matter with which students we start, so we are free to start by assigning a random number to Jack and then we assign a random number to Jill. After Jack has been assigned a random number there are 59 random numbers available for Jill and 19 of these will put her in the same group as Jack. Therefore the probability is 19/59

 

11) We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2 and for each toss of coin B, the probability of getting Heads is 1/3. All tosses of the same coin are independent. We select a coin at random and toss it till we get a head. The probability of selecting coin A is ¼ and coin B is 3/4. What is the expected number of tosses to get the first heads?

A) 2.75
B) 3.35
C) 4.13
D) 5.33
Solution: (A)

If coin A is selected then the number of times the coin would be tossed for a guaranteed Heads is 2, similarly, for coin B it is 3. Thus the number of times would be

Tosses = 2 * (1/4)[probability of selecting coin A] + 3*(3/4)[probability of selecting coin B]

             = 2.75

 

12) Suppose a life insurance company sells a $240,000 one year term life insurance policy to a 25-year old female for $210. The probability that the female survives the year is .999592. Find the expected value of this policy for the insurance company.

A) $131
B) $140

C) $112

D) $125
Solution: (C)

P(company loses the money ) = 0.99592

P(company does not lose the money ) = 0.000408

The amount of money company loses if it loses = 240,000 – 210 = 239790

While the money it gains is $210

Expected money the company will have to give = 239790*0.000408 = 97.8

Expect money company gets = 210.

Therefore the value = 210 – 98 = $112

 

13) 

A) True

B) False

Solution: (A)

The above statement is true. You would need to know that

P(A/B) = P(AꓵB)/P(B)

P(CcꓵA|A) = P(CcꓵAꓵA)/P(A) = P(CcꓵA)/P(A)

P(B|A ꓵ Cc) = P(AꓵBꓵCc)/P(A ꓵ Cc)

Multiplying the three we would get – P(AꓵBꓵCc), hence the equations holds true

 

14) When an event A independent of itself?

A) Always
B) If and only if P(A)=0
C) If and only if P(A)=1
D) If and only if P(A)=0 or 1
Solution: (D)

The event can only be independent of itself when either there is no chance of it happening or when it is certain to happen. Event A and B is independent when P(AꓵB) = P(A)*P(B). Now if B=A, P(AꓵA) = P(A) when P(A) = 0 or 1.

 

15) Suppose you’re in the final round of “Let’s make a deal” game show and you are supposed to choose from three doors – 1, 2 & 3. One of the three doors has a car behind it and other two doors have goats. Let’s say you choose Door 1 and the host opens Door 3 which has a goat behind it. To assure the probability of your win, which of the following options would you choose.

A) Switch your choice
B) Retain your choice
C) It doesn’t matter probability of winning or losing is the same with or without revealing one door
Solution: (A)

I would recommend reading this article for a detailed discussion of the Monty Hall’s Problem. 

 

16) Cross-fertilizing a red and a white flower produces red flowers 25% of the time. Now we cross-fertilize five pairs of red and white flowers and produce five offspring. What is the probability that there are no red flower plants in the five offspring? 

A) 23.7%
B) 37.2%
C) 22.5%
D) 27.3%
Solution: (A)

The probability of offspring being Red is 0.25, thus the probability of the offspring not being red is 0.75. Since all the pairs are independent of each other, the probability that all the offsprings are not red would be (0.75)5 = 0.237. You can think of this as a binomial with all failures.

 

17) A roulette wheel has 38 slots – 18 red, 18 black, and 2 green. You play five games and always bet on red slots. How many games can you expect to win?

A) 1.1165

B) 2.3684C) 2.6316

C) 2.6316

D) 4.7368
Solution: (B)

The probability that it would be Red in any spin is 18/38. Now, you are playing the game 5 times and all the games are independent of each other. Thus, the number of games that you can win would be 5*(18/38) = 2.3684

 

18) A roulette wheel has 38 slots, 18 are red, 18 are black, and 2 are green. You play five games and always bet on red. What is the probability that you win all the 5 games?

A) 0.0368
B) 0.0238
C) 0.0526
D) 0.0473
Solution: (B)

The probability that it would be Red in any spin is 18/38. Now, you are playing for game 5 times and all the games are independent of each other. Thus, the probability that you win all the games is (18/38)5 = 0.0238

 

19) Some test scores follow a normal distribution with a mean of 18 and a standard deviation of 6. What proportion of test takers have scored between 18 and 24?

A) 20%
B) 22%
C) 34%
D) None of the above
Solution: (C)

So here we would need to calculate the Z scores for value being 18 and 24. We can easily doing that by putting sample mean as 18 and population mean as 18 with σ = 6 and calculating Z. Similarly we can calculate Z for sample mean as 24.

Z= (X-μ)/σ

Therefore for 26 as X,

Z = (18-18)/6  = 0 , looking at the Z table we find 50% people have scores below 18.

For 24 as X

Z = (24-18)/6  = 1, looking at the Z table we find 84% people have scores below 24.

Therefore around 34% people have scores between 18 and 24.

 

20) A jar contains 4 marbles. 3 Red & 1 white. Two marbles are drawn with replacement after each draw. What is the probability that the same color marble is drawn twice?

A) 1/2
B) 1/3
C) 5/8
D) 1/8
Solution: (C)

If the marbles are of the same color then it will be 3/4 * 3/4 + 1/4 * 1/4 = 5/8.

 

21) Which of the following events is most likely? 

A) At least one 6, when 6 dice are rolled

B) At least 2 sixes when 12 dice are rolled

C) At least 3 sixes when 18 dice are rolled

D) All the above have same probability
Solution: (A)

Probability of ‘6’ turning up in a roll of dice is P(6) = (1/6) & P(6’) = (5/6). Thus, probability of

∞ Case 1: (1/6) * (5/6)5 = 0.06698

∞ Case 2: (1/6)2 * (5/6)10 = 0.00448

∞ Case 3: (1/6)3 * (5/6)15 = 0.0003

Thus, the highest probability is Case 1

 

22) Suppose you were interviewed for a technical role. 50% of the people who sat for the first interview received the call for second interview. 95% of the people who got a call for second interview felt good about their first interview. 75% of people who did not receive a second call, also felt good about their first interview. If you felt good after your first interview, what is the probability that you will receive a second interview call?

A) 66%
B) 56%

C) 75%

D) 85%
Solution: (B)

Let’s assume there are 100 people that gave the first round of interview. The 50 people got the interview call for the second round. Out of this 95 % felt good about their interview, which is 47.5. 50 people did not get a call for the interview; out of which 75% felt good about, which is 37.5. Thus, the total number of people that felt good after giving their interview is (37.5 + 47.5) 85. Thus, out of 85 people who felt good, only 47.5 got the call for next round. Hence, the probability of success is (47.5/85) = 0.558.

Another more accepted way to solve this problem is the Baye’s theorem. I leave it to you to check for yourself. 

 

23) A coin of diameter 1-inches is thrown on a table covered with a grid of lines each two inches apart. What is the probability that the coin lands inside a square without touching any of the lines of the grid? You can assume that the person throwing has no skill in throwing the coin and is throwing it randomly. 

You can assume that the person throwing has no skill in throwing the coin and is throwing it randomly.

A) 1/2
B) 1/4
C) Π/3
D) 1/3
Solution: (B)

Think about where all the center of the coin can be when it lands on 2 inches grid and it not touching the lines of the grid.



If the yellow region is a 1 inch square and the outside square is of 2 inches. If the center falls in the yellow region, the coin will not touch the grid line. Since the total area is 4 and the area of the yellow region is 1, the probability is ¼ .

 

24) There are a total of 8 bows of 2 each of green, yellow, orange & red. In how many ways can you select 1 bow? 



A) 1
B) 2
C) 4
D) 8
Solution: (C)

You can select one bow out of four different bows, so you can select one bow in four different ways. 

 

25) Consider the following probability density function: What is the probability for X≤6 i.e. P(x≤6)



What is the probability for X≤6 i.e. P(x≤6)

A) 0.3935
B) 0.5276
C) 0.1341
D) 0.4724
Solution: (B)

To calculate the area of a particular region of a probability density function, we need to integrate the function under the bounds of the values for which we need to calculate the probability.

Therefore on integrating the given function from 0 to 6, we get 0.5276

 

26) In a class of 30 students, approximately what is the probability that two of the students have their birthday on the same day (defined by same day and month) (assuming it’s not a leap year)?

For example – Students with birthday 3rd Jan 1993 and 3rd Jan 1994 would be a favorable event.

A) 49%

B) 52%
C) 70%

D) 35%
Solution: (C)

The total number of combinations possible for no two persons to have the same birthday in a class of 30 is 30 * (30-1)/2 = 435.

Now, there are 365 days in a year (assuming it’s not a leap year). Thus, the probability of people having a different birthday would be 364/365. Now there are 870 combinations possible. Thus, the probability that no two people have the same birthday is (364/365)^435 = 0.303.

Thus, the probability that two people would have their birthdays on the same date would be 1 – 0.303 = 0.696

 

27) Ahmed is playing a lottery game where he must pick 2 numbers from 0 to 9 followed by an English alphabet (from 26-letters). He may choose the same number both times.

If his ticket matches the 2 numbers and 1 letter drawn in order, he wins the grand prize and receives $10405. If just his letter matches but one or both of the numbers do not match, he wins $100. Under any other circumstance, he wins nothing. The game costs him $5 to play. Suppose he has chosen 04R to play.

What is the expected net profit from playing this ticket?

A) $-2.81
B) $2.81C) $-1.82

C) $-1.82

D) $1.82
Solution: (B)

Expected value in this case

E(X) = P(grand prize)*(10405-5)+P(small)(100-5)+P(losing)*(-5)

P(grand prize)=  (1/10)*(1/10)*(1/26)

P(small) = 1/26-1/2600, the reason we need to do this is we need to exclude the case where he gets the letter right and also the numbers rights. Hence, we need to remove the scenario of getting the letter right.

P(losing ) = 1-1/26-1/2600

Therefore we can fit in the values to get the expected value as $2.81

 

28) Assume you sell sandwiches. 70% people choose egg, and the rest choose chicken. What is the probability of selling 2 egg sandwiches to the next 3 customers?

A) 0.343
B) 0.063
C) 0.44
D) 0.027
Solution: (C)

The probability of selling Egg sandwich is 0.7 & that of a chicken sandwich is 0.3. Now, the probability that next 3 customers would order 2 egg sandwich is 3 * 0.7 * 0.7 *0.3 = 0.44. They can order them in any sequence, the probabilities would still be the same.

 

Question context: 29 – 30

HIV is still a very scary disease to even get tested for. The US military tests its recruits for HIV when they are recruited. They are tested on three rounds of Elisa( an HIV test) before they are termed to be positive.

The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.

29) What is the probability that a recruit has HIV, given he tested positive on first Elisa test? The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.

A) 12%
B) 80%
C) 42%
D) 14%
Solution: (A)

I recommend going through the Bayes updating section of this article for the understanding of the above question.

 

30) What is the probability of having HIV, given he tested positive on Elisa the second time as well.

The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.

A) 20%
B) 42%
C) 93%
D) 88%
Solution: (C)

I recommend going through the Bayes updating section of this article for the understanding of the above question.

 

31) Suppose you’re playing a game in which we toss a fair coin multiple times. You have already lost thrice where you guessed heads but a tails appeared. Which of the below statements would be correct in this case?

A) You should guess heads again since the tails has already occurred thrice and its more likely for heads to occur now
B) You should say tails because guessing heads is not making you win
C) You have the same probability of winning in guessing either, hence whatever you guess there is just a 50-50 chance of winning or losing
D) None of these
Solution: (C)

This is a classic problem of gambler’s fallacy/monte carlo fallacy, where the person falsely starts to think that the results should even out in a few turns. The gambler starts to believe that if we have received 3 heads, you should receive a 3 tails. This is however not true. The results would even out only in infinite number of trials.

 

32) The inference using the frequentist approach will always yield the same result as the Bayesian approach.

A) TRUE
B) FALSE
Solution: (B)

The frequentist Approach is highly dependent on how we define the hypothesis while Bayesian approach helps us update our prior beliefs. Therefore the frequentist approach might result in an opposite inference if we declare the hypothesis differently. Hence the two approaches might not yield the same results.

 

33) Hospital records show that 75% of patients suffering from a disease die due to that disease. What is the probability that 4 out of the 6 randomly selected patients recover?

A) 0.17798
B) 0.13184
C) 0.03295
D) 0.35596
Solution: (C)

Think of this as a binomial since there are only 2 outcomes, either the patient dies or he survives.

Here n =6, and x=4.  p=0.25(probability if living(success)) q = 0.75(probability of dying(failure))

P(X) = nCx pxqn-x = 6C4 (0.25)4(0.75)2 = 0.03295

 

34) The students of a particular class were given two tests for evaluation. Twenty-five percent of the class cleared both the tests and forty-five percent of the students were able to clear the first test.

Calculate the percentage of students who passed the second test given that they were also able to pass the first test.

A) 25%
B) 42%

C) 55%
D) 45%
Solution: (C)

This is a simple problem of conditional probability. Let A be the event of passing in first test.

B is the event of passing in the second test.

P(AꓵB) is passing in both the events

P(passing in second given he passed in the first one) = P(AꓵB)/P(A)

= 0.25/0.45 which is around 55%

 

35) While it is said that the probabilities of having a boy or a girl are the same, let’s assume that the actual probability of having a boy is slightly higher at 0.51. Suppose a couple plans to have 3 children. What is the probability that exactly 2 of them will be boys?

A) 0.38
B) 0.48
C) 0.58
D) 0.68
E) 0.78
Solution: (A)

Think of this as a binomial distribution where getting a success is a boy and failure is a girl. Therefore we need to calculate the probability of getting 2 out of three successes.

P(X) = nCx pxqn-x = 3C2 (0.51)2(0.49)1 = 0.382

 

36) Heights of 10 year-olds, regardless of gender, closely follow a normal distribution with mean 55 inches and standard deviation 6 inches. Which of the following is true?

A) We would expect more number of 10 year-olds to be shorter than 55 inches than the number of them who are taller than 55 inches
B) Roughly 95% of 10 year-olds are between 37 and 73 inches tall
C) A 10-year-old who is 65 inches tall would be considered more unusual than a 10-year-old who is 45 inches tall

D) None of these
Solution: (D)

None of the above statements are true.

 

37) About 30% of human twins are identical, and the rest are fraternal. Identical twins are necessarily the same sex, half are males and the other half are females. One-quarter of fraternal twins are both males, one-quarter both female, and one-half are mixed: one male, one female. You have just become a parent of twins and are told they are both girls. Given this information, what is the probability that they are identical?

A) 50%

B) 72%
C) 46%
D) 33%
 

Solution: (C)

This is a classic problem of Bayes theorem.  

P(I) denoted Probability of being identical and P(~I) denotes Probability of not being identical

P(Identical) = 0.3

P(not Identical)= 0.7

P(FF|I)= 0.5

P(MM|I)= 0.5

P(MM|~I)= 0.25

P(FF|~I)= 0.25

P(FM|~I)= 0.25

P(I|FF) = 0.46

 

38) Rob has fever and the doctor suspects it to be typhoid. To be sure, the doctor wants to conduct the test. The test results positive when the patient actually has typhoid 80% of the time. The test gives positive when the patient does not have typhoid 10% of the time. If 1% of the population has typhoid, what is the probability that Rob has typhoid provided he tested positive?

A) 12%
B) 7%
C) 25%
D) 31.5%
Solution: (B)

We need to find the probability of having typhoid given he tested positive.

=P(testing +ve and having typhoid) / P(testing positive)

= = 0.074

 

39) Jack is having two coins in his hand. Out of the two coins, one is a real coin and the second one is a faulty one with Tails on both sides. He blindfolds himself to choose a random coin and tosses it in the air. The coin falls down with Tails facing upwards. What is the probability that this tail is shown by the faulty coin?

A) 1/3
B) 2/3
C) 1/2
D) 1/4
Solution: (B)

We need to find the probability of the coin being faulty given that it showed tails.

P(Faulty) = 0.5

P(getting tails) = 3/4

P(faulty and tails) =0.5*1 = 0.5

Therefore the probability of coin being faulty given that it showed tails would be 2/3

 

40) A fly has a life between 4-6 days. What is the probability that the fly will die at exactly 5 days?

A) 1/2

B) 1/4

C) 1/3
D) 0
Solution: (D)

Here since the probabilities are continuous, the probabilities form a mass function. The probability of a certain event is calculated by finding the area under the curve for the given conditions. Here since we’re trying to calculate the probability of the fly dying at exactly 5 days – the area under the curve would be 0. Also to come to think of it, the probability if dying at exactly 5 days is impossible for us to even figure out since we cannot measure with infinite precision if it was exactly 5 days.

Understanding the Mathematical formulation of Correlation coefficient
The most widely used correlation coefficient is Pearson Coefficient. Here is the mathematical formula to derive Pearson Coefficient.

pearson

Explanation: It simply is the ratio of co-variance of two variables to a product of variance (of the variables). It takes a value between +1 and -1. An extreme value on both the side means they are strongly correlated with each other. A value of zero indicates a NIL correlation but not a non-dependence. You’ll understand this clearly in one of the following answers.

 

Answer – 1: Correlation vs. Dependency
A non-dependency between two variable means a zero correlation. However the inverse is not true. A zero correlation can even have a perfect dependency. Here is an example :

parabola

In this scenario, where the square of x is linearly dependent on y (the dependent variable), everything to the right of y axis is negative correlated and to left is positively correlated. So what will be the Pearson Correlation coefficient?

If you do the math, you will see a zero correlation between these two variables. What does that mean? For a pair of variables which are perfectly dependent on each other, can also give you a zero correlation.

Must remember tip: Correlation quantifies the linear dependence of two variables. It cannot capture non-linear relationship between two variables.

 

Good Read: Must Read Books in Analytics / Data Science

 

Answer – 2: Is Correlation Transitive?
Suppose that X, Y, and Z are random variables. X and Y are positively correlated and Y and Z are likewise positively correlated. Does it follow that X and Z must be positively correlated?

As we shall see by example, the answer is (perhaps surprisingly) “No.” We may prove that if the correlations are sufficiently close to 1, then X and Z must be positively correlated.

Let’s assume C(x,y) is the correlation coefficient between x and y. Like wise we have C(x,z) and C(y,z). Here is an equation which comes from solving correlation equation mathematically :

C(x,y) = C(y,z) * C(z,x) - Square Root ( (1 - C(y,z)^2 ) *  (1 - C(z,x)^2 ) )
Now if we want C(x,y) to be more than zero , we basically want the RHS of above equation to be positive. Hence, you need to solve for :

 C(y,z) * C(z,x) > Square Root ( (1 - C(y,z)^2 ) *  (1 - C(z,x)^2 ) )
We can actually solve the above equation for both C(y,z) > 0 and C(y,z) < 0 together by squaring both sides. This will finally give the result as C(x,y) is a non zero number if following equation holds true:

C(y,z) ^ 2 + C(z,x) ^ 2 > 1
Wow, this is an equation for a circle. Hence the following plot will explain everything :

correlation circle

If the two known correlation are in the A zone, the third correlation will be positive. If they lie in the B zone, the third correlation will be negative. Inside the circle, we cannot say anything about the relationship. A very interesting insight here is that even if C(y,z) and C(z,x) are 0.5, C(x,y) can actually also be negative.

 

Answer – 3: Is Pearson coefficient sensitive to outliers?
The answer is Yes. Even a single outlier can change the direction of the coefficient. Here are a few cases, all of which have the same correlation coefficient of 0.81 :

correlation scenarios

Consider the last two graphs(X 3Y3 and X 4Y4). X3Y3 is clearly a case of perfect correlation where a single outlier brings down the coefficient significantly. The last graph is complete opposite, the correlation coefficient becomes a high positive number because of a single outlier. Conclusively, this turns out to be the biggest concern with correlation coefficient, it is highly influenced by the outliers.

 

Check your potential: Should I become a Data Scientist?

 

Answer – 4: Does causation imply correlation?
If you have read our above three answers, I am sure you will be able to answer this one. The answer is No, because causation can also lead to a non-linear relationship. Let’s understand how!

Below is the graph showing density of water from 0 to 12 degree Celsius. We know that density is an effect of changing temperature. But, density can reach its maximum value at 4 degree Celsius. Therefore, it will not be linearly correlated to the temperature.

water

 

 

Answer – 5: Difference between Correlation and Simple Linear Regression
These two are really close. So let’s start with a few things which are common for both.

The square of Pearson’s correlation coefficient is the same as the one in simple linear regression
Neither simple linear regression nor correlation answer questions of causality directly. This point is important, because I’ve met people thinking that simple regression can magically allow an inference that X causes. That’s preposterous belief.
What’s the difference between correlation and simple linear regression?

Now let’s think of few differences between the two. Simple linear regression gives much more information about the relationship than Pearson Correlation. Here are a few things which regression will give but correlation coefficient will not.

The slope in  a linear regression gives the marginal change in output/target variable by changing the independent variable by unit distance. Correlation has no slope.
The intercept in a linear regression gives the value of target variable if one of the input/independent variable is set zero. Correlation does not have this information.
Linear regression can give you a prediction given all the input variables. Correlation analysis does not predict anything.
 

Answer – 6: Pearson vs. Spearman
The simplest answer here is Pearson captures how linearly dependent are the two variables whereas Spearman captures the monotonic behavior of the relation between the variables.

For instance consider following relationship :

y = exp ( x )

Here you will find Pearson coefficient to be 0.25 but the Spearman coefficient to be 1. As a thumb rule, you should only begin with Spearman when you have some initial hypothesis of the relation being non-linear. Otherwise, we generally try Pearson first and if that is low, try Spearman. This way you know whether the variables are linearly related or just have a monotonic behavior.

 

Answer – 7: Correlation vs. co-variance
If you skipped the mathematical formula of correlation at the start of this article, now is the time to revisit the same.

Correlation is simply the normalized co-variance with the standard deviation of both the factors. This is done to ensure we get a number between +1 and -1. Co-variance is very difficult to compare as it depends on the units of the two variable. It might come out to be the case that marks of student is more correlated to his toe nail in mili-meters than it is to his attendance rate.

This is just because of the difference in units of the second variable. Hence, we see a need to normalize this co-variance with some spread to make sure we compare apples with apples. This normalized number is known as the correlation.

1) Which of these measures are used to analyze the central tendency of data?

A) Mean and Normal Distribution

B) Mean, Median and Mode

C) Mode, Alpha & Range

D) Standard Deviation, Range and Mean

E) Median, Range and Normal Distribution

Solution: (B)
The mean, median, mode are the three statistical measures which help us to analyze the central tendency of data. We use these measures to find the central value of the data to summarize the entire data set.

 

2) Five numbers are given: (5, 10, 15, 5, 15). Now, what would be the sum of deviations of individual data points from their mean?

A) 10

B)25

C) 50

D) 0

E) None of the above

Solution: (D)
The sum of deviations of the individual will always be 0.

 

3) A test is administered annually. The test has a mean score of 150 and a standard deviation of 20. If Ravi’s z-score is 1.50, what was his score on the test?

A) 180
B) 130
C) 30
D) 150
E) None of the above

Solution: (A)
X= μ+Zσ where μ is the mean,  σ is the standard deviation and X is the score we’re calculating. Therefore X = 150+20*1.5 = 180

 

4) Which of the following measures of central tendency will always change if a single value in the data changes?

A) Mean

B) Median

C) Mode

D) All of these

Solution: (A)
The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset.

 

5) Below, we have represented six data points on a scale where vertical lines on scale represent unit.



Which of the following line represents the mean of the given data points, where the scale is divided into same units?

A) A
B) B
C) C
D) D

Solution: (C)
It’s a little tricky to visualize this one by just looking at the data points. We can simply substitute values to understand the mean. Let A be 1, B be 2, C be 3 and so on. The data values as shown will become {1,1,1,4,5,6} which will have mean to be 18/6 = 3 i.e. C.

 

6) If a positively skewed distribution has a median of 50, which of the following statement is true?

A) Mean is greater than 50
B) Mean is less than 50
C) Mode is less than 50
D) Mode is greater than 50
E) Both A and C
F) Both B and D

Solution: (E)

Below are the distributions for Negatively, Positively and no skewed curves.



As we can see for a positively skewed curve, Mode<Median<Mean. So if median is 50, mean would be more than 50 and mode will be less than 50.

7) Which of the following is a possible value for the median of the below distribution?



A) 32
B) 26
C) 17
D) 40

Solution: (B)

To answer this one we need to go to the basic definition of a median. Median is the value which has roughly half the values before it and half the values after. The number of values less than 25 are (36+54+69 = 159) and the number of values greater than 30 are (55+43+25+22+17= 162). So the median should lie somewhere between 25 and 30. Hence 26 is a possible value of the median.

 

8) Which of the following statements are true about Bessels Correction while calculating a sample standard deviation?

Bessels correction is always done when we perform any operation on a sample data.
Bessels correction is used when we are trying to estimate population standard deviation from the sample.
Bessels corrected standard deviation is less biased.
A)  Only 2

B) Only 3

C) Both 2 and 3

D) Both 1 and 3

Solution: (C)

Contrary to the popular belief Bessel’s correction should not be always done. It’s basically done when we’re trying to estimate the population standard deviation using the sample standard deviation. The bias is definitely reduced as the standard deviation will now(after correction) be depicting the dispersion of the population more than that of the sample.

 

9) If the variance of a dataset is correctly computed with the formula using (n – 1) in the denominator, which of the following option is true?

A) Dataset is a sample
B) Dataset is a population
C) Dataset could be either a sample or a population
D) Dataset is from a census
E) None of the above

Solution: (A)

If the variance has n-1 in the formula, it means that the set is a sample. We try to estimate the population variance by dividing the sum of squared difference with the mean with n-1.

When we have the actual population data we can directly divide the sum of squared differences with n instead of n-1.

 

10) [True or False] Standard deviation can be negative.

A) TRUE

B) FALSE

Solution: (B)

Below is the formula for standard deviation



Since the differences are squared, added and then rooted, negative standard deviations are not possible.

 

11) Standard deviation is robust to outliers?

A) True

B) False

Solution: (B)

If you look at the formula for standard deviation above, a very high or a very low value would increase standard deviation as it would be very different from the mean. Hence outliers will effect standard deviation.

 

12) For the below normal distribution, which of the following option holds true ?

σ1, σ2 and σ3 represent the standard deviations for curves 1, 2 and 3 respectively.


A) σ1> σ2> σ3

B) σ1< σ2< σ3

C) σ1= σ2= σ3

D) None

Solution: (B)

From the definition of normal distribution, we know that the area under the curve is 1 for all the 3 shapes. The curve 3 is more spread and hence more dispersed (most of values being within 40-160). Therefore it will have the highest standard deviation. Similarly, Curve 1 has a very low range and all the values are in a small range of 80-120. Hence, curve 1 has the least standard deviation.

 

13) What would be the critical values of Z for 98% confidence interval for a two-tailed test ?

A) +/- 2.33
B) +/- 1.96
C) +/- 1.64
D) +/- 2.55

Solution: (A)

We need to look at the z table for answering this. For a 2 tailed test, and a 98% confidence interval, we should check the area before the z value as 0.99 since 1% will be on the left side of the mean and 1% on the right side. Hence we should check for the z value for area>0.99. The value will be +/- 2.33

 

14) [True or False] The standard normal curve is symmetric about 0 and the total area under it is 1.

A)TRUE

B) FALSE

Solution: (A)

By the definition of the normal curve, the area under it is 1 and is symmetric about zero. The mean, median and mode are all equal and 0. The area to the left of mean is equal to the area on the right of mean. Hence it is symmetric.

 

Context for Questions 15-17

Studies show that listening to music while studying can improve your memory. To demonstrate this, a researcher obtains a sample of 36 college students and gives them a standard memory test while they listen to some background music. Under normal circumstances (without music), the mean score obtained was 25 and standard deviation is 6. The mean score for the sample after the experiment (i.e With music) is 28.

15) What is the null hypothesis in this case?

A) Listening to music while studying will not impact memory.
B) Listening to music while studying may worsen memory.
C) Listening to music while studying may improve memory.
D) Listening to music while studying will not improve memory but can make it worse.

Solution: (D)

The null hypothesis is generally assumed statement, that there is no relationship in the measured phenomena. Here the null hypothesis would be that there is no relationship between listening to music and improvement in memory.

 

16) What would be the Type I error?

A) Concluding that listening to music while studying improves memory, and it’s right.
B) Concluding that listening to music while studying improves memory when it actually doesn’t.
C) Concluding that listening to music while studying does not improve memory but it does.

Solution: (B)

Type 1 error means that we reject the null hypothesis when its actually true. Here the null hypothesis is that music does not improve memory. Type 1 error would be that we reject it and say that music does improve memory when it actually doesn’t.

 

17) After performing the Z-test, what can we conclude ____ ?

A) Listening to music does not improve memory.

B)Listening to music significantly improves memory at p

C) The information is insufficient for any conclusion.

D) None of the above

Solution: (B)

Let’s perform the Z test on the given case. We know that the null hypothesis is that listening to music does not improve memory.

Alternate hypothesis is that listening to music does improve memory.

In this case the standard error i.e. 

The Z score for a sample mean of 28 from this population is



Z critical value for α = 0.05 (one tailed) would be 1.65 as seen from the z table.

Therefore since the Z value observed is greater than the Z critical value, we can reject the null hypothesis and say that listening to music does improve the memory with 95% confidence.

 

18) A researcher concludes from his analysis that a placebo cures AIDS. What type of error is he making?

A) Type 1 error

B) Type 2 error

C) None of these. The researcher is not making an error.

D) Cannot be determined

Solution: (D)

By definition, type 1 error is rejecting the null hypothesis when its actually true and type 2 error is accepting the null hypothesis when its actually false. In this case to define the error, we need to first define the null and alternate hypothesis.

 

19) What happens to the confidence interval when we introduce some outliers to the data?

A) Confidence interval is robust to outliers

B) Confidence interval will increase with the introduction of outliers.

C) Confidence interval will decrease with the introduction of outliers.

D) We cannot determine the confidence interval in this case.

Solution: (B)

We know that confidence interval depends on the standard deviation of the data. If we introduce outliers into the data, the standard deviation increases, and hence the confidence interval also increases.

 

Context for questions 20- 22

A medical doctor wants to reduce blood sugar level of all his patients by altering their diet. He finds that the mean sugar level of all patients is 180 with a standard deviation of 18. Nine of his patients start dieting and the mean of the sample is observed to 175. Now, he is considering to recommend all his patients to go on a diet.

Note: He calculates 99% confidence interval.

20) What is the standard error of the mean?

A) 9
B) 6
C) 7.5
D) 18

Solution: (B)

The standard error of the mean is the standard deviation by the square root of the number of values. i.e.

Standard error =   = 6

 

21) What is the probability of getting a mean of 175 or less after all the patients start dieting?

A) 20%
B) 25%
C) 15%
D) 12%

Solution: (A)

This actually wants us to calculate the probability of population mean being 175 after the intervention. We can calculate the Z value for the given mean.



If we look at the z table, the corresponding value for z = -0.833 ~ 0.2033.

Therefore there is around 20% probability that if everyone starts dieting, the population mean would be 175.

 

22) Which of the following statement is correct?

A) The doctor has a valid evidence that dieting reduces blood sugar level.

B) The doctor does not have enough evidence that dieting reduces blood sugar level.

C) If the doctor makes all future patients diet in a similar way, the mean blood pressure will fall below 160.

Solution: (B) 

We need to check if we have sufficient evidence to reject the null. The null hypothesis is that dieting has no effect on blood sugar. This is a two tailed test. The z critical value for a 2 tailed test would be ±2.58.

The z value as we have calculated is -0.833.

Since Z value < Z critical value, we do not have enough evidence that dieting reduces blood sugar.

Question Context 23-25

A researcher is trying to examine the effects of two different teaching methods. He divides 20 students into two groups of 10 each. For group 1, the teaching method is using fun examples. Where as for group 2 the teaching method is using software to help students learn. After a 20 minutes lecture of both groups, a test is conducted for all the students.

We want to calculate if there is a significant difference in the scores of both the groups.

It is given that:

Alpha=0.05, two tailed.
Mean test score for group 1 = 10
Mean test score for group 2 = 7
Standard error = 0.94
23) What is the value of t-statistic?

A) 3.191
B) 3.395
C) Cannot be determined.
D) None of the above

Solution: (A)

The t statistic of the given group is nothing but the difference between the group means by the standard error.

=(10-7)/0.94 = 3.191

 

24) Is there a significant difference in the scores of the two groups?

A) Yes
B) No

Solution: (A)

The null hypothesis in this case would be that there is no difference between the groups, while the alternate hypothesis would be that the groups are significantly different.

The t critical value for a 2 tailed test at α = 0.05 is ±2.101. The t statistic obtained is 3.191. Since the t statistic is more than the critical value of t, we can reject the null hypothesis and say that the two groups are significantly different with 95% confidence.

 

25) What percentage of variability in scores is explained by the method of teaching?

A) 36.13
B) 45.21
C) 40.33
D) 32.97

Solution: (A)

The % variability in scores is given by the R2 value. The formula for R2 given by  

R2 =  

The degrees of freedom in this case would be 10+10 -2 since there are two groups with size 10 each. The degree of freedom is 18.

R2 =     = 36.13

 

26) [True or False] F statistic cannot be negative.

A) TRUE

B) FALSE

Solution: (A)

F statistic is the value we receive when we run an ANOVA test on different groups to understand the differences between them. The F statistic is given by the ratio of between group variability to within group variability

Below is the formula for f Statistic.



Since both the numerator and denominator possess square terms, F statistic cannot be negative.

27) Which of the graph below has very strong positive correlation?
A)
B)
C)
D)

Solution: (B)

A strong positive correlation would occur when the following condition is met. If x increases, y should also increase, if x decreases, y should also decrease. The slope of the line would be positive in this case and the data points will show a clear linear relationship. Option B shows a strong positive relationship.

 

28) Correlation between two variables (Var1 and Var2) is 0.65. Now, after adding numeric 2 to all the values of Var1, the correlation co-efficient will_______ ?

A) Increase
B) Decrease
C) None of the above

Solution: (C)

If a constant value is added or subtracted to either variable, the correlation coefficient would be unchanged. It is easy to understand if we look at the formula for calculating the correlation.



If we add a constant value to all the values of x, the xi and  will change by the same number, and the differences will remain the same. Hence, there is no change in the correlation coefficient.

 

29) It is observed that there is a very high correlation between math test scores and amount of physical exercise done by a student on the test day. What can you infer from this?

High correlation implies that after exercise the test scores are high.
Correlation does not imply causation.
Correlation measures the strength of linear relationship between amount of exercise and test scores.
A) Only 1
B) 1 and 3
C) 2 and 3
D) All the statements are true

Solution: (C)

Though sometimes causation might be intuitive from a high correlation but actually correlation does not imply any causal inference. It just tells us the strength of the relationship between the two variables. If both the variables move together, there is a high correlation among them.

 

30) If the correlation coefficient (r) between scores in a math test and amount of physical exercise by a student is 0.86, what percentage of variability in math test is explained by the amount of exercise?

A) 86%
B) 74%
C) 14%
D) 26%

Solution: (B)

The % variability is given by r2, the square of the correlation coefficient. This value represents the fraction of the variation in one variable that may be explained by the other variable. Therefore % variability explained would be 0.862.

31) Which of the following is true about below given histogram?

A) Above histogram is unimodal

B) Above histogram is bimodal

C) Given above is not a histogram

D) None of the above

Solution: (B)

The above histogram is bimodal. As we can see there are two values for which we can see peaks in the histograms indicating high frequencies for those values. Therefore the histogram is bimodal.

 

32) Consider a regression line y=ax+b, where a is the slope and b is the intercept. If we know the value of the slope then by using which option can we always find the value of the intercept?

A) Put the value (0,0) in the regression line True

B) Put any value from the points used to fit the regression line and compute the value of b False

C) Put the mean values of x & y in the equation along with the value a to get b False

D) None of the above can be used False

Solution: (C)

In case of ordinary least squares regression, the line would always pass through the mean values of x and y. If we know one point on the line and the value of slope, we can easily find the intercept.

 

33) What happens when we introduce more variables to a linear regression model?

A) The r squared value may increase or remain constant, the adjusted r squared may increase or decrease.

B) The r squared may increase or decrease while the adjusted r squared always increases.

C) Both r square and adjusted r square always increase on the introduction of new variables in the model.

D) Both might increase or decrease depending on the variables introduced.

Solution: (A)

The R square always increases or at least remains constant because in case of ordinary least squares the sum of square error never increases by adding more variables to the model. Hence the R squared does not decrease. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.

 

34) In a scatter diagram, the vertical distance of a point above or below regression line is known as ____ ?

A) Residual
B) Prediction Error
C) Prediction
D) Both A and B
E) None of the above

Solution: (D)

The lines as we see in the above plot are the vertical distance of points from the regression line. These are known as the residuals or the prediction error.

 

35) In univariate linear least squares regression, relationship between correlation coefficient and coefficient of determination is ______ ?

A) Both are unrelated False

B) The coefficient of determination is the coefficient of correlation squared True

C) The coefficient of determination is the square root of the coefficient of correlation False

D) Both are same F

Solution: (B)

The coefficient of determination is the R squared value and it tells us the amount of variability of the dependent variable explained by the independent variable. This is nothing but correlation coefficient squared. In case of multivariate regression the r squared value represents the ratio of the sum of explained variance to the sum of total variance.

 

36) What is the relationship between significance level and confidence level?

A) Significance level = Confidence level
B) Significance level = 1- Confidence level
C) Significance level = 1/Confidence level
D) Significance level = sqrt (1 – Confidence level)

Solution: (B)

Significance level is 1-confidence interval. If the significance level is 0.05, the corresponding confidence interval is 95% or 0.95. The significance level is the probability of obtaining a result as extreme as, or more extreme than, the result actually obtained when the null hypothesis is true. The confidence interval is the range of likely values for a population parameter, such as the population mean. For example, if you compute a 95% confidence interval for the average price of an ice cream, then you can be 95% confident that the interval contains the true average cost of all ice creams.

The significance level and confidence level are the complementary portions in the normal distribution.

 

37) [True or False] Suppose you have been given a variable V, along with its mean and median. Based on these values, you can find whether the variable “V” is left skewed or right skewed for the condition

mean(V) > median(V)
A) True
B) False

Solution: (B)

Since, its no where mentioned about the type distribution of the variable V, we cannot say whether it is left skewed or right skewed for sure.

 

38) The line described by the linear regression equation (OLS) attempts to ____ ?

A) Pass through as many points as possible.

B)  Pass through as few points as possible

C) Minimize the number of points it touches

D) Minimize the squared distance from the points

Solution: (D)

The regression line attempts to minimize the squared distance between the points and the regression line. By definition the ordinary least squares regression tries to have the minimum sum of squared errors. This means that the sum of squared residuals should be minimized. This may or may not be achieved by passing through the maximum points in the data. The most common case of not passing through all points and reducing the error is when the data has a lot of outliers or is not very strongly linear.

 

39) We have a linear regression equation ( Y = 5X +40) for the below table. 

X	Y
5	45
6	76
7	78
8	87
9	79
Which of the following is a MAE (Mean Absolute Error) for this linear model?

A) 8.4
B) 10.29
C) 42.5
D) None of the above

Solution: (A)

To calculate the mean absolute error for this case, we should first calculate the values of y with the given equation and then calculate the absolute error with respect to the actual values of y. Then the average value of this absolute error would be the mean absolute error. The below table summarises these values.



 

40) A regression analysis between weight (y) and height (x) resulted in the following least squares line: y = 120 + 5x. This implies that if the height is increased by 1 inch, the weight is expected to

A) increase by 1 pound
B) increase by 5 pound
C) increase by 125 pound
D) None of the above

Solution:  (B)

Looking at the equation given y=120+5x. If the height is increased by 1 unit, the weight will increase by 5 pounds. Since 120 will be the same in both cases and will go off in the difference.

 

41) [True or False] Pearson captures how linearly dependent two variables are whereas Spearman captures the monotonic behaviour of the relation between the variables.

A)TRUE

B) FALSE

Solution: (A)

The statement is true. Pearson correlation evaluated the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable.

The spearman evaluates a monotonic relationship. A monotonic relationship is one where the variables change together but not necessarily at a constant rate.

1) True-False: Linear Regression is a supervised machine learning algorithm.

A) TRUE
B) FALSE

Solution: (A)

Yes, Linear regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variable (x) and an output variable (Y) for each example.

 

2) True-False: Linear Regression is mainly used for Regression.

A) TRUE
B) FALSE

Solution: (A)

Linear Regression has dependent variables that have continuous values.

 

3) True-False: It is possible to design a Linear regression algorithm using a neural network?

A) TRUE
B) FALSE

Solution: (A)

True. A Neural network can be used as a universal approximator, so it can definitely implement a linear regression algorithm.

 

4) Which of the following methods do we use to find the best fit line for data in Linear Regression?

A) Least Square Error
B) Maximum Likelihood
C) Logarithmic Loss
D) Both A and B

Solution: (A)

In linear regression, we try to minimize the least square errors of the model to identify the line of best fit.

5) Which of the following evaluation metrics can be used to evaluate a model while modeling a continuous output variable?

A) AUC-ROC
B) Accuracy
C) Logloss
D) Mean-Squared-Error

Solution: (D)

Since linear regression gives output as continuous values, so in such case we use mean squared error metric to evaluate the model performance. Remaining options are use in case of a classification problem.

 

6) True-False: Lasso Regularization can be used for variable selection in Linear Regression.

A) TRUE
B) FALSE

Solution: (A)

True, In case of lasso regression we apply absolute penalty which makes some of the coefficients zero.

7) Which of the following is true about Residuals ?

A) Lower is better
B) Higher is better
C) A or B depend on the situation
D) None of these

Solution: (A)

Residuals refer to the error values of the model. Therefore lower residuals are desired.

8) Suppose that we have N independent variables (X1,X2… Xn) and dependent variable is Y. Now Imagine that you are applying linear regression by fitting the best fit line using least square error on this data.

You found that correlation coefficient for one of it’s variable(Say X1) with Y is -0.95.

Which of the following is true for X1?

A) Relation between the X1 and Y is weak
B) Relation between the X1 and Y is strong
C) Relation between the X1 and Y is neutral
D) Correlation can’t judge the relationship

Solution: (B)

The absolute value of the correlation coefficient denotes the strength of the relationship. Since  absolute correlation is very high it means that the relationship is strong between X1 and Y.

 

9) Looking at above two characteristics, which of the following option is the correct for Pearson correlation between V1 and V2?

If you are given the two variables V1 and V2 and they are following below two characteristics.

1. If V1 increases then V2 also increases

2. If V1 decreases then V2 behavior is unknown

A) Pearson correlation will be close to 1
B) Pearson correlation will be close to -1
C) Pearson correlation will be close to 0
D) None of these

Solution: (D)

We cannot comment on the correlation coefficient by using only statement 1.  We need to consider the both of these two statements. Consider V1 as x and V2 as |x|. The correlation coefficient would not be close to 1 in such a case.

10) Suppose Pearson correlation between V1 and V2 is zero. In such case, is it right to conclude that V1 and V2 do not have any relation between them?

A) TRUE
B) FALSE

Solution: (B)

Pearson correlation coefficient between 2 variables might be zero even when they have a relationship between them. If the correlation coefficient is zero, it just means that that they don’t move together. We can take examples like y=|x| or y=x^2.

11) Which of the following offsets, do we use in linear regression’s least square line fit? Suppose horizontal axis is independent variable and vertical axis is dependent variable.

 



A) Vertical offset
B) Perpendicular offset
C) Both, depending on the situation
D) None of above

Solution: (A)

We always consider residuals as vertical offsets. We calculate the direct differences between actual value and the Y labels. Perpendicular offset are useful in case of PCA.

12) True- False: Overfitting is more likely when you have huge amount of data to train?

A) TRUE
B) FALSE

Solution: (B)

With a small training dataset, it’s easier to find a hypothesis to fit the training data exactly i.e. overfitting.

 

13) We can also compute the coefficient of linear regression with the help of an analytical method called “Normal Equation”. Which of the following is/are true about Normal Equation?

We don’t have to choose the learning rate
It becomes slow when number of features is very large
Thers is no need to iterate
 

A) 1 and 2
B) 1 and 3
C) 2 and 3
D) 1,2 and 3

Solution: (D)

Instead of gradient descent, Normal Equation can also be used to find coefficients. Refer this article for read more about normal equation.

 

14) Which of the following statement is true about sum of residuals of A and B?

Below graphs show two fitted regression lines (A & B) on randomly generated data. Now, I want to find the sum of residuals in both cases A and B.

Note:

Scale is same in both graphs for both axis.
X axis is independent variable and Y-axis is dependent variable.


A) A has higher sum of residuals than B
B) A has lower sum of residual than B
C) Both have same sum of residuals
D) None of these

Solution: (C)

Sum of residuals will always be zero, therefore both have same sum of residuals

 

Question Context 15-17:
Suppose you have fitted a complex regression model on a dataset. Now, you are using Ridge regression with penality x.

15) Choose the option which describes bias in best manner.
A) In case of very large x; bias is low
B) In case of very large x; bias is high
C) We can’t say about bias
D) None of these

Solution: (B)

If the penalty is very large it means model is less complex, therefore the bias would be high.

 

16) What will happen when you apply very large penalty?

A) Some of the coefficient will become absolute zero
B) Some of the coefficient will approach zero but not absolute zero
C) Both A and B depending on the situation
D) None of these

Solution: (B)

In lasso some of the coefficient value become zero, but in case of Ridge, the coefficients become close to zero but not zero.

 

17) What will happen when you apply very large penalty in case of Lasso?
A) Some of the coefficient will become zero
B) Some of the coefficient will be approaching to zero but not absolute zero
C) Both A and B depending on the situation
D) None of these

Solution: (A)

As already discussed, lasso applies absolute penalty, so some of the coefficients will become zero.

 

18) Which of the following statement is true about outliers in Linear regression?

A) Linear regression is sensitive to outliers
B) Linear regression is not sensitive to outliers
C) Can’t say
D) None of these

Solution: (A)

The slope of the regression line will change due to outliers in most of the cases. So Linear Regression is sensitive to outliers.

 

19) Suppose you plotted a scatter plot between the residuals and predicted values in linear regression and you found that there is a relationship between them. Which of the following conclusion do you make about this situation?

 

A) Since the there is a relationship means our model is not good
B) Since the there is a relationship means our model is good
C) Can’t say
D) None of these

Solution: (A)

There should not be any relationship between predicted values and residuals. If there exists any relationship between them,it means that the model has not perfectly captured the information in the data.

 

Question Context 20-22:
Suppose that you have a dataset D1 and you design a linear regression model of degree 3 polynomial and you found that the training and testing error is “0” or in another terms it perfectly fits the data.

20) What will happen when you fit degree 4 polynomial in linear regression?
A) There are high chances that degree 4 polynomial will over fit the data
B) There are high chances that degree 4 polynomial will under fit the data
C) Can’t say
D) None of these

Solution: (A)

Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero.

 

21) What will happen when you fit degree 2 polynomial in linear regression?
A) It is high chances that degree 2 polynomial will over fit the data
B) It is high chances that degree 2 polynomial will under fit the data
C) Can’t say
D) None of these

Solution: (B)

If a degree 3 polynomial fits the data perfectly, it’s highly likely that a simpler model(degree 2 polynomial) might under fit the data.

 

22) In terms of bias and variance. Which of the following is true when you fit degree 2 polynomial?


A) Bias will be high, variance will be high
B) Bias will be low, variance will be high
C) Bias will be high, variance will be low
D) Bias will be low, variance will be low

Solution: (C)

Since a degree 2 polynomial will be less complex as compared to degree 3, the bias will be high and variance will be low.

 

Question Context 23:

Which of the following is true about below graphs(A,B, C left to right) between the cost function and Number of iterations?



23) Suppose l1, l2 and l3 are the three learning rates for A,B,C respectively. Which of the following is true about l1,l2 and l3?

 

A) l2 < l1 < l3

B) l1 > l2 > l3
C) l1 = l2 = l3
D) None of these

Solution: (A)

In case of high learning rate, step will be high, the objective function will decrease quickly initially, but it will not find the global minima and objective function starts increasing after a few iterations.

In case of low learning rate, the step will be small. So the objective function will decrease slowly

 

Question Context 24-25:

We have been given a dataset with n records in which we have input attribute as x and output attribute as y. Suppose we use a linear regression method to model this data. To test our linear regressor, we split the data in training set and test set randomly.

24) Now we increase the training set size gradually. As the training set size increases, what do you expect will happen with the mean training error?

 

A) Increase
B) Decrease
C) Remain constant
D) Can’t Say

Solution: (D)

Training error may increase or decrease depending on the values that are used to fit the model. If the values used to train contain more outliers gradually, then the error might just increase.

 

25) What do you expect will happen with bias and variance as you increase the size of training data?

 

A) Bias increases and Variance increases
B) Bias decreases and Variance increases
C) Bias decreases and Variance decreases
D) Bias increases and Variance decreases
E) Can’t Say False

Solution: (D)

As we increase the size of the training data, the bias would increase while the variance would decrease.

 

Question Context 26:

Consider the following data where one input(X) and one output(Y) is given.



26) What would be the root mean square training error for this data if you run a Linear Regression model of the form (Y = A0+A1X)?

 

A) Less than 0
B) Greater than zero
C) Equal to 0
D) None of these

Solution: (C)

We can perfectly fit the line on the following data so mean error will be zero.

 

Question Context 27-28:

Suppose you have been given the following scenario for training and validation error for Linear Regression.

Scenario	Learning Rate	Number of iterations	Training Error	Validation Error
1	0.1	1000	100	110
2	0.2	600	90	105
3	0.3	400	110	110
4	0.4	300	120	130
5	0.4	250	130	150
 

27) Which of the following scenario would give you the right hyper parameter?

A) 1
B) 2
C) 3
D) 4

Solution: (B)

Option B would be the better option because it leads to less training as well as validation error.

28) Suppose you got the tuned hyper parameters from the previous question. Now, Imagine you want to add a variable in variable space such that this added feature is important. Which of the following thing would you observe in such case?

A) Training Error will decrease and Validation error will increase

B) Training Error will increase and Validation error will increase
C) Training Error will increase and Validation error will decrease
D) Training Error will decrease and Validation error will decrease
E) None of the above

Solution: (D)

If the added feature is important, the training and validation error would decrease.

Question Context 29-30:

Suppose, you got a situation where you find that your linear regression model is under fitting the data.

29) In such situation which of the following options would you consider?

Add more variables
Start introducing polynomial degree variables
Remove some variables
A) 1 and 2
B) 2 and 3
C) 1 and 3
D) 1, 2 and 3

Solution: (A)

In case of under fitting, you need to induce more variables in variable space or you can add some polynomial degree variables to make the model more complex to be able to fir the data better.

 

30) Now situation is same as written in previous question(under fitting).Which of following regularization algorithm would you prefer?

 

A) L1
B) L2
C) Any
D) None of these

Solution: (D)

I won’t use any regularization methods because regularization is used in case of overfitting.

1) True-False: Is Logistic regression a supervised machine learning algorithm?

A) TRUE
B) FALSE

Solution: A

True, Logistic regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variables (x) and an target variable (Y) when you train the model .

 

2) True-False: Is Logistic regression mainly used for Regression?

A) TRUE
B) FALSE

Solution: B

Logistic regression is a classification algorithm, don’t confuse with the name regression.

 

3) True-False: Is it possible to design a logistic regression algorithm using a Neural Network Algorithm?

A) TRUE
B) FALSE

Solution: A

True, Neural network is a is a universal approximator so it can implement linear regression algorithm.

 

4) True-False: Is it possible to apply a logistic regression algorithm on a 3-class Classification problem?

A) TRUE
B) FALSE

Solution: A

Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression.

 

5) Which of the following methods do we use to best fit the data in Logistic Regression?

A) Least Square Error
B) Maximum Likelihood
C) Jaccard distance
D) Both A and B

Solution: B

Logistic regression uses maximum likely hood estimate for training a logistic regression.

 

6) Which of the following evaluation metrics can not be applied in case of logistic regression output to compare with target?

A) AUC-ROC
B) Accuracy
C) Logloss
D) Mean-Squared-Error

Solution: D

Since, Logistic Regression is a classification algorithm so it’s output can not be real time value so mean squared error can not use for evaluating it

 

7) One of the very good methods to analyze the performance of Logistic Regression is AIC, which is similar to R-Squared in Linear Regression. Which of the following is true about AIC?

A) We prefer a model with minimum AIC value
B) We prefer a model with maximum AIC value
C) Both but depend on the situation
D) None of these

Solution: A

We select the best model in logistic regression which can least AIC. For more information refer this source: http://www4.ncsu.edu/~shu3/Presentation/AIC.pdf

 

8) [True-False] Standardisation of features is required before training a Logistic Regression.

A) TRUE
B) FALSE

Solution: B

Standardization isn’t required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.

 

9) Which of the following algorithms do we use for Variable Selection?

A) LASSO
B) Ridge
C) Both
D) None of these

Solution: A

In case of lasso we apply a absolute penality, after increasing the penality in lasso some of the coefficient of variables may become zero.

 

Context: 10-11

Consider a following model for logistic regression: P (y =1|x, w)= g(w0 + w1x)
where g(z) is the logistic function.

In the above equation the P (y =1|x; w) , viewed as a function of x, that we can get by changing the parameters w.

10) What would be the range of p in such case?


A) (0, inf)
B) (-inf, 0 )
C) (0, 1)
D) (-inf, inf)

Solution: C

For values of x in the range of  real number from −∞ to +∞ Logistic function will give the output between (0,1)

 

11) In above question what do you think which function would make p between (0,1)?

A) logistic function
B) Log likelihood function
C) Mixture of both
D) None of them

Solution: A

Explanation is same as question number 10

 

Context: 12-13

Suppose you train a logistic regression classifier and your hypothesis function H is

 



 

12) Which of the following figure will represent the decision boundary as given by above classifier?

A)



B)



C)



D)



 

Solution: B

Option B would be the right answer. Since our line will be represented by y = g(-6+x2) which is shown in the option A and option B. But option B is the right answer because when you put the value x2 = 6 in the equation then y = g(0) you will get that means y= 0.5 will be on the line, if you increase the value of x2 greater then 6 you will get negative values so output will be the region y =0.

 

13) If you replace coefficient of x1 with x2 what would be the output figure?

A)


B)


C)



D)



Solution: D

Same explanation as in previous question.

 

14) Suppose you have been given a fair coin and you want to find out the odds of getting heads. Which of the following option is true for such a case?

A) odds will be 0
B) odds will be 0.5
C) odds will be 1
D) None of these

Solution: C

Odds are defined as the ratio of the probability of success and the probability of failure. So in case of fair coin probability of success is 1/2 and the probability of failure is 1/2 so odd would be 1

 

15) The logit function(given as l(x)) is the log of odds function. What could be the range of logit function in the domain x=[0,1]?

A) (– ∞ , ∞)
B) (0,1)
C) (0, ∞)
D) (- ∞, 0)

Solution: A

For our purposes, the odds function has the advantage of transforming the probability function, which has values from 0 to 1, into an equivalent function with values between 0 and ∞. When we take the natural log of the odds function, we get a range of values from -∞ to ∞.

 

16) Which of the following option is true?

A) Linear Regression errors values has to be normally distributed but in case of Logistic Regression it is not the case
B) Logistic Regression errors values has to be normally distributed but in case of Linear Regression it is not the case
C) Both Linear Regression and Logistic Regression error values have to be normally distributed
D) Both Linear Regression and Logistic Regression error values have not to be normally distributed

Solution:A

Only A is true. Refer this tutorial https://czep.net/stat/mlelr.pdf

 

17) Which of the following is true regarding the logistic function for any value “x”?

Note:
Logistic(x): is a logistic function of any number “x”

Logit(x): is a logit function of any number “x”

Logit_inv(x): is a inverse logit function of any number “x”

A) Logistic(x) = Logit(x)
B) Logistic(x) = Logit_inv(x)
C) Logit_inv(x) = Logit(x)
D) None of these

Solution: B

Refer this link for the solution: https://en.wikipedia.org/wiki/Logit

 

18) How will the bias change on using high(infinite) regularisation?

Suppose you have given the two scatter plot “a” and “b” for two classes( blue for positive and red for negative class). In scatter plot “a”, you correctly classified all data points using logistic regression ( black line is a decision boundary).

 



A) Bias will be high
B) Bias will be low
C) Can’t say
D) None of these

Solution: A

Model will become very simple so bias will be very high.

 

19) Suppose, You applied a Logistic Regression model on a given data and got a training accuracy X and testing accuracy Y. Now, you want to add a few new features in the same data. Select the option(s) which is/are correct in such a case.

Note: Consider remaining parameters are same.

A) Training accuracy increases
B) Training accuracy increases or remains the same
C) Testing accuracy decreases
D) Testing accuracy increases or remains the same

Solution: A and D

Adding more features to model will increase the training accuracy because model has to consider more data to fit the logistic regression. But testing accuracy increases if feature is found to be significant

 

20) Choose which of the following options is true regarding One-Vs-All method in Logistic Regression.

A) We need to fit n models in n-class classification problem
B) We need to fit n-1 models to classify into n classes
C) We need to fit only 1 model to classify into n classes
D) None of these

Solution: A

If there are n classes, then n separate logistic regression has to fit, where the probability of each category is predicted over the rest of the categories combined.

 

21) Below are two different logistic models with different values for β0 and β1.

Which of the following statement(s) is true about β0 and β1 values of two logistics models (Green, Black)?

Note: consider Y = β0 + β1*X. Here, β0 is intercept and β1 is coefficient.

A) β1 for Green is greater than Black
B) β1 for Green is lower than Black
C) β1 for both models is same
D) Can’t Say

Solution: B

β0 and β1: β0 = 0, β1 = 1 is in X1 color(black) and β0 = 0, β1 = −1 is in X4 color (green)

 

Context 22-24

Below are the three scatter plot(A,B,C left to right) and hand drawn decision boundaries for logistic regression.



22) Which of the following above figure shows that the decision boundary is overfitting the training data?

A) A
B) B
C) C
D)None of these

Solution: C

Since in figure 3, Decision boundary is not smooth that means it will over-fitting the data.

 

23) What do you conclude after seeing this visualization?

The training error in first plot is maximum as compare to second and third plot.
The best model for this regression problem is the last (third) plot because it has minimum training error (zero).
The second model is more robust than first and third because it will perform best on unseen data.
The third model is overfitting more as compare to first and second.
All will perform same because we have not seen the testing data.
A) 1 and 3
B) 1 and 3
C) 1, 3 and 4
D) 5

Solution: C

The trend in the graphs looks like a quadratic trend over independent variable X. A higher degree(Right graph) polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset. But if you see in left graph we will have training error maximum because it underfits the training data

 

24) Suppose, above decision boundaries were generated for the different value of regularization. Which of the above decision boundary shows the maximum regularization?

A) A
B) B
C) C
D) All have equal regularization

Solution: A

Since, more regularization means more penality means less complex decision boundry that shows in first figure A.

 

25) The below figure shows AUC-ROC curves for three logistic regression models. Different colors show curves for different hyper parameters values. Which of the following AUC-ROC will give best result?


A) Yellow
B) Pink
C) Black
D) All are same

Solution: A

The best classification is the largest area under the curve so yellow line has largest area under the curve.

 

26) What would do if you want to train logistic regression on same data that will take less time as well as give the comparatively similar accuracy(may not be same)?

Suppose you are using a Logistic Regression model on a huge dataset. One of the problem you may face on such huge data is that Logistic regression will take very long time to train.

A) Decrease the learning rate and decrease the number of iteration
B) Decrease the learning rate and increase the number of iteration
C) Increase the learning rate and increase the number of iteration
D) Increase the learning rate and decrease the number of iteration

Solution: D

If you decrease the number of iteration while training it will take less time for surly but will not give the same accuracy for getting the similar accuracy but not exact you need to increase the learning rate.

 

27) Which of the following image is showing the cost function for y =1.

Following is the loss function in logistic regression(Y-axis loss function and x axis log probability) for two class classification problem.

Note: Y is the target class



A) A
B) B
C) Both
D) None of these

Solution: A

A is the true answer as loss function decreases as the log probability increases

 

28) Suppose, Following graph is a cost function for logistic regression.


Now, How many local minimas are present in the graph?

A) 1
B) 2
C) 3
D) 4

Solution: C

There are three local minima present in the graph

 

29) Imagine, you have given the below graph of logistic regression  which is shows the relationships between cost function and number of iteration for 3 different learning rate values (different colors are showing different curves at different learning rates ). 

 

Suppose, you save the graph for future reference but you forgot to save the value of different learning rates for this graph. Now, you want to find out the relation between the leaning rate values of these curve. Which of the following will be the true relation?

Note:

The learning rate for blue is l1
The learning rate for red is l2
The learning rate for green is l3
A) l1>l2>l3
B) l1 = l2 = l3
C) l1 < l2 < l3

D) None of these

Solution: C

If you have low learning rate means your cost function will decrease slowly but in case of large learning rate cost function will decrease very fast.

 

30) Can a Logistic Regression classifier do a perfect classification on the below data?

 



 

Note: You can use only X1 and X2 variables where X1 and X2 can take only two binary values(0,1).

A) TRUE
B) FALSE
C) Can’t say
D) None of these

Solution: B

No, logistic regression only forms linear decision surface, but the examples in the figure are not linearly separable.

https://www.cs.cmu.edu/~tom/10701_sp11/midterm_sol.pdf

A feature F1 can take certain value: A, B, C, D, E, & F and represents grade of students from a college.

1) Which of the following statement is true in following case?

A) Feature F1 is an example of nominal variable.
B) Feature F1 is an example of ordinal variable.
C) It doesn’t belong to any of the above category.
D) Both of these

Solution: (B)

Ordinal variables are the variables which has some order in their categories. For example, grade A should be consider as high grade than grade B.

 

2) Which of the following is an example of a deterministic algorithm?

A) PCA

B) K-Means

C) None of the above

Solution: (A)A deterministic algorithm is that in which output does not change on different runs. PCA would give the same result if we run again, but not k-means.

 

3) [True or False] A Pearson correlation between two variables is zero but, still their values can still be related to each other.

A) TRUE

B) FALSE

Solution: (A)

Y=X2. Note that, they are not only associated, but one is a function of the other and Pearson correlation between them is 0.

 

4) Which of the following statement(s) is / are true for Gradient Decent (GD) and Stochastic Gradient Decent (SGD)?

In GD and SGD, you update a set of parameters in an iterative manner to minimize the error function. 
In SGD, you have to run through all the samples in your training set for a single update of a parameter in each iteration. 
In GD, you either use the entire data or a subset of training data to update a parameter in each iteration. 
A) Only 1

B) Only 2

C) Only 3

D) 1 and 2

E) 2 and 3

F) 1,2 and 3

Solution: (A)In SGD for each iteration you choose the batch which is generally contain the random sample of data But in case of GD each iteration contain the all of the training observations.

 

5) Which of the following hyper parameter(s), when increased may cause random forest to over fit the data? 

Number of Trees
Depth of Tree
Learning Rate
A) Only 1

B) Only 2

C) Only 3

D) 1 and 2

E) 2 and 3

F) 1,2 and 3

Solution: (B)Usually, if we increase the depth of tree it will cause overfitting. Learning rate is not an hyperparameter in random forest. Increase in the number of tree will cause under fitting.

 

6) Imagine, you are working with “Analytics Vidhya” and you want to develop a machine learning algorithm which predicts the number of views on the articles. 

Your analysis is based on features like author name, number of articles written by the same author on Analytics Vidhya in past and a few other features. Which of the following evaluation metric would you choose in that case?

Mean Square Error
Accuracy
F1 Score
A) Only 1

B) Only 2

C) Only 3

D) 1 and 3

E) 2 and 3

F) 1 and 2

Solution:(A)

You can think that the number of views of articles is the continuous target variable which fall under the regression problem. So, mean squared error will be used as an evaluation metrics.

 

7) Given below are three images (1,2,3). Which of the following option is correct for these images?

A)

B)

C) 
A) 1 is tanh, 2 is ReLU and 3 is SIGMOID activation functions.

B) 1 is SIGMOID, 2 is ReLU and 3 is tanh activation functions.

C) 1 is ReLU, 2 is tanh and 3 is SIGMOID activation functions.

D) 1 is tanh, 2 is SIGMOID and 3 is ReLU activation functions.

Solution: (D)

The range of SIGMOID function is [0,1].

The range of the tanh function is [-1,1].

The range of the RELU function is [0, infinity].

So Option D is the right answer.

 

8) Below are the 8 actual values of target variable in the train file.

[0,0,0,1,1,1,1,1]

What is the entropy of the target variable? 

A) -(5/8 log(5/8) + 3/8 log(3/8))

B) 5/8 log(5/8) + 3/8 log(3/8)

C) 3/8 log(5/8) + 5/8 log(3/8)

D) 5/8 log(3/8) – 3/8 log(5/8)

Solution: (A)The formula for entropy is  

So the answer is A.

 

9) Let’s say, you are working with categorical feature(s) and you have not looked at the distribution of the categorical variable in the test data.

You want to apply one hot encoding (OHE) on the categorical feature(s). What challenges you may face if you have applied OHE on a categorical variable of train dataset? 

A) All categories of categorical variable are not present in the test dataset.

B) Frequency distribution of categories is different in train as compared to the test dataset.

C) Train and Test always have same distribution.

D) Both A and B

E) None of these

Solution: (D)Both are true, The OHE will fail to encode the categories which is present in test but not in train so it could be one of the main challenges while applying OHE. The challenge given in option B is also true you need to more careful while applying OHE if frequency distribution doesn’t same in train and test.

 

10) Skip gram model is one of the best models used in Word2vec algorithm for words embedding. Which one of the following models depict the skip gram model?



A) A

B) B

C) Both A and B

D) None of these

Solution: (B)

Both models (model1 and model2) are used in Word2vec algorithm. The model1 represent a CBOW model where as Model2 represent the Skip gram model.

 

11) Let’s say, you are using activation function X in hidden layers of neural network. At a particular neuron for any given input, you get the output as “-0.0001”. Which of the following activation function could X represent? 

A) ReLU

B) tanh

C) SIGMOID

D) None of these

Solution: (B)The function is a tanh because the this function output range is between (-1,-1).

 

12) [True or False] LogLoss evaluation metric can have negative values.

A) TRUE
B) FALSE

Solution: (B)Log loss cannot have negative values.

 

13) Which of the following statements is/are true about “Type-1” and “Type-2” errors?

Type1 is known as false positive and Type2 is known as false negative.
Type1 is known as false negative and Type2 is known as false positive.
Type1 error occurs when we reject a null hypothesis when it is actually true.
A) Only 1

B) Only 2

C) Only 3

D) 1 and 2

E) 1 and 3

F) 2 and 3

Solution: (E)

In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a “false positive”), while a type II error is incorrectly retaining a false null hypothesis (a “false negative”).

 

14) Which of the following is/are one of the important step(s) to pre-process the text in NLP based projects?

Stemming
Stop word removal
Object Standardization
A) 1 and 2

B) 1 and 3

C) 2 and 3

D) 1,2 and 3

Solution: (D)

Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.

Stop words are those words which will have not relevant to the context of the data for example is/am/are.

Object Standardization is also one of the good way to pre-process the text.

 

15) Suppose you want to project high dimensional data into lower dimensions. The two most famous dimensionality reduction algorithms used here are PCA and t-SNE. Let’s say you have applied both algorithms respectively on data “X” and you got the datasets “X_projected_PCA” , “X_projected_tSNE”.

Which of the following statements is true for “X_projected_PCA” & “X_projected_tSNE” ?

A) X_projected_PCA will have interpretation in the nearest neighbour space.

B) X_projected_tSNE will have interpretation in the nearest neighbour space.

C) Both will have interpretation in the nearest neighbour space.

D) None of them will have interpretation in the nearest neighbour space.

Solution: (B)

t-SNE algorithm considers nearest neighbour points to reduce the dimensionality of the data. So, after using t-SNE we can think that reduced dimensions will also have interpretation in nearest neighbour space. But in the case of PCA it is not the case.

Context: 16-17
Given below are three scatter plots for two features (Image 1, 2 & 3 from left to right).



16) In the above images, which of the following is/are examples of multi-collinear features?

A) Features in Image 1

B) Features in Image 2

C) Features in Image 3

D) Features in Image 1 & 2

E) Features in Image 2 & 3

F) Features in Image 3 & 1

Solution: (D)

In Image 1, features have high positive correlation where as in Image 2 has high negative correlation between the features so in both images pair of features are the example of multicollinear features.

 

17) In previous question, suppose you have identified multi-collinear features. Which of the following action(s) would you perform next? 

Remove both collinear variables.
Instead of removing both variables, we can remove only one variable.
Removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression.
A) Only 1

B)Only 2

C) Only 3

D) Either 1 or 3

E) Either 2 or 3

Solution: (E)

You cannot remove the both features because after removing the both features  you will lose all of the information so you should either remove the only 1 feature or you can use the regularization algorithm like L1 and L2.

 

18) Adding a non-important feature to a linear regression model may result in.

Increase in R-square
Decrease in R-square
A) Only 1 is correct

B) Only 2 is correct

C) Either 1 or 2

D) None of these

Solution: (A)

After adding a feature in feature space, whether that feature is important or unimportant features the R-squared always increase.

 

19) Suppose, you are given three variables X, Y and Z. The Pearson correlation coefficients for (X, Y), (Y, Z) and (X, Z) are C1, C2 & C3 respectively.

Now, you have added 2 in all values of X (i.enew values become X+2), subtracted 2 from all values of Y (i.e. new values are Y-2) and Z remains the same. The new coefficients for (X,Y), (Y,Z) and (X,Z) are given by D1, D2 & D3 respectively. How do the values of D1, D2 & D3 relate to C1, C2 & C3? 

A) D1= C1, D2 < C2, D3 > C3

B) D1 = C1, D2 > C2, D3 > C3

C) D1 = C1, D2 > C2, D3 < C3

D) D1 = C1, D2 < C2, D3 < C3

E) D1 = C1, D2 = C2, D3 = C3

F) Cannot be determined

Solution: (E)Correlation between the features won’t change if you add or subtract a value in the features.

 

20) Imagine, you are solving a classification problems with highly imbalanced class. The majority class is observed 99% of times in the training data.

Your model has 99% accuracy after taking the predictions on test data. Which of the following is true in such a case?

Accuracy metric is not a good idea for imbalanced class problems.
Accuracy metric is a good idea for imbalanced class problems.
Precision and recall metrics are good for imbalanced class problems.
Precision and recall metrics aren’t good for imbalanced class problems.
A) 1 and 3

B) 1 and 4

C) 2 and 3

D) 2 and 4

Solution: (A)Refer the question number 4 from in this article.

 

21) In ensemble learning, you aggregate the predictions for weak learners, so that an ensemble of these models will give a better prediction than prediction of individual models.

Which of the following statements is / are true for weak learners used in ensemble model?

They don’t usually overfit.
They have high bias, so they cannot solve complex learning problems
They usually overfit.
A) 1 and 2

B) 1 and 3

C) 2 and 3

D) Only 1

E) Only 2

F) None of the above

Solution: (A)

Weak learners are sure about particular part of a problem. So, they usually don’t overfit which means that weak learners have low variance and high bias.

 

22) Which of the following options is/are true for K-fold cross-validation?

Increase in K will result in higher time required to cross validate the result.
Higher values of K will result in higher confidence on the cross-validation result as compared to lower value of K.
If K=N, then it is called Leave one out cross validation, where N is the number of observations.
 

A) 1 and 2

B) 2 and 3

C) 1 and 3

D) 1,2 and 3

Solution: (D)

Larger k value means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) and higher running time (as you are getting closer to the limit case: Leave-One-Out CV). We also need to consider the variance between the k folds accuracy while selecting the k.

 

Question Context 23-24
Cross-validation is an important step in machine learning for hyper parameter tuning. Let’s say you are tuning a hyper-parameter “max_depth” for GBM by selecting it from 10 different depth values (values are greater than 2) for tree based model using 5-fold cross validation.

Time taken by an algorithm for training (on a model with max_depth 2) 4-fold is 10 seconds and for the prediction on remaining 1-fold is 2 seconds.

Note: Ignore hardware dependencies from the equation.

23) Which of the following option is true for overall execution time for 5-fold cross validation with 10 different values of “max_depth”? 

A) Less than 100 seconds

B) 100 – 300 seconds

C) 300 – 600 seconds

D) More than or equal to 600 seconds

C) None of the above

D) Can’t estimate

Solution: (D)

Each iteration for depth “2” in 5-fold cross validation will take 10 secs for training and 2 second for testing. So, 5 folds will take 12*5 = 60 seconds. Since we are searching over the 10 depth values so the algorithm would take 60*10 = 600 seconds. But training and testing a model on depth greater than 2 will take more time than depth “2” so overall timing would be greater than 600.

 

24) In previous question, if you train the same algorithm for tuning 2 hyper parameters say “max_depth” and “learning_rate”.

You want to select the right value against “max_depth” (from given 10 depth values) and learning rate (from given 5 different learning rates). In such cases, which of the following will represent the overall time?

A) 1000-1500 second

B) 1500-3000 Second

C) More than or equal to 3000 Second

D) None of these

Solution: (D)Same as question number 23.

 

25) Given below is a scenario for training error TE and Validation error VE for a machine learning algorithm M1. You want to choose a hyperparameter (H) based on TE and VE.

H	TE	VE
1	105	90
2	200	85
3	250	96
4	105	85
5	300	100
 Which value of H will you choose based on the above table?

A) 1

B) 2

C) 3

D) 4

E) 5

Solution: (D)Looking at the table, option D seems the best

 

26) What would you do in PCA to get the same projection as SVD?

A) Transform data to zero mean

B) Transform data to zero median

C) Not possible

D) None of these

Solution: (A)When the data has a zero mean vector PCA will have same projections as SVD, otherwise you have to centre the data first before taking SVD.

 

Question Context 27-28
Assume there is a black box algorithm, which takes training data with multiple observations (t1, t2, t3,…….. tn) and a new observation (q1). The black box outputs the nearest neighbor of q1 (say ti) and its corresponding class label ci. 

You can also think that this black box algorithm is same as 1-NN (1-nearest neighbor).

27) It is possible to construct a k-NN classification algorithm based on this black box alone.

Note: Where n (number of training observations) is very large compared to k.

A) TRUE

B) FALSE

Solution: (A)

In first step, you pass an observation (q1) in the black box algorithm so this algorithm would return a nearest observation and its class.

In second step, you through it out nearest observation from train data and again input the observation (q1). The black box algorithm will again return the a nearest observation and it’s class.

You need to repeat this procedure k times

 

28) Instead of using 1-NN black box we want to use the j-NN (j>1) algorithm as black box. Which of the following option is correct for finding k-NN using j-NN?

J must be a proper factor of k
J > k
Not possible
A)  1

B) 2

C) 3

Solution: (A)Same as question number 27

 

29) Suppose you are given 7 Scatter plots 1-7 (left to right) and you want to compare Pearson correlation coefficients between variables of each scatterplot.

Which of the following is in the right order?



1<2<3<4
1>2>3 > 4
7<6<5<4
7>6>5>4
A) 1 and 3

B) 2 and 3

C) 1 and 4

D) 2 and 4

Solution: (B)

from image 1to 4 correlation is decreasing (absolute value). But from image 4 to 7 correlation is increasing but values are negative (for example, 0, -0.3, -0.7, -0.99).

30) You can evaluate the performance of a binary class classification problem using different metrics such as accuracy, log-loss, F-Score. Let’s say, you are using the log-loss function as evaluation metric.

Which of the following option is / are true for interpretation of log-loss as an evaluation metric?


If a classifier is confident about an incorrect classification, then log-loss will penalise it heavily.
For a particular observation, the classifier assigns a very small probability for the correct class then the corresponding contribution to the log-loss will be very large.
Lower the log-loss, the better is the model.
A) 1 and 3

B) 2 and 3

C) 1 and 2

D) 1,2 and 3

Solution: (D)Options are self-explanatory.

 

Context Question 31-32
Below are five samples given in the dataset.



Note: Visual distance between the points in the image represents the actual distance.

 

31) Which of the following is leave-one-out cross-validation accuracy for 3-NN (3-nearest neighbor)?

A) 0

D) 0.4

C) 0.8

D) 1

Solution: (C)

In Leave-One-Out cross validation, we will select (n-1) observations for training and 1 observation of validation. Consider each point as a cross validation point and then find the 3 nearest point to this point. So if you repeat this procedure for all points you will get the correct classification for all positive class given in the above figure but negative class will be misclassified. Hence you will get 80% accuracy.

 

32) Which of the following value of K will have least leave-one-out cross validation accuracy?

A) 1NN

B) 3NN

C) 4NN

D) All have same leave one out error

Solution: (A)Each point which will always be misclassified in 1-NN which means that you will get the 0% accuracy.

 

33) Suppose you are given the below data and you want to apply a logistic regression model for classifying it in two given classes.



You are using logistic regression with L1 regularization.

Where C is the regularization parameter and w1 & w2 are the coefficients of x1 and x2.

Which of the following option is correct when you increase the value of C from zero to a very large value?

A) First w2 becomes zero and then w1 becomes zero

B) First w1 becomes zero and then w2 becomes zero

C) Both becomes zero at the same time

D) Both cannot be zero even after very large value of C

Solution: (B)

By looking at the image, we see that even on just using x2, we can efficiently perform classification. So at first w1 will become 0. As regularization parameter increases more, w2 will come more and more closer to 0.

34) Suppose we have a dataset which can be trained with 100% accuracy with help of a decision tree of depth 6. Now consider the points below and choose the option based on these points.

Note: All other hyper parameters are same and other factors are not affected. 

Depth 4 will have high bias and low variance
Depth 4 will have low bias and low variance
A) Only 1

B) Only 2

C) Both 1 and 2

D) None of the above

Solution: (A)If you fit decision tree of depth 4 in such data means it will more likely to underfit the data. So, in case of underfitting you will have high bias and low variance.

 

35) Which of the following options can be used to get global minima in k-Means Algorithm?

Try to run algorithm for different centroid initialization
Adjust number of iterations
Find out the optimal number of clusters
A) 2 and 3

B) 1 and 3

C) 1 and 2

D) All of above

Solution: (D)All of the option can be tuned to find the global minima.

 

36) Imagine you are working on a project which is a binary classification problem. You trained a model on training dataset and get the below confusion matrix on validation dataset.



Based on the above confusion matrix, choose which option(s) below will give you correct predictions?

Accuracy is ~0.91
Misclassification rate is ~ 0.91
False positive rate is ~0.95
True positive rate is ~0.95
A) 1 and 3

B) 2 and 4

C) 1 and 4

D) 2 and 3

Solution: (C)

The Accuracy (correct classification) is (50+100)/165 which is nearly equal to 0.91.

The true Positive Rate is how many times you are predicting positive class correctly so true positive rate would be 100/105 = 0.95 also known as “Sensitivity” or “Recall”

37) For which of the following hyperparameters, higher value is better for decision tree algorithm?

Number of samples used for split
Depth of tree
Samples for leaf
A)1 and 2

B) 2 and 3

C) 1 and 3

D) 1, 2 and 3

E) Can’t say

Solution: (E)

For all three options A, B and C, it is not necessary that if you increase the value of parameter the performance may increase. For example, if we have a very high value of depth of tree, the resulting tree may overfit the data, and would not generalize well. On the other hand, if we have a very low value, the tree may underfit the data. So, we can’t say for sure that “higher is better”.

 

Context 38-39
Imagine, you have a 28 * 28 image and you run a 3 * 3 convolution neural network on it with the input depth of 3 and output depth of 8.

Note: Stride is 1 and you are using same padding.

38) What is the dimension of output feature map when you are using the given parameters.

A) 28 width, 28 height and 8 depth

B) 13 width, 13 height and 8 depth

C) 28 width, 13 height and 8 depth

D) 13 width, 28 height and 8 depth

Solution: (A)The formula for calculating output size is

output size = (N – F)/S + 1

where, N is input size, F is filter size and S is stride.

Read this article to get a better understanding.

 

39)  What is the dimensions of output feature map when you are using following parameters.

A)  28 width, 28 height and 8 depth

B) 13 width, 13 height and 8 depth

C) 28 width, 13 height and 8 depth

D) 13 width, 28 height and 8 depth

Solution: (B)Same as above

 

40) Suppose, we were plotting the visualization for different values of C (Penalty parameter) in SVM algorithm. Due to some reason, we forgot to tag the C values with visualizations. In that case, which of the following option best explains the C values for the images below (1,2,3 left to right, so C values are C1 for image1, C2 for image2 and C3 for image3 ) in case of rbf kernel.



A) C1 = C2 = C3

B) C1 > C2 > C3

C) C1 < C2 < C3

D) None of these

Solution: (C)

Penalty parameter C of the error term. It also controls the trade-off between smooth decision boundary and classifying the training points correctly. For large values of C, the optimization will choose a smaller-margin hyperplane. Read more here.

Q1 Which of the following techniques can be used for the purpose of keyword normalization, the process of converting a keyword into its base form?

Lemmatization
Levenshtein
Stemming
Soundex
A) 1 and 2
B) 2 and 4
C) 1 and 3
D) 1, 2 and 3
E) 2, 3 and 4
F) 1, 2, 3 and 4

Solution: (C)

Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.

&nsbp;
2) N-grams are defined as the combination of N keywords together. How many bi-grams can be generated from a given sentence:

“Analytics Vidhya is a great source to learn data science”

A) 7
B) 8
C) 9
D) 10
E) 11

Solution: (C)

Bigrams: Analytics Vidhya, Vidhya is, is a, a great, great source, source to, To learn, learn data, data science

 

3) How many trigrams phrases can be generated from the following sentence, after performing following text cleaning steps:

Stopword Removal
Replacing punctuations by a single space
“#Analytics-vidhya is a great source to learn @data_science.”

A) 3
B) 4
C) 5
D) 6
E) 7

Solution: (C)

After performing stopword removal and punctuation replacement the text becomes: “Analytics vidhya great source learn data science”

Trigrams – Analytics vidhya great, vidhya great source, great source learn, source learn data, learn data science

 

4) Which of the following regular expression can be used to identify date(s) present in the text object:

“The next meetup on data science will be held on 2017-09-21, previously it happened on 31/03, 2016”

A) \d{4}-\d{2}-\d{2}
B) (19|20)\d{2}-(0[1-9]|1[0-2])-[0-2][1-9]
C) (19|20)\d{2}-(0[1-9]|1[0-2])-([0-2][1-9]|3[0-1])
D) None of the above

Solution: (D)

None if these expressions would be able to identify the dates in this text object.

 

Question Context 5-6:

You have collected a data of about 10,000 rows of tweet text and no other information. You want to create a tweet classification model that categorizes each of the tweets in three buckets – positive, negative and neutral.

5) Which of the following models can perform tweet classification with regards to context mentioned above?

A) Naive Bayes
B) SVM
C) None of the above

Solution: (C)

Since, you are given only the data of tweets and no other information, which means there is no target variable present. One cannot train a supervised learning model, both svm and naive bayes are supervised learning techniques.

 

6) You have created a document term matrix of the data, treating every tweet as one document. Which of the following is correct, in regards to document term matrix?

Removal of stopwords from the data will affect the dimensionality of data
Normalization of words in the data will reduce the dimensionality of data
Converting all the words in lowercase will not affect the dimensionality of the data
A) Only 1
B) Only 2
C) Only 3
D) 1 and 2
E) 2 and 3
F) 1, 2 and 3

Solution: (D)

Choices A and B are correct because stopword removal will decrease the number of features in the matrix, normalization of words will also reduce redundant features, and, converting all words to lowercase will also decrease the dimensionality.

 

7) Which of the following features can be used for accuracy improvement of a classification model?

A) Frequency count of terms
B) Vector Notation of sentence
C) Part of Speech Tag
D) Dependency Grammar
E) All of these

Solution: (E)

All of the techniques can be used for the purpose of engineering features in a model.

 

8) What percentage of the total statements are correct with regards to Topic Modeling?

It is a supervised learning technique
LDA (Linear Discriminant Analysis) can be used to perform topic modeling
Selection of number of topics in a model does not depend on the size of data
Number of topic terms are directly proportional to size of the data
A) 0
B) 25
C) 50
D) 75
E) 100

Solution: (A)

LDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant analysis. Selection of the number of topics is directly proportional to the size of the data, while number of topic terms is not directly proportional to the size of the data. Hence none of the statements are correct.

 

9) In Latent Dirichlet Allocation model for text classification purposes, what does alpha and beta hyperparameter represent-

A) Alpha: number of topics within documents, beta: number of terms within topics False
B) Alpha: density of terms generated within topics, beta: density of topics generated within terms False
C) Alpha: number of topics within documents, beta: number of terms within topics False
D) Alpha: density of topics generated within documents, beta: density of terms generated within topics True

Solution: (D)

Option D is correct

 

10) Solve the equation according to the sentence “I am planning to visit New Delhi to attend Analytics Vidhya Delhi Hackathon”.

A = (# of words with Noun as the part of speech tag)
B = (# of words with Verb as the part of speech tag)
C = (# of words with frequency count greater than one)

What are the correct values of A, B, and C?

A) 5, 5, 2
B) 5, 5, 0
C) 7, 5, 1
D) 7, 4, 2
E) 6, 4, 3

Solution: (D)

Nouns: I, New, Delhi, Analytics, Vidhya, Delhi, Hackathon (7)

Verbs: am, planning, visit, attend (4)

Words with frequency counts > 1: to, Delhi (2)

Hence option D is correct.

 

11) In a corpus of N documents, one document is randomly picked. The document contains a total of T terms and the term “data” appears K times.

What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term “data” appears in approximately one-third of the total documents?

A) KT * Log(3)
B) K * Log(3) / T
C) T * Log(3) / K
D) Log(3) / KT

Solution: (B)

formula for TF is K/T

formula for IDF is log(total docs / no of docs containing “data”)

= log(1 / (⅓))

= log (3)

Hence correct choice is Klog(3)/T

 

Question Context 12 to 14:

Refer the following document term matrix


12) Which of the following documents contains the same number of terms and the number of terms in the one of the document is not equal to least number of terms in any document in the entire corpus.

A) d1 and d4
B) d6 and d7
C) d2 and d4
D) d5 and d6

Solution: (C)

Both of the documents d2 and d4 contains 4 terms and does not contain the least number of terms which is 3.

 

13) Which are the most common and the rarest term of the corpus?

A) t4, t6
B) t3, t5
C) t5, t1
D) t5, t6

Solution: (A)

T5 is most common terms across 5 out of 7 documents, T6 is rare term only appears in d3 and d4

 

14) What is the term frequency of a term which is used a maximum number of times in that document?

A) t6 – 2/5
B) t3 – 3/6
C) t4 – 2/6
D) t1 – 2/6

Solution: (B)

t3 is used max times in entire corpus = 3, tf for t3 is 3/6

 

15) Which of the following technique is not a part of flexible text matching?

A) Soundex
B) Metaphone
C) Edit Distance
D) Keyword Hashing

Solution: (D)

Except Keyword Hashing all other are the techniques used in flexible string matching

Feel like improving your skillset? Click Here

16) True or False: Word2Vec model is a machine learning model used to create vector notations of text objects. Word2vec contains multiple deep neural networks

A) TRUE
B) FALSE

Solution: (B)

Word2vec also contains preprocessing model which is not a deep neural network

 

17) Which of the following statement is(are) true for Word2Vec model?

A) The architecture of word2vec consists of only two layers – continuous bag of words and skip-gram model
B) Continuous bag of word (CBOW) is a Recurrent Neural Network model
C) Both CBOW and Skip-gram are shallow neural network models
D) All of the above

Solution: (C)

Word2vec contains the Continuous bag of words and skip-gram models, which are deep neural nets.

 

18) With respect to this context-free dependency graphs, how many sub-trees exists in the sentence?


A) 3
B) 4
C) 5
D) 6

Solution: (D)

Subtrees in the dependency graph can be viewed as nodes having an outward link, for example:

Media, networking, play, role, billions, and lives are the roots of subtrees

 

19) What is the right order for a text classification model components

Text cleaning
Text annotation
Gradient descent
Model tuning
Text to predictors
A) 12345
B) 13425
C) 12534
D) 13452

Solution: (C)

A right text classification model contains – cleaning of text to remove noise, annotation to create more features, converting text-based features into predictors, learning a model using gradient descent and finally tuning a model.

 

20) Polysemy is defined as the coexistence of multiple meanings for a word or phrase in a text object. Which of the following models is likely the best choice to correct this problem?

A) Random Forest Classifier
B) Convolutional Neural Networks
C) Gradient Boosting
D) All of these

Solution: (B)

CNNs are popular choice for text classification problems because they take into consideration left and right contexts of the words as features which can solve the problem of polysemy

 

21) Which of the following models can be used for the purpose of document similarity?

A) Training a word 2 vector model on the corpus that learns context present in the document
B) Training a bag of words model that learns occurrence of words in the document
C) Creating a document-term matrix and using cosine similarity for each document
D) All of the above

Solution: (D)

word2vec model can be used for measuring document similarity based on context. Bag Of Words and document term matrix can be used for measuring similarity based on terms.

 

22) What are the possible features of a text corpus

Count of word in a document
Boolean feature – presence of word in a document
Vector notation of word
Part of Speech Tag
Basic Dependency Grammar
Entire document as a feature
A) 1
B) 12
C) 123
D) 1234
E) 12345
F) 123456

Solution: (E)

Except for entire document as the feature, rest all can be used as features of text classification learning model.

 

23) While creating a machine learning model on text data, you created a document term matrix of the input data of 100K documents. Which of the following remedies can be used to reduce the dimensions of data –

Latent Dirichlet Allocation
Latent Semantic Indexing
Keyword Normalization
A) only 1
B) 2, 3
C) 1, 3
D) 1, 2, 3

Solution: (D)

All of the techniques can be used to reduce the dimensions of the data.

 

24) Google Search’s feature – “Did you mean”, is a mixture of different techniques. Which of the following techniques are likely to be ingredients?

Collaborative Filtering model to detect similar user behaviors (queries)
Model that checks for Levenshtein distance among the dictionary terms
Translation of sentences into multiple languages
A) 1
B) 2
C) 1, 2
D) 1, 2, 3

Solution: (C)

Collaborative filtering can be used to check what are the patterns used by people, Levenshtein is used to measure the distance among dictionary terms.

 

25) While working with text data obtained from news sentences, which are structured in nature, which of the grammar-based text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection and object detection.

A) Part of speech tagging
B) Dependency Parsing and Constituency Parsing
C) Skip Gram and N-Gram extraction
D) Continuous Bag of Words

Solution: (B)

Dependency and constituent parsing extract these relations from the text

 

26) Social Media platforms are the most intuitive form of text data. You are given a corpus of complete social media data of tweets. How can you create a model that suggests the hashtags?

A) Perform Topic Models to obtain most significant words of the corpus
B) Train a Bag of Ngrams model to capture top n-grams – words and their combinations
C) Train a word2vector model to learn repeating contexts in the sentences
D) All of these

Solution: (D)

All of the techniques can be used to extract most significant terms of a corpus.

 

27) While working with context extraction from a text data, you encountered two different sentences: The tank is full of soldiers. The tank is full of nitrogen. Which of the following measures can be used to remove the problem of word sense disambiguation in the sentences?

A) Compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood
B) Co-reference resolution in which one resolute the meaning of ambiguous word with the proper noun present in the previous sentence
C) Use dependency parsing of sentence to understand the meanings

Solution: (A)

Option 1 is called Lesk algorithm, used for word sense disambiguation, rest others cannot be used.

 

28) Collaborative Filtering and Content Based Models are the two popular recommendation engines, what role does NLP play in building such algorithms.

A) Feature Extraction from text
B) Measuring Feature Similarity
C) Engineering Features for vector space learning model
D) All of these

Solution: (D)

NLP can be used anywhere where text data is involved – feature extraction, measuring feature similarity, create vector features of the text.

 

29) Retrieval based models and Generative models are the two popular techniques used for building chatbots. Which of the following is an example of retrieval model and generative model respectively.

A) Dictionary based learning and Word 2 vector model
B) Rule-based learning and Sequence to Sequence model
C) Word 2 vector and Sentence to Vector model
D) Recurrent neural network and convolutional neural network

Solution: (B)

choice 2 best explains examples of retrieval based models and generative models

 

30) What is the major difference between CRF (Conditional Random Field) and HMM (Hidden Markov Model)?

A) CRF is Generative whereas HMM is Discriminative model
B) CRF is Discriminative whereas HMM is Generative model
C) Both CRF and HMM are Generative model
D) Both CRF and HMM are Discriminative model

Solution: (B)

Option B is correct

1) Which of the following is/are true about bagging trees?

In bagging trees, individual trees are independent of each other
Bagging is the method for improving the performance by aggregating the results of weak learners
A) 1
B) 2
C) 1 and 2
D) None of these

Solution: C

Both options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features and samples.

 

2) Which of the following is/are true about boosting trees?

In boosting trees, individual weak learners are independent of each other
It is the method for improving the performance by aggregating the results of weak learners
A) 1
B) 2
C) 1 and 2
D) None of these

Solution: B

In boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. Bagging and boosting both can be consider as improving the base learners results.

 

3) Which of the following is/are true about Random Forest and Gradient Boosting ensemble methods?

Both methods can be used for classification task
Random Forest is use for classification whereas Gradient Boosting is use for regression task
Random Forest is use for regression whereas Gradient Boosting is use for Classification task
Both methods can be used for regression task
A) 1
B) 2
C) 3
D) 4
E) 1 and 4

Solution: E

Both algorithms are design for classification as well as regression task.

 

4) In Random forest you can generate hundreds of trees (say T1, T2 …..Tn) and then aggregate the results of these tree. Which of the following is true about individual(Tk) tree in Random Forest?

Individual tree is built on a subset of the features
Individual tree is built on all the features
Individual tree is built on a subset of observations
Individual tree is built on full set of observations
A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4

Solution: A

Random forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees.

 

5) Which of the following is true about “max_depth” hyperparameter in Gradient Boosting?

Lower is better parameter in case of same validation accuracy
Higher is better parameter in case of same validation accuracy
Increase the value of max_depth may overfit the data
Increase the value of max_depth may underfit the data
A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4

Solution: A

Increase the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always prefer the small depth in final model building.

 

6) Which of the following algorithm doesn’t uses learning Rate as of one of its hyperparameter?

Gradient Boosting
Extra Trees
AdaBoost
Random Forest
A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4

Solution: D

Random Forest and Extra Trees don’t have learning rate as a hyperparameter.

 

7) Which of the following algorithm would you take into the consideration in your final model building on the basis of performance?

Suppose you have given the following graph which shows the ROC curve for two different classification algorithms such as Random Forest(Red) and Logistic Regression(Blue)



A) Random Forest
B) Logistic Regression
C) Both of the above
D) None of these

Solution: A

Since, Random forest has largest AUC given in the picture so I would prefer Random Forest

 

8) Which of the following is true about training and testing error in such case?

Suppose you want to apply AdaBoost algorithm on Data D which has T observations. You set half the data for training and half for testing initially. Now you want to increase the number of data points for training T1, T2 … Tn where T1 < T2…. Tn-1 < Tn.

A) The difference between training error and test error increases as number of observations increases
B) The difference between training error and test error decreases as number of observations increases
C) The difference between training error and test error will not change
D) None of These

Solution: B

As we have more and more data, training error increases and testing error de-creases. And they all converge to the true error.

 

9) In random forest or gradient boosting algorithms, features can be of any type. For example, it can be a continuous feature or a categorical feature. Which of the following option is true when you consider these types of features?

A) Only Random forest algorithm handles real valued attributes by discretizing them
B) Only Gradient boosting algorithm handles real valued attributes by discretizing them
C) Both algorithms can handle real valued attributes by discretizing them
D) None of these

Solution: C

Both can handle real valued features.

 

10) Which of the following algorithm are not an example of ensemble learning algorithm?

A) Random Forest
B) Adaboost
C) Extra Trees
D) Gradient Boosting
E) Decision Trees

Solution: E

Decision trees doesn’t aggregate the results of multiple trees so it is not an ensemble algorithm.

 

11) Suppose you are using a bagging based algorithm say a RandomForest in model building. Which of the following can be true?

Number of tree should be as large as possible
You will have interpretability after using RandomForest
A) 1
B) 2
C) 1 and 2
D) None of these

Solution: A

Since Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building.  Random Forest is a black box model you will lose interpretability after using it.

 

Context 12-15

Consider the following figure for answering the next few questions. In the figure, X1 and X2 are the two features and the data point is represented by dots (-1 is negative class and +1 is a positive class). And you first split the data based on feature X1(say splitting point is x11) which is shown in the figure using vertical line. Every value less than x11 will be predicted as positive class and greater than x will be predicted as negative class.

12) How many data points are misclassified in above image?

A) 1
B) 2
C) 3
D) 4

Solution: A

Only one observation is misclassified, one negative class is showing at the left side of vertical line which will be predicting as a positive class.

 

13) Which of the following splitting point on feature x1 will classify the data correctly?

A) Greater than x11
B) Less than x11
C) Equal to x11
D) None of above

Solution: D

If you search any point on X1 you won’t find any point that gives 100% accuracy.

 

14) If you consider only feature X2 for splitting. Can you now perfectly separate the positive class from negative class for any one split on X2?

A) Yes
B) No

Solution: B

It is also not possible.

 

15) Now consider only one splitting on both (one on X1 and one on X2) feature. You can split both features at any point. Would you be able to classify all data points correctly?

A) TRUE
B) FALSE

Solution: B

You won’t find such case because you can get minimum 1 misclassification.

 

Context 16-17

Suppose, you are working on a binary classification problem with 3 input features. And you chose to apply a bagging algorithm(X) on this data. You chose max_features = 2 and the n_estimators =3. Now, Think that each estimators have 70% accuracy.

Note: Algorithm X is aggregating the results of individual estimators based on maximum voting

16) What will be the maximum accuracy you can get?

A) 70%
B) 80%
C) 90%
D) 100%

Solution: D

Refer below table for models M1, M2 and M3.

 

 

 

Actual predictions	M1	M2	M3	 Output
1	1	0	1	1
1	1	0	1	1
1	1	0	1	1
1	0	1	1	1
1	0	1	1	1
1	0	1	1	1
1	1	1	1	1
1	1	1	0	1
1	1	1	0	1
1	1	1	0	1
 

 

17) What will be the minimum accuracy you can get?

A) Always greater than 70%
B) Always greater than and equal to 70%
C) It can be less than 70%
D) None of these

Solution: C

Refer below table for models M1, M2 and M3.

Actual predictions	M1	M2	M3	 Output
1	1	0	0	0
1	1	1	1	1
1	1	0	0	0
1	0	1	0	0
1	0	1	1	1
1	0	0	1	0
1	1	1	1	1
1	1	1	1	1
1	1	1	1	1
1	1	1	1	1
 

18) Suppose you are building random forest model, which split a node on the attribute, that has highest information gain. In the below image, select the attribute which has the highest information gain?

 


A) Outlook
B) Humidity
C) Windy
D) Temperature

Solution: A

Information gain increases with the average purity of subsets. So option A would be the right answer.

 

19) Which of the following is true about the Gradient Boosting trees?

In each stage, introduce a new regression tree to compensate the shortcomings of existing model
We can use gradient decent method for minimize the loss function
A) 1
B) 2
C) 1 and 2
D) None of these

Solution: C

Both are true and self explanatory

 

20) True-False: The bagging is suitable for high variance low bias models?

A) TRUE
B) FALSE 

Solution: A

The bagging is suitable for high variance low bias models or you can say for complex models.


21) Which of the following is true when you choose fraction of observations for building the base learners in tree based algorithm?

A) Decrease the fraction of samples to build a base learners will result in decrease in variance
B) Decrease the fraction of samples to build a base learners will result in increase in variance
C) Increase the fraction of samples to build a base learners will result in decrease in variance
D) Increase the fraction of samples to build a base learners will result in Increase in variance

Solution: A

Answer is self explanatory

 

Context 22-23

Suppose, you are building a Gradient Boosting model on data, which has millions of observations and 1000’s of features. Before building the model you want to consider the difference parameter setting for time measurement.


22) Consider the hyperparameter “number of trees” and arrange the options in terms of time taken by each hyperparameter for building the Gradient Boosting model?

Note: remaining hyperparameters are same

Number of trees = 100
Number of trees = 500
Number of trees = 1000
A) 1~2~3
B) 1<2<3

C) 1>2>3
D) None of these

Solution: B

The time taken by building 1000 trees is maximum and time taken by building the 100 trees is minimum which is given in solution B

 

23) Now, Consider the learning rate hyperparameter and arrange the options in terms of time taken by each hyperparameter for building the Gradient boosting model?

Note: Remaining hyperparameters are same

1. learning rate = 1
2. learning rate = 2
3. learning rate = 3

A) 1~2~3
B) 1<2<3

C) 1>2>3
D) None of these

Solution: A

Since learning rate doesn’t affect time so all learning rates would take equal time.

 

24) In greadient boosting it is important use learning rate to get optimum output. Which of the following is true abut choosing the learning rate?

A) Learning rate should be as high as possible
B) Learning Rate should be as low as possible
C) Learning Rate should be low but it should not be very low
D) Learning rate should be high but it should not be very high

Solution: C

Learning rate should be low but it should not be very low otherwise algorithm will take so long to finish the training because you need to increase the number trees.

25) [True or False] Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.

A) TRUE
B) FALSE

Solution: A

 

26) When you use the boosting algorithm you always consider the weak learners. Which of the following is the main reason for having weak learners?

To prevent overfitting
To prevent under fitting
A) 1
B) 2
C) 1 and 2
D) None of these

Solution: A

To prevent overfitting, since the complexity of the overall learner increases at each step. Starting with weak learners implies the final classifier will be less likely to overfit.

 

27) To apply bagging to regression trees which of the following is/are true in such case?

We build the N regression with N bootstrap sample
We take the average the of N regression tree
Each tree has a high variance with low bias
A) 1 and 2
B) 2 and 3
C) 1 and 3
D) 1,2 and 3

Solution: D

All of the options are correct and self explanatory

 

28) How to select best hyperparameters in tree based models?

A) Measure performance over training data
B) Measure performance over validation data
C) Both of these
D) None of these

Solution: B

We always consider the validation results to compare with the test result.

 

29) In which of the following scenario a gain ratio is preferred over Information Gain?

A) When a categorical variable has very large number of category
B) When a categorical variable has very small number of category
C) Number of categories is the not the reason
D) None of these

Solution: A

When high cardinality problems, gain ratio is preferred over Information Gain technique.

 

30) Suppose you have given the following scenario for training and validation error for Gradient Boosting. Which of the following hyper parameter would you choose in such case?

Scenario	Depth	Training Error	Validation Error
1	2	100	110
2	4	90	105
3	6	50	100
4	8	45	105
5	10	30	150
 

A) 1
B) 2
C) 3
D) 4

Solution: B

Scenario 2 and 4 has same validation accuracies but we would select 2 because depth is lower is better hyper parameter.

1) If you remove the following any one red points from the data. Does the decision boundary will change?

A) Yes
B) No

Solution: A

These three examples are positioned such that removing any one of them introduces slack in the constraints. So the decision boundary would completely change.

 

2) [True or False] If you remove the non-red circled points from the data, the decision boundary will change?

A) True
B) False

Solution: B

On the other hand, rest of the points in the data won’t affect the decision boundary much.

 

3) What do you mean by generalization error in terms of the SVM?

A) How far the hyperplane is from the support vectors
B) How accurately the SVM can predict outcomes for unseen data
C) The threshold amount of error in an SVM

Solution: B

Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.

 

4) When the C parameter is set to infinite, which of the following holds true?

A) The optimal hyperplane if exists, will be the one that completely separates the data
B) The soft-margin classifier will separate the data
C) None of the above

Solution: A

At such a high level of misclassification penalty, soft margin will not hold existence as there will be no room for error.

 

5) What do you mean by a hard margin?

A) The SVM allows very low error in classification
B) The SVM allows high amount of error in classification
C) None of the above

Solution: A

A hard margin means that an SVM is very rigid in classification and tries to work extremely well in the training set, causing overfitting.

 

6) The minimum time complexity for training an SVM is O(n2). According to this fact, what sizes of datasets are not best suited for SVM’s?

A) Large datasets
B) Small datasets
C) Medium sized datasets
D) Size does not matter

Solution: A

Datasets which have a clear classification boundary will function best with SVM’s.

 

7) The effectiveness of an SVM depends upon:

A) Selection of Kernel
B) Kernel Parameters
C) Soft Margin Parameter C
D) All of the above

Solution: D

The SVM effectiveness depends upon how you choose the basic 3 requirements mentioned above in such a way that it maximises your efficiency, reduces error and overfitting.

 

8) Support vectors are the data points that lie closest to the decision surface.

A) TRUE
B) FALSE

Solution: A

They are the points closest to the hyperplane and the hardest ones to classify. They also have a direct bearing on the location of the decision surface.

 

9) The SVM’s are less effective when:

A) The data is linearly separable
B) The data is clean and ready to use
C) The data is noisy and contains overlapping points

Solution: C

When the data has noise and overlapping points, there is a problem in drawing a clear hyperplane without misclassifying.

 

10) Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?

A) The model would consider even far away points from hyperplane for modeling
B) The model would consider only the points close to the hyperplane for modeling
C) The model would not be affected by distance of points from hyperplane for modeling
D) None of the above

Solution: B

The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.

For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.

For a higher gamma, the model will capture the shape of the dataset well.

 

11) The cost parameter in the SVM means:

A) The number of cross-validations to be made
B) The kernel to be used
C) The tradeoff between misclassification and simplicity of the model
D) None of the above

Solution: C

The cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. It is also simply referred to as the cost of misclassification.

 

12)

Suppose you are building a SVM model on data X. The data X can be error prone which means that you should not trust any specific data point too much. Now think that you want to build a SVM model which has quadratic kernel function of polynomial degree 2 that uses Slack variable C as one of it’s hyper parameter. Based upon that give the answer for following question.

What would happen when you use very large value of C(C->infinity)?

Note: For small C was also classifying all data points correctly

A) We can still classify data correctly for given setting of hyper parameter C
B) We can not classify data correctly for given setting of hyper parameter C
C) Can’t Say
D) None of these

Solution: A

For large values of C, the penalty for misclassifying points is very high, so the decision boundary will perfectly separate the data if possible.

 

13) What would happen when you use very small C (C~0)?

A) Misclassification would happen
B) Data will be correctly classified
C) Can’t say
D) None of these

Solution: A

The classifier can maximize the margin between most of the points, while misclassifying a few points, because the penalty is so low.

 

14) If I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on validation set, what should I look out for?

A) Underfitting
B) Nothing, the model is perfect
C) Overfitting

Solution: C

If we’re achieving 100% training accuracy very easily, we need to check to verify if we’re overfitting our data.

 

15) Which of the following are real world applications of the SVM?

A) Text and Hypertext Categorization
B) Image Classification
C) Clustering of News Articles
D) All of the above

Solution: D

SVM’s are highly versatile models that can be used for practically all real world problems ranging from regression to clustering and handwriting recognitions.

 

Question Context: 16 – 18

Suppose you have trained an SVM with linear decision boundary after training SVM, you correctly infer that your SVM model is under fitting.

16) Which of the following option would you more likely to consider iterating SVM next time?

A) You want to increase your data points
B) You want to decrease your data points
C) You will try to calculate more variables
D) You will try to reduce the features

Solution: C

The best option here would be to create more features for the model.

 

17) Suppose you gave the correct answer in previous question. What do you think that is actually happening?

1. We are lowering the bias
2. We are lowering the variance
3. We are increasing the bias
4. We are increasing the variance

A) 1 and 2
B) 2 and 3
C) 1 and 4
D) 2 and 4

Solution: C

Better model will lower the bias and increase the variance

 

18) In above question suppose you want to change one of it’s(SVM) hyperparameter so that effect would be same as previous questions i.e model will not under fit?

A) We will increase the parameter C
B) We will decrease the parameter C
C) Changing in C don’t effect
D) None of these

Solution: A

Increasing C parameter would be the right thing to do here, as it will ensure regularized model

 

19) We usually use feature normalization before using the Gaussian kernel in SVM. What is true about feature normalization?

1. We do feature normalization so that new feature will dominate other
2. Some times, feature normalization is not feasible in case of categorical variables
3. Feature normalization always helps when we use Gaussian kernel in SVM

A) 1
B) 1 and 2
C) 1 and 3
D) 2 and 3

Solution: B

Statements one and two are correct.

 

Question Context: 20-22

Suppose you are dealing with 4 class classification problem and you want to train a SVM model on the data for that you are using One-vs-all method. Now answer the below questions?

20) How many times we need to train our SVM model in such case?

A) 1
B) 2
C) 3
D) 4

Solution: D

For a 4 class problem, you would have to train the SVM at least 4 times if you are using a one-vs-all method.

 

21) Suppose you have same distribution of classes in the data. Now, say for training 1 time in one vs all setting the SVM is taking 10 second. How many seconds would it require to train one-vs-all method end to end?

A) 20
B) 40
C) 60
D) 80

Solution: B

It would take 10×4 = 40 seconds

 

22) Suppose your problem has changed now. Now, data has only 2 classes. What would you think how many times we need to train SVM in such case?

A) 1
B) 2
C) 3
D) 4

Solution: A

Training the SVM only one time would give you appropriate results

 

Question context: 23 – 24

Suppose you are using SVM with linear kernel of polynomial degree 2, Now think that you have applied this on data and found that it perfectly fit the data that means, Training and testing accuracy is 100%.

23) Now, think that you increase the complexity(or degree of polynomial of this kernel). What would you think will happen?

A) Increasing the complexity will overfit the data
B) Increasing the complexity will underfit the data
C) Nothing will happen since your model was already 100% accurate
D) None of these

Solution: A

Increasing the complexity of the data would make the algorithm overfit the data.

 

24) In the previous question after increasing the complexity you found that training accuracy was still 100%. According to you what is the reason behind that?

1. Since data is fixed and we are fitting more polynomial term or parameters so the algorithm starts memorizing everything in the data
2. Since data is fixed and SVM doesn’t need to search in big hypothesis space

A) 1
B) 2
C) 1 and 2
D) None of these

Solution: C

Both the given statements are correct.

 

25) What is/are true about kernel in SVM?

1. Kernel function map low dimensional data to high dimensional space
2. It’s a similarity function

A) 1
B) 2
C) 1 and 2
D) None of these

Solution: C

Both the given statements are correct.

1) Imagine, you have 1000 input features and 1 target feature in a machine learning problem. You have to select 100 most important features based on the relationship between input features and the target features.

Do you think, this is an example of dimensionality reduction?

A. Yes

B. No

Solution: (A)

 

 

2) [ True or False ] It is not necessary to have a target variable for applying dimensionality reduction algorithms.

A. TRUE

B. FALSE

Solution: (A)

LDA is an example of supervised dimensionality reduction algorithm.

 

3) I have 4 variables in the dataset such as – A, B, C & D. I have performed the following actions:

Step 1: Using the above variables, I have created two more variables, namely E = A + 3 * B and F = B + 5 * C + D.

Step 2: Then using only the variables E and F I have built a Random Forest model.

Could the steps performed above represent a dimensionality reduction method?

A. True

B. False

Solution: (A)

Yes, Because Step 1 could be used to represent the data into 2 lower dimensions.

 

4) Which of the following techniques would perform better for reducing dimensions of a data set?

A. Removing columns which have too many missing values

B. Removing columns which have high variance in data

C. Removing columns with dissimilar data trends

D. None of these

Solution: (A)

If columns have too many missing values, (say 99%) then we can remove such columns.

 

5) [ True or False ] Dimensionality reduction algorithms are one of the possible ways to reduce the computation time required to build a model.

A. TRUE

B. FALSE

Solution: (A)

Reducing the dimension of data will take less time to train a model.

 

6) Which of the following algorithms cannot be used for reducing the dimensionality of data?

A. t-SNE

B. PCA

C. LDA False

D. None of these

Solution: (D)

All of the algorithms are the example of dimensionality reduction algorithm.

 

7) [ True or False ] PCA can be used for projecting and visualizing data in lower dimensions.

A. TRUE

B. FALSE

Solution: (A)

Sometimes it is very useful to plot the data in lower dimensions. We can take the first 2 principal components and then visualize the data using scatter plot.

 

8) The most popularly used dimensionality reduction algorithm is Principal Component Analysis (PCA). Which of the following is/are true about PCA?

PCA is an unsupervised method
It searches for the directions that data have the largest variance
Maximum number of principal components <= number of features
All principal components are orthogonal to each other
A. 1 and 2

B. 1 and 3

C. 2 and 3

D. 1, 2 and 3

E. 1,2 and 4

F. All of the above

Solution: (F)

All options are self explanatory.

 

9) Suppose we are using dimensionality reduction as pre-processing technique, i.e, instead of using all the features, we reduce the data to k dimensions with PCA. And then use these PCA projections as our features. Which of the following statement is correct?

A. Higher ‘k’ means more regularization

B. Higher ‘k’ means less regularization

C. Can’t Say

Solution: (B)

Higher k would lead to less smoothening as we would be able to preserve more characteristics in data, hence less regularization.

 

10) In which of the following scenarios is t-SNE better to use than PCA for dimensionality reduction while working on a local machine with minimal computational power?

A. Dataset with 1 Million entries and 300 features

B. Dataset with 100000 entries and 310 features

C. Dataset with 10,000 entries and 8 features

D. Dataset with 10,000 entries and 200 features

Solution: (C)

t-SNE has quadratic time and space complexity. Thus it is a very heavy algorithm in terms of system resource utilization.

 

11) Which of the following statement is true for a t-SNE cost function?

A. It is asymmetric in nature.

B. It is symmetric in nature.

C. It is same as the cost function for SNE.

Solution: (B)

Cost function of SNE is asymmetric in nature. Which makes it difficult to converge using gradient decent. A symmetric cost function is one of the major differences between SNE and t-SNE.

 

Question 12

Imagine you are dealing with text data. To represent the words you are using word embedding (Word2vec). In word embedding, you will end up with 1000 dimensions. Now, you want to reduce the dimensionality of this high dimensional data such that, similar words should have a similar meaning in nearest neighbor space.In such case, which of the following algorithm are you most likely choose?

A. t-SNE

B. PCA

C. LDA

D. None of these

Solution: (A)

t-SNE stands for t-Distributed Stochastic Neighbor Embedding which consider the nearest neighbours for reducing the data.

 

13) [True or False] t-SNE learns non-parametric mapping.

A. TRUE

B. FALSE

Solution: (A)

t-SNE learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. For more information read from this link.

 

14) Which of the following statement is correct for t-SNE and PCA?

A. t-SNE is linear whereas PCA is non-linear

B. t-SNE and PCA both are linear

C. t-SNE and PCA both are nonlinear

D. t-SNE is nonlinear whereas PCA is linear

Solution: (D)

Option D is correct. Read the explanation from this link

 

15) In t-SNE algorithm, which of the following hyper parameters can be tuned?

A. Number of dimensions

B. Smooth measure of effective number of neighbours

C. Maximum number of iterations

D. All of the above

Solution: (D)

All of the hyper-parameters in the option can tuned.

 

16) What is of the following statement is true about t-SNE in comparison to PCA?

A. When the data is huge (in size), t-SNE may fail to produce better results.

B. T-NSE always produces better result regardless of the size of the data

C. PCA always performs better than t-SNE for smaller size data.

D. None of these

Solution: (A)

Option A is correct

 

17) Xi and Xj are two distinct points in the higher dimension representation, where as Yi & Yj are the representations of Xi and Xj in a lower dimension.

1. The similarity of datapoint Xi to datapoint Xj is the conditional probability p (j|i) .

2. The similarity of datapoint Yi to datapoint Yj is the conditional probability q (j|i) .

Which of the following must be true for perfect representation of xi and xj in lower dimensional space?

A. p (j|i) = 0 and q (j|i) = 1

B. p (j|i) < q (j|i)

C. p (j|i) = q (j|i)

D. p (j|i) > q (j|i)

Solution: (C)

The conditional probabilities for similarity of two points must be equal because similarity between the points must remain unchanged in both higher and lower dimension for them to be perfect representations.

 

18) Which of the following is true about LDA?


A. LDA aims to maximize the distance between class and minimize the within class distance

B. LDA aims to minimize both distance between class and distance within class

C. LDA aims to minimize the distance between class and maximize the distance within class

D. LDA aims to maximize both distance between class and distance within class

Solution: (A)

Option A is correct.

 

19) In which of the following case LDA will fail?

A. If the discriminatory information is not in the mean but in the variance of the data

B. If the discriminatory information is in the mean but not in the variance of the data

C. If the discriminatory information is in the mean and variance of the data

D. None of these

Solution: (A)

Option A is correct

 

20) Which of the following comparison(s) are true about PCA and LDA?

Both LDA and PCA are linear transformation techniques
LDA is supervised whereas PCA is unsupervised
PCA maximize the variance of the data, whereas LDA maximize the separation between different classes,
A. 1 and 2

B. 2 and 3

C. 1 and 3

D. Only 3

E. 1, 2 and 3

Solution: (E)

All of the options are correct

 

21) What will happen when eigenvalues are roughly equal?

A. PCA will perform outstandingly

B. PCA will perform badly

C. Can’t Say

D.None of above

Solution: (B)

When all eigen vectors are same in such case you won’t be able to select the principal components because in that case all principal components are equal.

 

22) PCA works better if there is?

A linear structure in the data
If the data lies on a curved surface and not on a flat surface
If variables are scaled in the same unit
A. 1 and 2

B. 2 and 3

C. 1 and 3

D. 1 ,2 and 3

Solution: (C)

Option C is correct

 

23) What happens when you get features in lower dimensions using PCA?

The features will still have interpretability
The features will lose interpretability
The features must carry all information present in data
The features may not carry all information present in data
A. 1 and 3

B. 1 and 4

C. 2 and 3

D. 2 and 4

Solution: (D)

When you get the features in lower dimensions then you will lose some information of data most of the times and you won’t be able to interpret the lower dimension data.

 

24) Imagine, you are given the following scatterplot between height and weight.

Select the angle which will capture maximum variability along a single axis?

A. ~ 0 degree

B. ~ 45 degree

C. ~ 60 degree

D. ~ 90 degree

Solution: (B)

Option B has largest possible variance in data.

 

25) Which of the following option(s) is / are true?

You need to initialize parameters in PCA
You don’t need to initialize parameters in PCA
PCA can be trapped into local minima problem
PCA can’t be trapped into local minima problem
A. 1 and 3

B. 1 and 4

C. 2 and 3

D. 2 and 4

Solution: (D)

PCA is a deterministic algorithm which doesn’t have parameters to initialize and it doesn’t have local minima problem like most of the machine learning algorithms has.

 

Question Context 26

The below snapshot shows the scatter plot of two features (X1 and X2) with the class information (Red, Blue). You can also see the direction of PCA and LDA.

26) Which of the following method would result into better class prediction?

A. Building a classification algorithm with PCA (A principal component in direction of PCA)

B. Building a classification algorithm with LDA

C. Can’t say

D. None of these

Solution: (B)

If our goal is to classify these points, PCA projection does only more harm than good—the majority of blue and red points would land overlapped on the first principal component.hence PCA would confuse the classifier.

 

27) Which of the following options are correct, when you are applying PCA on a image dataset?

It can be used to effectively detect deformable objects.
It is invariant to affine transforms.
It can be used for lossy image compression.
It is not invariant to shadows.
A. 1 and 2

B. 2 and 3

C. 3 and 4

D. 1 and 4

Solution: (C)

Option C is correct

 

28) Under which condition SVD and PCA produce the same projection result?

A. When data has zero median

B. When data has zero mean

C. Both are always same

D. None of these

Solution: (B)

When the data has a zero mean vector, otherwise you have to center the data first before taking SVD.

 

Question Context 29

Consider 3 data points in the 2-d space: (-1, -1), (0,0), (1,1).

29) What will be the first principal component for this data?

[ √ 2 /2 , √ 2/ 2 ]
(1/ √ 3, 1/ √ 3)
([ -√ 2/ 2 , √ 2/ 2 ])
(- 1/ √ 3, – 1/ √ 3)
A. 1 and 2

B. 3 and 4

C. 1 and 3

D. 2 and 4

Solution: (C)

The first principal component is v = [ √ 2 /2 , √ 2/ 2 ] T (you shouldn’t really need to solve any SVD or eigenproblem to see this). Note that the principal component should be normalized to have unit length. (The negation v = [− √ 2/ 2 , − √ 2/ 2 ] T is also correct.)

 

30) If we project the original data points into the 1-d subspace by the principal component [ √ 2 /2, √ 2 /2 ] T. What are their coordinates in the 1-d subspace?

A. (− √ 2 ), (0), (√ 2)

B. (√ 2 ), (0), (√ 2)

C. ( √ 2 ), (0), (-√ 2)

D. (-√ 2 ), (0), (-√ 2)

Solution: (A)

The coordinates of three points after projection should be z1 = x T 1 v = [−1, −1][ √ 2/ 2 , √ 2 /2 ] T = − √ 2, z2 = x T 2 v = 0, z3 = x T 3 v = √ 2.

 

31) For the projected data you just obtained projections ( (− √ 2 ), (0), (√ 2) ). Now if we represent them in the original 2-d space and consider them as the reconstruction of the original data points, what is the reconstruction error? Context: 29-31:

A. 0%

B. 10%

C. 30%

D. 40%

Solution: (A)

The reconstruction error is 0, since all three points are perfectly located on the direction of the first principal component. Or, you can actually calculate the reconstruction: z1 ·v.

xˆ1 = − √ 2·[ √ 2/ 2 , √ 2/2 ] T = [−1, −1]T
xˆ2 = 0*[0, 0]T = [0,0]
xˆ3 = √ 2* [1, 1]T = [1,1]

which are exactly x1, x2, x3.

 

32) In LDA, the idea is to find the line that best separates the two classes. In the given image which of the following is a good projection?
A. LD1

B. LD2

C. Both

D. None of these

Solution: (A)

LD1 Is a good projection because it best separates the class.

 

Question Context 33

PCA is a good technique to try, because it is simple to understand and is commonly used to reduce the dimensionality of the data. Obtain the eigenvalues λ1 ≥ λ2 ≥ • • • ≥ λN and plot. 



To see how f(M) increases with M and takes maximum value 1 at M = D. We have two graph given below:

33) Which of the above graph shows better performance of PCA? Where M is first M principal components and D is total number of features?

A. Left

B. Right

C. Any of A and B

D. None of these

Solution: (A)

PCA is good if f(M) asymptotes rapidly to 1. This happens if the first eigenvalues are big and the remainder are small. PCA is bad if all the eigenvalues are roughly equal. See examples of both cases in figure.

 

34) Which of the following option is true?

A. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class.

B. Both attempt to model the difference between the classes of data.

C. PCA explicitly attempts to model the difference between the classes of data. LDA on the other hand does not take into account any difference in class.

D. Both don’t attempt to model the difference between the classes of data.

Solution: (A)

Options are self explanatory.

 

35) Which of the following can be the first 2 principal components after applying PCA?

(0.5, 0.5, 0.5, 0.5) and (0.71, 0.71, 0, 0)
(0.5, 0.5, 0.5, 0.5) and (0, 0, -0.71, -0.71)
(0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5)
(0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5)
A. 1 and 2

B. 1 and 3

C. 2 and 4

D. 3 and 4

Solution: (D)

For the first two choices, the two loading vectors are not orthogonal.

 

36) Which of the following gives the difference(s) between the logistic regression and LDA?

If the classes are well separated, the parameter estimates for logistic regression can be unstable.
If the sample size is small and distribution of features are normal for each class. In such case, linear discriminant analysis is more stable than logistic regression.
A. 1

B. 2

C. 1 and 2

D. None of these

Solution: (C)

Refer this video

 

37) Which of the following offset, do we consider in PCA?
A. Vertical offset

B. Perpendicular offset

C. Both

D. None of these

Solution: (B)

We always consider residual as vertical offsets. Perpendicular offset are useful in case of PCA

 

38) Imagine you are dealing with 10 class classification problem and you want to know that at most how many discriminant vectors can be produced by LDA. What is the correct answer?

A. 20

B. 9

C. 21

D. 11

E. 10

Solution: (B)

LDA produces at most c − 1 discriminant vectors. You may refer this link for more information.

 

Question Context 39

The given dataset consists of images of “Hoover Tower” and some other towers. Now, you want to use PCA (Eigenface) and the nearest neighbour method to build a classifier that predicts whether new image depicts “Hoover tower” or not. The figure gives the sample of your input training images.



39) In order to get reasonable performance from the “Eigenface” algorithm, what pre-processing steps will be required on these images?

Align the towers in the same position in the image.
Scale or crop all images to the same size.
A. 1

B. 2

C. 1 and 2

D. None of these

Solution: (C)

Both the statements are correct.

 

40) What are the optimum number of principle components in the below figure ?


A. 7

B. 30

C. 40

D. Can’t Say

Solution: (B)

We can see in the above figure that the number of components = 30 is giving highest variance with lowest number of components. Hence option ‘B’ is the right answer.

Q1. Movie Recommendation systems are an example of:

Classification
Clustering
Reinforcement Learning
Regression
Options:

B. A. 2 Only

C. 1 and 2

D. 1 and 3

E. 2 and 3

F. 1, 2 and 3

H. 1, 2, 3 and 4

Solution: (E)

Generally, movie recommendation systems cluster the users in a finite number of similar groups based on their previous activities and profile. Then, at a fundamental level, people in the same cluster are made similar recommendations.

In some scenarios, this can also be approached as a classification problem for assigning the most appropriate movie class to the user of a specific group of users. Also, a movie recommendation system can be viewed as a reinforcement learning problem where it learns by its previous recommendations and improves the future recommendations.

 

Q2. Sentiment Analysis is an example of:

Regression
Classification
Clustering
Reinforcement Learning
Options:

A. 1 Only

B. 1 and 2

C. 1 and 3

D. 1, 2 and 3

E. 1, 2 and 4

F. 1, 2, 3 and 4

Solution: (E)

Sentiment analysis at the fundamental level is the task of classifying the sentiments represented in an image, text or speech into a set of defined sentiment classes like happy, sad, excited, positive, negative, etc. It can also be viewed as a regression problem for assigning a sentiment score of say 1 to 10 for a corresponding image, text or speech.

Another way of looking at sentiment analysis is to consider it using a reinforcement learning perspective where the algorithm constantly learns from the accuracy of past sentiment analysis performed to improve the future performance.

 

Q3. Can decision trees be used for performing clustering?

A. True

B. False

Solution:  (A)

Decision trees can also be used to for clusters in the data but clustering often generates natural clusters and is not dependent on any objective function.

 

Q4. Which of the following is the most appropriate strategy for data cleaning before performing clustering analysis, given less than desirable number of data points:

Capping and flouring of variables
Removal of outliers
Options:

A. 1 only

B. 2 only

C. 1 and 2

D. None of the above

Solution: (A)

Removal of outliers is not recommended if the data points are few in number. In this scenario, capping and flouring of variables is the most appropriate strategy.

 

Q5. What is the minimum no. of variables/ features required to perform clustering?

A. 0

B. 1

C. 2

D. 3

Solution: (B)

At least a single variable is required to perform clustering analysis. Clustering analysis with a single variable can be visualized with the help of a histogram.

 

Q6. For two runs of K-Mean clustering is it expected to get same clustering results?

A. Yes

B. No

Solution: (B)

K-Means clustering algorithm instead converses on local minima which might also correspond to the global minima in some cases but not always. Therefore, it’s advised to run the K-Means algorithm multiple times before drawing inferences about the clusters.

However, note that it’s possible to receive same clustering results from K-means by setting the same seed value for each run. But that is done by simply making the algorithm choose the set of same random no. for each run.

 

Q7. Is it possible that Assignment of observations to clusters does not change between successive iterations in K-Means

A. Yes

B. No

C. Can’t say

D. None of these

Solution: (A)

When the K-Means algorithm has reached the local or global minima, it will not alter the assignment of data points to clusters for two successive iterations.

 

Q8. Which of the following can act as possible termination conditions in K-Means?

For a fixed number of iterations.
Assignment of observations to clusters does not change between iterations. Except for cases with a bad local minimum.
Centroids do not change between successive iterations.
Terminate when RSS falls below a threshold.
Options:

A. 1, 3 and 4

B. 1, 2 and 3

C. 1, 2 and 4

D. All of the above

Solution: (D)

All four conditions can be used as possible termination condition in K-Means clustering:

This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations.
Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long.
This also ensures that the algorithm has converged at the minima.
Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. Practically, it’s a good practice to combine it with a bound on the number of iterations to guarantee termination.
 

Q9. Which of the following clustering algorithms suffers from the problem of convergence at local optima?

K- Means clustering algorithm
Agglomerative clustering algorithm
Expectation-Maximization clustering algorithm
Diverse clustering algorithm
Options:

A. 1 only

B. 2 and 3

C. 2 and 4

D. 1 and 3

E. 1,2 and 4

F. All of the above

Solution: (D) 

Out of the options given, only K-Means clustering algorithm and EM clustering algorithm has the drawback of converging at local minima.

 

Q10. Which of the following algorithm is most sensitive to outliers?

A. K-means clustering algorithm

B. K-medians clustering algorithm

C. K-modes clustering algorithm

D. K-medoids clustering algorithm

Solution: (A)

Out of all the options, K-Means clustering algorithm is most sensitive to outliers as it uses the mean of cluster data points to find the cluster center.

 

Q11. After performing K-Means Clustering analysis on a dataset, you observed the following dendrogram. Which of the following conclusion can be drawn from the dendrogram?



A. There were 28 data points in clustering analysis

B. The best no. of clusters for the analyzed data points is 4

C. The proximity function used is Average-link clustering

D. The above dendrogram interpretation is not possible for K-Means clustering analysis

Solution: (D)

A dendrogram is not possible for K-Means clustering analysis. However, one can create a cluster gram based on K-Means clustering analysis.

 

Q12. How can Clustering (Unsupervised Learning) be used to improve the accuracy of Linear Regression model (Supervised Learning):

Creating different models for different cluster groups.
Creating an input feature for cluster ids as an ordinal variable.
Creating an input feature for cluster centroids as a continuous variable.
Creating an input feature for cluster size as a continuous variable.
Options:

A. 1 only

B. 1 and 2

C. 1 and 4

D. 3 only

E. 2 and 4

F. All of the above

Solution: (F)

Creating an input feature for cluster ids as ordinal variable or creating an input feature for cluster centroids as a continuous variable might not convey any relevant information to the regression model for multidimensional data. But for clustering in a single dimension, all of the given methods are expected to convey meaningful information to the regression model. For example, to cluster people in two groups based on their hair length, storing clustering ID as ordinal variable and cluster centroids as continuous variables will convey meaningful information.

 

Q13. What could be the possible reason(s) for producing two different dendrograms using agglomerative clustering algorithm for the same dataset?

A. Proximity function used

B. of data points used

C. of variables used

D. B and c only

E. All of the above

Solution: (E)

Change in either of Proximity function, no. of data points or no. of variables will lead to different clustering results and hence different dendrograms.

 

Q14. In the figure below, if you draw a horizontal line on y-axis for y=2. What will be the number of clusters formed?



A. 1

B. 2

C. 3

D. 4

Solution: (B)

Since the number of vertical lines intersecting the red horizontal line at y=2 in the dendrogram are 2, therefore, two clusters will be formed.

 

Q15. What is the most appropriate no. of clusters for the data points represented by the following dendrogram:



A. 2

B. 4

C. 6

D. 8

Solution: (B)

The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.



In the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB.

 

Q16. In which of the following cases will K-Means clustering fail to give good results?

Data points with outliers
Data points with different densities
Data points with round shapes
Data points with non-convex shapes
Options:

A. 1 and 2

B. 2 and 3

C. 2 and 4

D. 1, 2 and 4

E. 1, 2, 3 and 4

Solution: (D)

K-Means clustering algorithm fails to give good results when the data contains outliers, the density spread of data points across the data space is different and the data points follow non-convex shapes.



 

Q17. Which of the following metrics, do we have for finding dissimilarity between two clusters in hierarchical clustering?

Single-link
Complete-link
Average-link
Options:

A. 1 and 2

B. 1 and 3

C. 2 and 3

D. 1, 2 and 3

Solution: (D)

All of the three methods i.e. single link, complete link and average link can be used for finding dissimilarity between two clusters in hierarchical clustering.

 

Q18. Which of the following are true?

Clustering analysis is negatively affected by multicollinearity of features
Clustering analysis is negatively affected by heteroscedasticity
Options:

A. 1 only

B. 2 only

C. 1 and 2

D. None of them

Solution: (A)

Clustering analysis is not negatively affected by heteroscedasticity but the results are negatively impacted by multicollinearity of features/ variables used in clustering as the correlated feature/ variable will carry extra weight on the distance calculation than desired.

 

Q19. Given, six points with the following attributes:



Which of the following clustering representations and dendrogram depicts the use of MIN or Single link proximity function in hierarchical clustering:

A. 

B. 

C. 

D. 

 

Solution: (A)

For the single link or MIN version of hierarchical clustering, the proximity of two clusters is defined to be the minimum of the distance between any two points in the different clusters. For instance, from the table, we see that the distance between points 3 and 6 is 0.11, and that is the height at which they are joined into one cluster in the dendrogram. As another example, the distance between clusters {3, 6} and {2, 5} is given by dist({3, 6}, {2, 5}) = min(dist(3, 2), dist(6, 2), dist(3, 5), dist(6, 5)) = min(0.1483, 0.2540, 0.2843, 0.3921) = 0.1483.

 

Q20 Given, six points with the following attributes:

 

Which of the following clustering representations and dendrogram depicts the use of MAX or Complete link proximity function in hierarchical clustering:

A.   

B.  

C.  

D.  

Solution: (B)

For the single link or MAX version of hierarchical clustering, the proximity of two clusters is defined to be the maximum of the distance between any two points in the different clusters. Similarly, here points 3 and 6 are merged first. However, {3, 6} is merged with {4}, instead of {2, 5}. This is because the dist({3, 6}, {4}) = max(dist(3, 4), dist(6, 4)) = max(0.1513, 0.2216) = 0.2216, which is smaller than dist({3, 6}, {2, 5}) = max(dist(3, 2), dist(6, 2), dist(3, 5), dist(6, 5)) = max(0.1483, 0.2540, 0.2843, 0.3921) = 0.3921 and dist({3, 6}, {1}) = max(dist(3, 1), dist(6, 1)) = max(0.2218, 0.2347) = 0.2347.

 

Q21 Given, six points with the following attributes:

 

Which of the following clustering representations and dendrogram depicts the use of Group average proximity function in hierarchical clustering:

A.  

B.  
C. 

D. 

Solution: (C)

For the group average version of hierarchical clustering, the proximity of two clusters is defined to be the average of the pairwise proximities between all pairs of points in the different clusters. This is an intermediate approach between MIN and MAX. This is expressed by the following equation:



Here, the distance between some clusters. dist({3, 6, 4}, {1}) = (0.2218 + 0.3688 + 0.2347)/(3 ∗ 1) = 0.2751. dist({2, 5}, {1}) = (0.2357 + 0.3421)/(2 ∗ 1) = 0.2889. dist({3, 6, 4}, {2, 5}) = (0.1483 + 0.2843 + 0.2540 + 0.3921 + 0.2042 + 0.2932)/(6∗1) = 0.2637. Because dist({3, 6, 4}, {2, 5}) is smaller than dist({3, 6, 4}, {1}) and dist({2, 5}, {1}), these two clusters are merged at the fourth stage

 

Q22. Given, six points with the following attributes:

 

Which of the following clustering representations and dendrogram depicts the use of Ward’s method proximity function in hierarchical clustering:

A. 

B. 

C.  

D. 

Solution: (D)

Ward method is a centroid method. Centroid method calculates the proximity between two clusters by calculating the distance between the centroids of clusters. For Ward’s method, the proximity between two clusters is defined as the increase in the squared error that results when two clusters are merged. The results of applying Ward’s method to the sample data set of six points. The resulting clustering is somewhat different from those produced by MIN, MAX, and group average.

 

Q23. What should be the best choice of no. of clusters based on the following results:



A. 1

B. 2

C. 3

D. 4

Solution: (C)

The silhouette coefficient is a measure of how similar an object is to its own cluster compared to other clusters. Number of clusters for which silhouette coefficient is highest represents the best choice of the number of clusters.

 

Q24. Which of the following is/are valid iterative strategy for treating missing values before clustering analysis?

A. Imputation with mean

B. Nearest Neighbor assignment

C. Imputation with Expectation Maximization algorithm

D. All of the above

Solution: (C)

All of the mentioned techniques are valid for treating missing values before clustering analysis but only imputation with EM algorithm is iterative in its functioning.

 

Q25. K-Mean algorithm  has some limitations. One of the limitation it has is, it makes hard assignments(A point either completely belongs to a cluster or not belongs at all) of points to clusters.

Note: Soft assignment can be consider as the probability of being assigned to each cluster: say K = 3 and for some point xn, p1 = 0.7, p2 = 0.2, p3 = 0.1)

Which of the following algorithm(s) allows soft assignments?

Gaussian mixture models
Fuzzy K-means
Options:

A. 1 only

B. 2 only

C. 1 and 2

D. None of these

Solution: (C)

Both, Gaussian mixture models and Fuzzy K-means allows soft assignments.

 

Q26. Assume, you want to cluster 7 observations into 3 clusters using K-Means clustering algorithm. After first iteration clusters, C1, C2, C3 has following observations:

C1: {(2,2), (4,4), (6,6)}

C2: {(0,4), (4,0)}

C3: {(5,5), (9,9)}

What will be the cluster centroids if you want to proceed for second iteration?

A. C1: (4,4), C2: (2,2), C3: (7,7)

B. C1: (6,6), C2: (4,4), C3: (9,9)

C. C1: (2,2), C2: (0,0), C3: (5,5)

D. None of these

Solution: (A)

Finding centroid for data points in cluster C1 = ((2+4+6)/3, (2+4+6)/3) = (4, 4)

Finding centroid for data points in cluster C2 = ((0+4)/2, (4+0)/2) = (2, 2)

Finding centroid for data points in cluster C3 = ((5+9)/2, (5+9)/2) = (7, 7)

Hence, C1: (4,4),  C2: (2,2), C3: (7,7)

 

Q27. Assume, you want to cluster 7 observations into 3 clusters using K-Means clustering algorithm. After first iteration clusters, C1, C2, C3 has following observations:

C1: {(2,2), (4,4), (6,6)}

C2: {(0,4), (4,0)}

C3: {(5,5), (9,9)}

What will be the Manhattan distance for observation (9, 9) from cluster centroid C1. In second iteration.

A. 10

B. 5*sqrt(2)

C. 13*sqrt(2)

D. None of these

Solution: (A)

Manhattan distance between centroid C1 i.e. (4, 4) and (9, 9) = (9-4) + (9-4) = 10

 

Q28. If two variables V1 and V2, are used for clustering. Which of the following are true for K means clustering with k =3?

If V1 and V2 has a correlation of 1, the cluster centroids will be in a straight line
If V1 and V2 has a correlation of 0, the cluster centroids will be in straight line
Options:

A. 1 only

B. 2 only

C. 1 and 2

D. None of the above

Solution: (A)

If the correlation between the variables V1 and V2 is 1, then all the data points will be in a straight line. Hence, all the three cluster centroids will form a straight line as well.

 

Q29. Feature scaling is an important step before applying K-Mean algorithm. What is reason behind this?

A. In distance calculation it will give the same weights for all features

B. You always get the same clusters. If you use or don’t use feature scaling

C. In Manhattan distance it is an important step but in Euclidian it is not

D. None of these

Solution; (A)

Feature scaling ensures that all the features get same weight in the clustering analysis. Consider a scenario of clustering people based on their weights (in KG) with range 55-110 and height (in inches) with range 5.6 to 6.4. In this case, the clusters produced without scaling can be very misleading as the range of weight is much higher than that of height. Therefore, its necessary to bring them to same scale so that they have equal weightage on the clustering result.

 

Q30. Which of the following method is used for finding optimal of cluster in K-Mean algorithm?

A. Elbow method

B. Manhattan method

C. Ecludian mehthod

D. All of the above

E. None of these

Solution: (A)

Out of the given options, only elbow method is used  for finding the optimal number of clusters. The elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t give much better modeling of the data.

 

Q31. What is true about K-Mean Clustering?

K-means is extremely sensitive to cluster center initializations
Bad initialization can lead to Poor convergence speed
Bad initialization can lead to bad overall clustering
Options:

A. 1 and 3

B. 1 and 2

C. 2 and 3

D. 1, 2 and 3

Solution: (D)

All three of the given statements are true. K-means is extremely sensitive to cluster center initialization. Also, bad initialization can lead to Poor convergence speed as well as bad overall clustering.

 

Q32. Which of the following can be applied to get good results for K-means algorithm corresponding to global minima?

Try to run algorithm for different centroid initialization
Adjust number of iterations
Find out the optimal number of clusters
Options:

A. 2 and 3

B. 1 and 3

C. 1 and 2

D. All of above

Solution: (D)

All of these are standard practices that are used in order to obtain good clustering results.

 

Q33. What should be the best choice for number of clusters based on the following results:



A. 5

B. 6

C. 14

D. Greater than 14

Solution: (B)

Based on the above results, the best choice of number of clusters using elbow method is 6.

 

Q34. What should be the best choice for number of clusters based on the following results:



A. 2

B. 4

C. 6

D. 8

Solution: (C)

Generally, a higher average silhouette coefficient indicates better clustering quality. In this plot, the optimal clustering number of grid cells in the study area should be 2, at which the value of the average silhouette coefficient is highest. However, the SSE of this clustering solution (k = 2) is too large. At k = 6, the SSE is much lower. In addition, the value of the average silhouette coefficient at k = 6 is also very high, which is just lower than k = 2. Thus, the best choice is k = 6.

 

Q35. Which of the following sequences is correct for a K-Means algorithm using Forgy method of initialization?

Specify the number of clusters
Assign cluster centroids randomly
Assign each data point to the nearest cluster centroid
Re-assign each point to nearest cluster centroids
Re-compute cluster centroids
Options:

A. 1, 2, 3, 5, 4

B. 1, 3, 2, 4, 5

C. 2, 1, 3, 4, 5

D. None of these

Solution: (A)

The methods used for initialization in K means are Forgy and Random Partition. The Forgy method randomly chooses k observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster’s randomly assigned points.

 

Q36. If you are using Multinomial mixture models with the expectation-maximization algorithm for clustering a set of data points into two clusters, which of the assumptions are important:

A. All the data points follow two Gaussian distribution

B. All the data points follow n Gaussian distribution (n >2)

C. All the data points follow two multinomial distribution

D. All the data points follow n multinomial distribution (n >2)

Solution: (C)

In EM algorithm for clustering its essential to choose the same no. of clusters to classify the data points into as the no. of different distributions they are expected to be generated from and also the distributions must be of the same type.

 

Q37. Which of the following is/are not true about Centroid based K-Means clustering algorithm and Distribution based expectation-maximization clustering algorithm:

Both starts with random initializations
Both are iterative algorithms
Both have strong assumptions that the data points must fulfill
Both are sensitive to outliers
Expectation maximization algorithm is a special case of K-Means
Both requires prior knowledge of the no. of desired clusters
The results produced by both are non-reproducible.
Options:

A. 1 only

B. 5 only

C. 1 and 3

D. 6 and 7

E. 4, 6 and 7

F. None of the above

Solution: (B)

All of the above statements are true except the 5th as instead K-Means is a special case of EM algorithm in which only the centroids of the cluster distributions are calculated at each iteration.

 

Q38. Which of the following is/are not true about DBSCAN clustering algorithm:

For data points to be in a cluster, they must be in a distance threshold to a core point
It has strong assumptions for the distribution of data points in dataspace
It has substantially high time complexity of order O(n3)
It does not require prior knowledge of the no. of desired clusters
It is robust to outliers
Options:

A. 1 only

B. 2 only

C. 4 only

D. 2 and 3

E. 1 and 5

F. 1, 3 and 5

Solution: (D)

DBSCAN can form a cluster of any arbitrary shape and does not have strong assumptions for the distribution of data points in the dataspace.
DBSCAN has a low time complexity of order O(n log n) only.
 

Q39. Which of the following are the high and low bounds for the existence of F-Score?

A. [0,1]

B. (0,1)

C. [-1,1]

D. None of the above

Solution: (A)

The lowest and highest possible values of F score are 0 and 1 with 1 representing that every data point is assigned to the correct cluster and 0 representing that the precession and/ or recall of the clustering analysis are both 0. In clustering analysis, high value of F score is desired.

 

Q40. Following are the results observed for clustering 6000 data points into 3 clusters: A, B and C:



What is the F1-Score with respect to cluster B?

A. 3

B. 4

C. 5

D. 6

Solution: (D)

Here,

True Positive, TP = 1200

True Negative, TN = 600 + 1600 = 2200

False Positive, FP = 1000 + 200 = 1200

False Negative, FN = 400 + 400 = 800

Therefore,

Precision = TP / (TP + FP) = 0.5

Recall = TP / (TP + FN) = 0.6

Hence,

F1  = 2 * (Precision * Recall)/ (Precision + recall) = 0.54 ~ 0.5

Q1. A neural network model is said to be inspired from the human brain.



The neural network consists of many neurons, each neuron takes an input, processes it and gives an output. Here’s a diagrammatic representation of a real neuron.



 Which of the following statement(s) correctly represents a real neuron?

A. A neuron has a single input and a single output only

B. A neuron has multiple inputs but a single output only

C. A neuron has a single input but multiple outputs

D. A neuron has multiple inputs and multiple outputs

E. All of the above statements are valid

Solution: (E)

A neuron can have a single Input / Output or multiple Inputs / Outputs.

 

Q2. Below is a mathematical representation of a neuron.

The different components of the neuron are denoted as:

x1, x2,…, xN: These are inputs to the neuron. These can either be the actual observations from input layer or an intermediate value from one of the hidden layers.
w1, w2,…,wN: The Weight of each input.
bi: Is termed as Bias units. These are constant values added to the input of the activation function corresponding to each weight. It works similar to an intercept term.
a:  Is termed as the activation of the neuron which can be represented as
and y: is the output of the neuron


Considering the above notations, will a line equation (y = mx + c) fall into the category of a neuron?

A. Yes

B. No

Solution: (A)

A single neuron with no non-linearity can be considered as a linear regression function.

 

Q3. Let us assume we implement an AND function to a single neuron. Below is a tabular representation of an AND function:

X1	X2	X1 AND X2
0	0	0
0	1	0
1	0	0
1	1	1
The activation function of our neuron is denoted as:





What would be the weights and bias? 

(Hint: For which values of w1, w2 and b does our neuron implement an AND function?)

A. Bias = -1.5, w1 = 1, w2 = 1

B. Bias = 1.5, w1 = 2, w2 = 2

C. Bias = 1, w1 = 1.5, w2 = 1.5

D. None of these

Solution: (A)

A.

f(-1.5*1 + 1*0 + 1*0) = f(-1.5) = 0
f(-1.5*1 + 1*0 + 1*1) = f(-0.5) = 0
f(-1.5*1 + 1*1 + 1*0) = f(-0.5) = 0
f(-1.5*1 + 1*1+ 1*1) = f(0.5) = 1
Therefore option A is correct

 

Q4. A network is created when we multiple neurons stack together. Let us take an example of a neural network simulating an XNOR function.



You can see that the last neuron takes input from two neurons before it. The activation function for all the neurons is given by:



 Suppose X1 is 0 and X2 is 1, what will be the output for the above neural network?

A. 0

B. 1

Solution: (A)

Output of a1: f(0.5*1 + -1*0 + -1*1) = f(-0.5) = 0

Output of a2: f(-1.5*1 + 1*0 + 1*1) = f(-0.5) = 0

Output of a3: f(-0.5*1 + 1*0 + 1*0) = f(-0.5) = 0

So the correct answer is A

 

Q5. In a neural network, knowing the weight and bias of each neuron is the most important step. If you can somehow get the correct value of weight and bias for each neuron, you can approximate any function. What would be the best way to approach this?

A. Assign random values and pray to God they are correct

B. Search every possible combination of weights and biases till you get the best value

C. Iteratively check that after assigning a value how far you are from the best values, and slightly change the assigned values values to make them better

D. None of these

Solution: (C)

Option C is the description of gradient descent.

 

Q6. What are the steps for using a gradient descent algorithm?

Calculate error between the actual value and the predicted value
Reiterate until you find the best weights of network
Pass an input through the network and get values from output layer
Initialize random weight and bias
Go to each neurons which contributes to the error and change its respective values to reduce the error
A. 1, 2, 3, 4, 5

B. 5, 4, 3, 2, 1

C. 3, 2, 1, 5, 4

D. 4, 3, 1, 5, 2

Solution: (D)

Option D is correct

 

Q7. Suppose you have inputs as x, y, and z with values -2, 5, and -4 respectively. You have a neuron ‘q’ and neuron ‘f’ with functions:

q = x + y

f = q * z

Graphical representation of the functions is as follows:



 What is the gradient of F with respect to x, y, and z?

(HINT: To calculate gradient, you must find (df/dx), (df/dy) and (df/dz))

A. (-3,4,4)

B. (4,4,3)

C. (-4,-4,3)

D. (3,-4,-4)

Solution: (C)

Option C is correct.

 

Q8. Now let’s revise the previous slides. We have learned that:

A neural network is a (crude) mathematical representation of a brain, which consists of smaller components called neurons.
Each neuron has an input, a processing function, and an output.
These neurons are stacked together to form a network, which can be used to approximate any function.
To get the best possible neural network, we can use techniques like gradient descent to update our neural network model.
Given above is a description of a neural network. When does a neural network model become a deep learning model?

A. When you add more hidden layers and increase depth of neural network

B. When there is higher dimensionality of data

C. When the problem is an image recognition problem

D. None of these

Solution: (A)

More depth means the network is deeper. There is no strict rule of how many layers are necessary to make a model deep, but still if there are more than 2 hidden layers, the model is said to be deep.

 

Q9. A neural network can be considered as multiple simple equations stacked together. Suppose we want to replicate the function for the below mentioned decision boundary.


Using two simple inputs h1 and h2



What will be the final equation?

A. (h1 AND NOT h2) OR (NOT h1 AND h2)

B. (h1 OR NOT h2) AND (NOT h1 OR h2)

C. (h1 AND h2) OR (h1 OR h2)

D. None of these

Solution: (A)

As you can see, combining h1 and h2 in an intelligent way can get you a complex equation easily. Refer Chapter 9 of this book

 

Q10. “Convolutional Neural Networks can perform various types of transformation (rotations or scaling) in an input”. Is the statement correct True or False?

A. True

B. False

Solution: (B)

Data Preprocessing steps (viz rotation, scaling) is necessary before you give the data to neural network because neural network cannot do it itself.

 

Q11. Which of the following techniques perform similar operations as dropout in a neural network?

A. Bagging

B. Boosting

C. Stacking

D. None of these

Solution: (A)

Dropout can be seen as an extreme form of bagging in which each model is trained on a single case and each parameter of the model is very strongly regularized by sharing it with the corresponding parameter in all the other models. Refer here
 

Q 12. Which of the following gives non-linearity to a neural network?

A. Stochastic Gradient Descent

B. Rectified Linear Unit

C. Convolution function

D. None of the above

Solution: (B)

Rectified Linear unit is a non-linear activation function.

 

Q13. In training a neural network, you notice that the loss does not decrease in the few starting epochs.



The reasons for this could be:

The learning is rate is low
Regularization parameter is high
Stuck at local minima
What according to you are the probable reasons?

A. 1 and 2

B. 2 and 3

C. 1 and 3

D. Any of these

Solution: (D)

The problem can occur due to any of the reasons mentioned.

 

Q14. Which of the following is true about model capacity (where model capacity means the ability of neural network to approximate complex functions) ?

A. As number of hidden layers increase, model capacity increases

B. As dropout ratio increases, model capacity increases

C. As learning rate increases, model capacity increases

D. None of these

Solution: (A)

Only option A is correct.

 

Q15. If you increase the number of hidden layers in a Multi Layer Perceptron, the classification error of test data always decreases. True or False?

A. True

B. False

Solution: (B)

This is not always true. Overfitting may cause the error to increase.

 

Q16. You are building a neural network where it gets input from the previous layer as well as from itself.



Which of the following architecture has feedback connections?

A. Recurrent Neural network

B. Convolutional Neural Network

C. Restricted Boltzmann Machine

D. None of these

Solution: (A)

Option A is correct.

 

Q17. What is the sequence of the following tasks in a perceptron?

Initialize weights of perceptron randomly
Go to the next batch of dataset
If the prediction does not match the output, change the weights
For a sample input, compute an output
A. 1, 2, 3, 4

B. 4, 3, 2, 1

C. 3, 1, 2, 4

D. 1, 4, 3, 2

Solution: (D)

Sequence D is correct.

 

Q18. Suppose that you have to minimize the cost function by changing the parameters. Which of the following technique could be used for this?

A. Exhaustive Search

B. Random Search

C. Bayesian Optimization

D. Any of these

Solution: (D)

Any of the above mentioned technique can be used to change parameters.

 

Q19. First Order Gradient descent would not work correctly (i.e. may get stuck) in which of the following graphs?

A.  

B.    

C.

D. None of these

Solution: (B)

This is a classic example of saddle point problem of gradient descent.

 

Q20. The below graph shows the accuracy of a trained 3-layer convolutional neural network vs the number of parameters (i.e. number of feature kernels).


The trend suggests that as you increase the width of a neural network, the accuracy increases till a certain threshold value, and then starts decreasing.

What could be the possible reason for this decrease?

A. Even if number of kernels increase, only few of them are used for prediction

B. As the number of kernels increase, the predictive power of neural network decrease

C. As the number of kernels increase, they start to correlate with each other which in turn helps overfitting

D. None of these

Solution: (C)

As mentioned in option C, the possible reason could be kernel correlation.

 

Q21. Suppose we have one hidden layer neural network as shown above. The hidden layer in this network works as a dimensionality reductor. Now instead of using this hidden layer, we replace it with a dimensionality reduction technique such as PCA.



Would the network that uses a dimensionality reduction technique always give same output as network with hidden layer?

A. Yes

B. No

Solution: (B)

Because PCA works on correlated features, whereas hidden layers work on predictive capacity of features.

 

Q22. Can a neural network model the function (y=1/x)?

A. Yes

B. No

Solution: (A)

Option A is true, because activation function can be reciprocal function.

 

Q23. In which neural net architecture, does weight sharing occur?

A. Convolutional neural Network

B. Recurrent Neural Network

C. Fully Connected Neural Network

D. Both A and B

Solution: (D)

Option D is correct.

 

Q24. Batch Normalization is helpful because

A. It normalizes (changes) all the input before sending it to the next layer

B. It returns back the normalized mean and standard deviation of weights

C. It is a very efficient backpropagation technique

D. None of these

Solution: (A)

To read more about batch normalization, see refer this video

 

Q25. Instead of trying to achieve absolute zero error, we set a metric called bayes error which is the error we hope to achieve. What could be the reason for using bayes error?

A. Input variables may not contain complete information about the output variable

B. System (that creates input-output mapping) may be stochastic

C. Limited training data

D. All the above

Solution: (D)

In reality achieving accurate prediction is a myth. So we should hope to achieve an “achievable result”.

 

Q26. The number of neurons in the output layer should match the number of classes (Where the number of classes is greater than 2) in a supervised learning task. True or False?

A. True

B. False

Solution: (B)

It depends on output encoding. If it is one-hot encoding, then its true. But you can have two outputs for four classes, and take the binary values as four classes(00,01,10,11).

 

Q27. In a neural network, which of the following techniques is used to deal with overfitting?

A. Dropout

B. Regularization

C. Batch Normalization

D. All of these

Solution: (D)

All of the techniques can be used to deal with overfitting.

 

Q28. Y = ax^2 + bx + c (polynomial equation of degree 2)

Can this equation be represented by a neural network of single hidden layer with linear threshold?

A. Yes

B. No

Solution: (B)

The answer is no because having a linear threshold restricts your neural network and in simple terms, makes it a consequential linear transformation function.

 

Q29. What is a dead unit in a neural network?

A. A unit which doesn’t update during training by any of its neighbour

B. A unit which does not respond completely to any of the training patterns

C. The unit which produces the biggest sum-squared error

D. None of these

Solution: (A)

Option A is correct.

Q30. Which of the following statement is the best description of early stopping?

A. Train the network until a local minimum in the error function is reached

B. Simulate the network on a test dataset after every epoch of training. Stop training when the generalization error starts to increase

C. Add a momentum term to the weight update in the Generalized Delta Rule, so that training converges more quickly

D. A faster version of backpropagation, such as the `Quickprop’ algorithm

Solution: (B)

Option B is correct.

 

Q31. What if we use a learning rate that’s too large?

A. Network will converge

B. Network will not converge

C. Can’t Say

Solution: B

Option B is correct because the error rate would become erratic and explode.

 

Q32. The network shown in Figure 1 is trained to recognize the characters H and T as shown below:



What would be the output of the network?




Could be A or B depending on the weights of neural network
Solution: (D)

Without knowing what are the weights and biases of a neural network, we cannot comment on what output it would give.

 

Q33. Suppose a convolutional neural network is trained on ImageNet dataset (Object recognition dataset). This trained model is then given a completely white image as an input.The output probabilities for this input would be equal for all classes. True or False?

A. True

B. False

Solution: (B)

There would be some neurons which are do not activate for white pixels as input. So the classes wont be equal.

 

Q34. When pooling layer is added in a convolutional neural network, translation in-variance is preserved. True or False?

A. True

B. False

Solution: (A)

Translation invariance is induced when you use pooling.

 

Q35. Which gradient technique is more advantageous when the data is too big to handle in RAM simultaneously?

A. Full Batch Gradient Descent

B. Stochastic Gradient Descent

Solution: (B)

Option B is correct.

 

Q36. The graph represents gradient flow of a four-hidden layer neural network which is trained using sigmoid activation function per epoch of training. The neural network suffers with the vanishing gradient problem.



Which of the following statements is true?

A. Hidden layer 1 corresponds to D, Hidden layer 2 corresponds to C, Hidden layer 3 corresponds to B and Hidden layer 4 corresponds to A

B. Hidden layer 1 corresponds to A, Hidden layer 2 corresponds to B, Hidden layer 3 corresponds to C and Hidden layer 4 corresponds to D

Solution: (A)

This is a description of a vanishing gradient problem. As the backprop algorithm goes to starting layers, learning decreases.

 

Q37. For a classification task, instead of random weight initializations in a neural network, we set all the weights to zero. Which of the following statements is true?

A. There will not be any problem and the neural network will train properly

B. The neural network will train but all the neurons will end up recognizing the same thing

C. The neural network will not train as there is no net gradient change

D. None of these

Solution: (B)

Option B is correct.

 

Q38. There is a plateau at the start. This is happening because the neural network gets stuck at local minima before going on to global minima.



To avoid this, which of the following strategy should work?

A. Increase the number of parameters, as the network would not get stuck at local minima

B. Decrease the learning rate by 10 times at the start and then use momentum

C. Jitter the learning rate, i.e. change the learning rate for a few epochs

D. None of these

Solution: (C)

Option C can be used to take a neural network out of local minima in which it is stuck.

 

Q39. For an image recognition problem (recognizing a cat in a photo), which architecture of neural network would be better suited to solve the problem?

A. Multi Layer Perceptron

B. Convolutional Neural Network

C. Recurrent Neural network

D. Perceptron

Solution: (B)

Convolutional Neural Network would be better suited for image related problems because of its inherent nature for taking into account changes in nearby locations of an image

 

Q40. Suppose while training, you encounter this issue. The error suddenly increases after a couple of iterations.



You determine that there must a problem with the data. You plot the data and find the insight that, original data is somewhat skewed and that may be causing the problem.

 



What will you do to deal with this challenge?

A. Normalize

B. Apply PCA and then Normalize

C. Take Log Transform of the data

D. None of these

Solution: (B)

First you would remove the correlations of the data and then zero center it.

 

Q41. Which of the following is a decision boundary of Neural Network? 



A) B

B) A

C) D

D) C

E) All of these

Solution: (E)

A neural network is said to be a universal function approximator, so it can theoretically represent any decision boundary.

 

Q42. In the graph below, we observe that the error has many “ups and downs”



Should we be worried?

A. Yes, because this means there is a problem with the learning rate of neural network.

B. No, as long as there is a cumulative decrease in both training and validation error, we don’t need to worry.

Solution: (B)

Option B is correct. In order to decrease these “ups and downs” try to increase the batch size.

 

Q43. What are the factors to select the depth of neural network?

Type of neural network (eg. MLP, CNN etc)
Input data
Computation power, i.e. Hardware capabilities and software capabilities
Learning Rate
The output function to map
A. 1, 2, 4, 5

B. 2, 3, 4, 5

C. 1, 3, 4, 5

D. All of these

Solution: (D)

All of the above factors are important to select the depth of neural network

 

Q44. Consider the scenario. The problem you are trying to solve has a small amount of data. Fortunately, you have a pre-trained neural network that was trained on a similar problem. Which of the following methodologies would you choose to make use of this pre-trained network?

A. Re-train the model for the new dataset

B. Assess on every layer how the model performs and only select a few of them

C. Fine tune the last couple of layers only

D. Freeze all the layers except the last, re-train the last layer

Solution: (D)

If the dataset is mostly similar, the best method would be to train only the last layer, as previous all layers work as feature extractors.

 

Q45. Increase in size of a convolutional kernel would necessarily increase the performance of a convolutional network.

A. True

B. False

Solution: (B)

Increasing kernel size would not necessarily increase performance. This depends heavily on the dataset.

1) Is the data linearly separable?



A) Yes
B) No
Solution: B

If you can draw a line or plane between the data points, it is said to be linearly separable.

 

2) Which of the following are universal approximators?

A) Kernel SVM
B) Neural Networks
C) Boosted Decision Trees
D) All of the above

Solution: D

All of the above methods can approximate any function.

 

3) In which of the following applications can we use deep learning to solve the problem?

A) Protein structure prediction
B) Prediction of chemical reactions
C) Detection of exotic particles
D) All of these

Solution: D

We can use neural network to approximate any function so it can theoretically be used to solve any problem.

 

4) Which of the following statements is true when you use 1×1 convolutions in a CNN?

A) It can help in dimensionality reduction
B) It can be used for feature pooling
C) It suffers less overfitting due to small kernel size
D) All of the above

Solution: D

1×1 convolutions are called bottleneck structure in CNN.

 

5) Question Context:

Statement 1: It is possible to train a network well by initializing all the weights as 0
Statement 2: It is possible to train a network well by initializing biases as 0

Which of the statements given above is true?

A) Statement 1 is true while Statement 2 is false
B) Statement 2 is true while statement 1 is false
C) Both statements are true
D) Both statements are false

Solution: B

Even if all the biases are zero, there is a chance that neural network may learn. On the other hand, if all the weights are zero; the neural neural network may never learn to perform the task.

 

6) The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer are

A) 50
B) Less than 50
C) More than 50
D) It is an arbitrary value

Solution: A

Since MLP is a fully connected directed graph, the number of connections are a multiple of number of nodes in input layer and hidden layer.

 

7) The input image has been converted into a matrix of size 28 X 28 and a kernel/filter of size 7 X 7 with a stride of 1. What will be the size of the convoluted matrix?

A) 22 X 22
B) 21 X 21
C) 28 X 28
D) 7 X 7

Solution: A

The size of the convoluted matrix is given by C=((I-F+2P)/S)+1, where C is the size of the Convoluted matrix, I is the size of the input matrix, F the size of the filter matrix and P the padding applied to the input matrix. Here P=0, I=28, F=7 and S=1.  There the answer is 22.

 

8) In a simple MLP model with 8 neurons in the input layer, 5 neurons in the hidden layer and 1 neuron in the output layer. What is the size of the weight matrices between hidden output layer and input hidden layer?

A) [1 X 5] , [5 X 8]

B) [8 X 5] , [ 1 X 5]

C) [8 X 5] , [5 X 1]

D) [5 x 1] , [8 X 5]

Solution: D

The size of weights between any layer 1 and layer 2 Is given by [nodes in layer 1 X nodes in layer 2]

 

9) Given below is an input matrix named I, kernel F and Convoluted matrix named C. Which of the following is the correct option for matrix C with stride =2 ?



A) 
B)
C)
D)
Solution: C

1 and 2 are automatically eliminated since they do not conform to the output size for a stride of 2. Upon calculation option 3 is the correct answer.

 

10) Given below is an input matrix of shape 7 X 7. What will be the output on applying a max pooling of size 3 X 3 with a stride of 2?

 



A)
B)
C)
D)
Solution: A

Max pooling takes a 3 X 3 matrix and takes the maximum of the matrix as the output. Slide it over the entire input matrix with a stride of 2 and you will get option (1) as the answer.

 

11) Which of the following functions can be used as an activation function in the output layer if we wish to predict the probabilities of n classes (p1, p2..pk) such that sum of p over all n equals to 1?

A) Softmax
B) ReLu
C) Sigmoid
D) Tanh

Solution: A

Softmax function is of the form  in which the sum of probabilities over all k sum to 1.

 

12) Assume a simple MLP model with 3 neurons and inputs= 1,2,3. The weights to the input neurons are 4,5 and 6 respectively. Assume the activation function is a linear constant value of 3. What will be the output ?

A) 32
B) 643
C) 96
D) 48

Solution: C

The output will be calculated as 3(1*4+2*5+6*3) = 96

 

13) Which of following activation function can’t be used at output layer to classify an image ?

A) sigmoid
B) Tanh
C) ReLU
D) If(x>5,1,0)
E) None of the above

Solution: C

ReLU gives continuous output in range 0 to infinity. But in output layer, we want a finite range of values. So option C is correct.

 

14) [True | False] In the neural network, every parameter can have their different learning rate.

A) TRUE
B) FALSE

Solution: A

Yes, we can define the learning rate for each parameter and it can be different from other parameters.

 

15) Dropout can be applied at visible layer of Neural Network model?

A) TRUE
B) FALSE

Solution: A

Look at the below model architecture, we have added a new Dropout layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning one in 5 inputs will be randomly excluded from each update cycle.

def create_model():  
    # create model  
    model = Sequential()  
    model.add(Dropout(0.2, input_shape=(60,)))  
    model.add(Dense(60, activation='relu'))  
    model.add(Dense(1, activation='sigmoid'))  
    # Compile model  sgd = SGD(lr=0.1)  
    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])  
    return model
16) I am working with the fully connected architecture having one hidden layer with 3 neurons and one output neuron to solve a binary classification challenge. Below is the structure of input and output:

Input dataset: [ [1,0,1,0] , [1,0,1,1] , [0,1,0,1] ]

Output: [ [1] , [1] , [0] ]

To train the model, I have initialized all weights for hidden and output layer with 1.

What do you say model will able to learn the pattern in the data?

A) Yes
B) No

Solution: B

As all the weights of the neural network model are same, so all the neurons will try to do the same thing and the model will never converge.

 

17) Which of the following neural network training challenge can be solved using batch normalization?

A) Overfitting
B) Restrict activations to become too high or low
C) Training is too slow
D) Both B and C
E) All of the above

Solution: D

Batch normalization restricts the activations and indirectly improves training time.

 

18) Which of the following would have a constant input in each epoch of training a Deep Learning model?

A) Weight between input and hidden layer
B) Weight between hidden and output layer
C) Biases of all hidden layer neurons
D) Activation function of output layer
E) None of the above

Solution: A

Weights between input and hidden layer are constant.

 

19) True/False: Changing Sigmoid activation to ReLu will help to get over the vanishing gradient issue?

A) TRUE
B) FALSE

Solution: A

ReLU can help in solving vanishing gradient problem.

 

20) In CNN, having max pooling always decrease the parameters?

A) TRUE
B) FALSE

Solution: B

This is not always true. If we have a max pooling layer of pooling size as 1, the parameters would remain the same.

 

21) [True or False] BackPropogation cannot be applied when using pooling layers

A) TRUE
B) FALSE

Solution: B

BackPropogation can be applied on pooling layers too.

 

22) What value would be in place of question mark?


Here we see a convolutional function being applied to input.
A) 3
B) 4
C) 5
D) 6

Solution: B

Option B is correct

 

23) For a binary classification problem, which of the following architecture would you choose?





A) 1
B) 2
C) Any one of these
D) None of these

Solution: C

We can either use one neuron as output for binary classification problem or two separate neurons.

 

24) Suppose there is an issue while training a neural network. The training loss/validation loss remains constant. What could be the possible reason?

A) Architecture is not defined correctly
B) Data given to the model is noisy
C) Both of these

Solution: C

Both architecture and data could be incorrect. Refer this article https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/

 

25) 



The red curve above denotes training accuracy with respect to each epoch in a deep learning algorithm. Both the green and blue curves denote validation accuracy.

Which of these indicate overfitting?

A) Green Curve
B) Blue Curve

Solution: B

Blue curve shows overfitting, whereas green curve is generalized.

 

26) Which of the following statement is true regrading dropout?

1: Dropout gives a way to approximate by combining many different architectures
2: Dropout demands high learning rates
3: Dropout can help preventing overfitting

A) Both 1 and 2
B) Both 1 and 3
C) Both 2 and 3
D) All 1, 2 and 3

Solution: B

Statements 1 and 3 are correct, statement 2 is not always true. Even after applying dropout and with low learning rate, a neural network can learn.

 

27) Gated Recurrent units can help prevent vanishing gradient problem in RNN.

A) True
B) False

Solution: A

Option A is correct. This is because it has implicit memory to remember past behavior.

 

28) Suppose you are using early stopping mechanism with patience as 2, at which point will the neural network model stop training?

Sr. No.	Training Loss	Validation Loss
1	1.0	1.1
2	0.9	1.0
3	0.8	1.0
4	0.7	1.0
5	0.6	1.1
 

A) 2
B) 3
C) 4
D) 5

Solution: C

As we have set patience as 2, the network will automatically stop training after  epoch 4.

 

29) [True or False] Sentiment analysis using Deep Learning is a many-to one prediction task

A) TRUE
B) FALSE

Solution: A

Option A is correct. This is because from a sequence of words, you have to predict whether the sentiment was positive or negative.

 

30) What steps can we take to prevent overfitting in a Neural Network?

A) Data Augmentation
B) Weight Sharing
C) Early Stopping
D) Dropout
E) All of the above

Solution: E

All of the above mentioned methods can help in preventing overfitting problem.

1) The difference between deep learning and machine learning algorithms is that there is no need of feature engineering in machine learning algorithms, whereas, it is recommended to do feature engineering first and then apply deep learning.

A) TRUE

B) FALSE

Solution: (B)
Deep learning itself does feature engineering whereas machine learning requires manual feature engineering.

 

2) Which of the following is a representation learning algorithm?

A) Neural network

B) Random Forest

C) k-Nearest neighbor

D) None of the above

Solution: (A)
Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning.

 

3) Which of the following option is correct for the below-mentioned techniques?

AdaGrad uses first order differentiation
L-BFGS uses second order differentiation
AdaGrad uses second order differentiation
L-BFGS uses first order differentiation
A) 1 and 2

B) 3 and 4

C) 1 and 4

D) 2 and 3

Solution: (A)
Option A is correct.

 

4) Increase in size of a convolutional kernel would necessarily increase the performance of a convolutional neural network. 

A) TRUE

B) FALSE

Solution: (B)
Kernel size is a hyperparameter and therefore by changing it we can increase or decrease performance.

 

Question Context 

Suppose we have a deep neural network model which was trained on a vehicle detection problem. The dataset consisted of images on cars and trucks and the aim was to detect name of the vehicle (the number of classes of vehicles are 10).
Now you want to use this model on different dataset which has images of only Ford Mustangs (aka car) and the task is to locate the car in an image.

5) Which of the following categories would be suitable for this type of problem?

A) Fine tune only the last couple of layers and change the last layer (classification layer) to regression layer

B) Freeze all the layers except the last, re-train the last layer

C) Re-train the model for the new dataset

D) None of these

Solution: (A)
 

6) Suppose you have 5 convolutional kernel of size 7 x 7 with zero padding and stride 1 in the first layer of a convolutional neural network. You pass an input of dimension 224 x 224 x 3 through this layer. What are the dimensions of the data which the next layer will receive? 

A) 217 x 217 x 3

B) 217 x 217 x 8

C) 218 x 218 x 5

D) 220 x 220 x 7

Solution: (C)

 

7) Suppose we have a neural network with ReLU activation function. Let’s say, we replace ReLu activations by linear activations.

Would this new neural network be able to approximate an XNOR function? 

Note: The neural network was able to approximate XNOR function with activation function ReLu.

A) Yes

B) No

Solution: (B)

If ReLU activation is replaced by linear activation, the neural network loses its power to approximate non-linear function.

 

 

8) Suppose we have a 5-layer neural network which takes 3 hours to train on a GPU with 4GB VRAM. At test time, it takes 2 seconds for single data point. 

Now we change the architecture such that we add dropout after 2nd and 4th layer with rates 0.2 and 0.3 respectively.

What would be the testing time for this new architecture?

A) Less than 2 secs

B) Exactly 2 secs

C) Greater than 2 secs

D) Can’t Say

Solution: (B)

The changes is architecture when we add dropout only changes in the training, and not at test time.

 

9) Which of the following options can be used to reduce overfitting in deep learning models?

Add more data
Use data augmentation 
Use architecture that generalizes well
Add regularization
Reduce architectural complexity
A) 1, 2, 3

B) 1, 4, 5

C) 1, 3, 4, 5

D) All of these

Solution: (D)

All of the above techniques can be used to reduce overfitting.

 

10) Perplexity is a commonly used evaluation technique when applying deep learning for NLP tasks. Which of the following statement is correct?

A) Higher the perplexity the better

B) Lower the perplexity the better

Solution: (B)

 

11) Suppose an input to Max-Pooling layer is given above. The pooling size of neurons in the layer is (3, 3).



What would be the output of this Pooling layer?

A) 3

B) 5

C) 5.5

D) 7

Solution: (D)

Max pooling works as follows, it first takes the input using the pooling size we defined, and gives out the highest activated input.

 

12) Suppose there is a neural network with the below configuration. 

If we remove the ReLU layers, we can still use this neural network to model non-linear functions.

A) TRUE

B) FALSE

Solution: (B)

 

13) Deep learning can be applied to which of the following NLP tasks?

A) Machine translation

B) Sentiment analysis

C) Question Answering system

D) All of the above

Solution: (D)

Deep learning can be applied to all of the above-mentioned NLP tasks.

 

14) Scenario 1: You are given data of the map of Arcadia city, with aerial photographs of the city and its outskirts. The task is to segment the areas into industrial land, farmland and natural landmarks like river, mountains, etc.

Scenario 2: You are given data of the map of Arcadia city, with detailed roads and distances between landmarks. This is represented as a graph structure. The task is to find out the nearest distance between two landmarks.

Deep learning can be applied to Scenario 1 but not Scenario 2.

A) TRUE

B) FALSE

Solution: (B)

Scenario 1 is on Euclidean data and scenario 2 is on Graphical data. Deep learning can be applied to both types of data.

 

15) Which of the following is a data augmentation technique used in image recognition tasks?

Horizontal flipping
Random cropping
Random scaling
Color jittering
Random translation
Random shearing
A) 1, 2, 4

B) 2, 3, 4, 5, 6

C) 1, 3, 5, 6

D) All of these

Solution: (D)

 

16) Given an n-character word, we want to predict which character would be the n+1th character in the sequence. For example, our input is “predictio” (which is a 9 character word) and we have to predict what would be the 10th character.

Which neural network architecture would be suitable to complete this task?

A) Fully-Connected Neural Network

B) Convolutional Neural Network

C) Recurrent Neural Network

D) Restricted Boltzmann Machine

Solution: (C)

Recurrent neural network works best for sequential data. Therefore, it would be best for the task.

 

17) What is generally the sequence followed when building a neural network architecture for semantic segmentation for image?

A) Convolutional network on input and deconvolutional network on output

B) Deconvolutional network on input and convolutional network on output

Solution: (A)

 

18) Sigmoid was the most commonly used activation function in neural network, until an issue was identified. The issue is that when the gradients are too large in positive or negative direction, the resulting gradients coming out of the activation function get squashed. This is called saturation of the neuron.



That is why ReLU function was proposed, which kept the gradients same as before in the positive direction.


A ReLU unit in neural network never gets saturated.

 A) TRUE

B) FALSE

Solution: (B)

ReLU can get saturated too. This can be on the negative side of x-axis.

 

19) What is the relationship between dropout rate and regularization?

Note: we have defined dropout rate as the probability of keeping a neuron active?

A) Higher the dropout rate, higher is the regularization

B) Higher the dropout rate, lower is the regularization

Solution: (B)

Higher dropout rate says that more neurons are active. So there would be less regularization.

 

20) What is the technical difference between vanilla backpropagation algorithm and backpropagation through time (BPTT) algorithm?

A) Unlike backprop, in BPTT we sum up gradients for corresponding weight for each time step

B) Unlike backprop, in BPTT we subtract gradients for corresponding weight for each time step

Solution: (A)

BPTT is used in context of recurrent neural networks. It works by summing up gradients for each time step

 

21) Exploding gradient problem is an issue in training deep networks where the gradient getS so large that the loss goes to an infinitely high value and then explodes.

What is the probable approach when dealing with “Exploding Gradient” problem in RNNs?

A) Use modified architectures like LSTM and GRUs

B) Gradient clipping

C) Dropout

D) None of these

Solution: (B)

To deal with exploding gradient problem, it’s best to threshold the gradient values at a specific point. This is called gradient clipping.

 

22) There are many types of gradient descent algorithms. Two of the most notable ones are l-BFGS and SGD. l-BFGS is a second order gradient descent technique whereas SGD is a first order gradient descent technique.

In which of the following scenarios would you prefer l-BFGS over SGD?

Data is sparse
Number of parameters of neural network are small
A) Both 1 and 2

B) Only 1

C) Only 2

D) None of these

Solution: (A)

l-BFGS works best for both of the scenarios.

 

23) Which of the following is not a direct prediction technique for NLP tasks?

A) Recurrent Neural Network

B) Skip-gram model

C) PCA

D) Convolutional neural network

Solution: (C)

 

24) Which of the following would be the best for a non-continuous objective during optimization in deep neural net?

A) L-BFGS

B) SGD

C) AdaGrad

D) Subgradient method

Solution: (D)

Other optimization algorithms might fail on non-continuous objectives, but sub-gradient method would not.

 

25) Which of the following is correct?

Dropout randomly masks the input weights to a neuron
Dropconnect randomly masks both input and output weights to a neuron
A) 1 is True and 2 is False

B) 1 is False and 2 is True

C) Both 1 and 2 are True

D) Both 1 and 2 are False

Solution: (D)

In dropout, neurons are dropped; whereas in dropconnect; connections are dropped. So both input and output weights will be rendered in useless, i.e. both will be dropped for a neuron. Whereas in dropconnect, only one of them should be dropped

 

26) While training a neural network for image recognition task, we plot the graph of training error and validation error for debugging.

What is the best place in the graph for early stopping?

A) A

B) B

C) C

D) D

Solution: (C)

You would “early stop” where the model is most generalized. Therefore option C is correct.

 

27) Research is going on to solve image inpainting problem using deep learning. For this, which loss function would be appropriate for computing the pixel-wise region to be inpainted?



Image inpainting is one of those problems which requires human expertise for solving it. It is particularly useful to repair damaged photos or videos. Below is an example of input and output of an image inpainting example.

A) Euclidean loss

B) Negative-log Likelihood loss

C) Any of the above

Solution: (C)

Both A and B can be used as a loss function for image inpainting problem.

 

28) Backpropagation works by first calculating the gradient of ___ and then propagating it backwards. 

A) Sum of squared error with respect to inputs

B) Sum of squared error with respect to weights

C) Sum of squared error with respect to outputs

D) None of the above

Solution: (C)

29) Mini-Batch sizes when defining a neural network are preferred to be multiple of 2’s such as 256 or 512. What is the reason behind it?

A) Gradient descent optimizes best when you use an even number

B) Parallelization of neural network is best when the memory is used optimally

C) Losses are erratic when you don’t use an even number

D) None of these

Solution: (B)

 

30) Xavier initialization is most commonly used to initialize the weights of a neural network. Below is given the formula for initialization.



If weights at the start are small, then signals reaching the end will be too tiny.
If weights at the start are too large, signals reaching the end will be too large.
Weights from Xavier’s init are drawn from the Gaussian distribution.
Xavier’s init helps reduce vanishing gradient problem.

Xavier’s init is used to help the input signals reach deep into the network. Which of the following statements are true?

A) 1, 2, 4

B) 2, 3, 4

C) 1, 3, 4

D) 1, 2, 3

E) 1, 2, 3, 4

Solution: (D)

All of the above statements are true.

 

31) As the length of sentence increases, it becomes harder for a neural translation machine to perform as sentence meaning is represented by a fixed dimensional vector. To solve this, which of the following could we do?

A) Use recursive units instead of recurrent

B)Use attention mechanism

C) Use character level translation

D) None of these

Solution: (B)

32) A recurrent neural network can be unfolded into a full-connected neural network with infinite length.

A) TRUE

B) FALSE

Solution: (A)

Recurrent neuron can be thought of as a neuron sequence of infinite length of time steps.

 

33) Which of the following is a bottleneck for deep learning algorithm?

A) Data related to the problem

B) CPU to GPU communication

C) GPU memory

D) All of the above

Solution: (D)

Along with having the knowledge of how to apply deep learning algorithms, you should also know the implementation details. Therefore you should know that all the above mentioned problems are a bottleneck for deep learning algorithm.

 

34) Dropout is a regularization technique used especially in the context of deep learning. It works as following, in one iteration we first randomly choose neurons in the layers and masks them. Then this network is trained and optimized in the same iteration. In the next iteration, another set of randomly chosen neurons are selected and masked and the training continues.



Dropout technique is not an advantageous technique for which of the following layers?

A) Affine layer

B) Convolutional layer

C) RNN layer

D) None of these

Solution: (C)

Dropout does not work well with recurrent layer. You would have to modify dropout technique a bit to get good results.

 

35) Suppose your task is to predict the next few notes of song when you are given the preceding segment of the song.

For example:

The input given to you is an image depicting the music symbols as given below,



Your required output is an image of succeeding symbols.



Which architecture of neural network would be better suited to solve the problem?

A) End-to-End fully connected neural network

B) Convolutional neural network followed by recurrent units

C) Neural Turing Machine

D) None of these

Solution: (B)

CNN work best on image recognition problems, whereas RNN works best on sequence prediction. Here you would have to use best of both worlds!

 

36) When deriving a memory cell in memory networks, we choose to read values as vector values instead of scalars. Which type of addressing would this entail?

A) Content-based addressing

B) Location-based addressing

Solution: (A)

 

37) It is generally recommended to replace pooling layers in generator part of convolutional generative adversarial nets with ________ ?

A) Affine layer

B) Strided convolutional layer

C) Fractional strided convolutional layer

D) ReLU layer

Solution: (C)

Option C is correct. Go through this link.

 

Question Context 38-40

GRU is a special type of Recurrent Neural Networks proposed to overcome the difficulties of classical RNNs. This is the paper in which they were proposed: “On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. Read the full paper here. 

38) Which of the following statements is true with respect to GRU?

Units with short-term dependencies have reset gate very active.
Units with long-term dependencies have update gate very active
A) Only 1

B) Only 2

C) None of them

D) Both 1 and 2

Solution: (D)

 

39) If calculation of reset gate in GRU unit is close to 0, which of the following would occur?

A) Previous hidden state would be ignored

B) Previous hidden state would be not be ignored

Solution: (A)

 

40) If calculation of update gate in GRU unit is close to 1, which of the following would occur? 

A) Forgets the information for future time steps

B) Copies the information through many time steps

Solution: (B)

1) Match the following image formats to their correct number of channels

GrayScale
RGB
 1 channel
2 channels
3 channels
4 channels
None
A) RGB -> I, GrayScale-> III
B) RGB -> IV, GrayScale-> II
C) RGB -> III, GrayScale -> I
D) RGB -> II, GrayScale -> I

Solution: C
Grayscale images have one number per pixel, and are stored as an m × n matrix, whereas Color images  have 3 numbers per pixel – red, green, and blue brightness (RGB)

 

2)

Suppose you have to rotate an image. Image rotation is nothing but multiplication of image by a specific matrix to get a new transformed image.


 
 

For simplicity, we consider one point in the image to rotate with co-ordinates as (1, 0) to a co-ordinate of (0, 1), which of the following matrix would we have to multiply with?

A) 
B) 
C) 
D) 

Solution: C
The calculation of would be like this;

[[0], [1]] = [[0, -1], [1, 0]] x [1, 0]

 

3) [True or False] To blur an image, you can use a linear filter

A) TRUE
B) FALSE

Solution: B
Blurring compares neighboring pixels in a filter and smooth them. For this, you cannot use a linear filter.

 

4) Which of the following is a challenge when dealing with computer vision problems?

A) Variations due to geometric changes (like pose, scale etc)
B) Variations due to photometric factors (like illumination, appearance etc)
C) Image occlusion
D) All of the above

Solution: D
All the above mentioned options are challenges in computer vision

 

5)

Suppose we have an image given below.


 
Our task is to segment the objects in the image. A simple way to do this is to represent the image in terms of the intensity of pixels and the cluster them according to the values. On doing this, we got this type of structure.



Suppose we choose k-means clustering to solve the problem, what would be the appropriate value of k from just a visual inspection of the intensity graph?
A) 1
B) 2
C) 3
D) 4

Solution: C
Three clusters will be formed; points in the circle, points in the square and the points excluding both of these objects

 

6)

 


 
In this image, you can find an edge labelled in the red region. Which form of discontinuity creates this kind of edge?

A) Depth Discontinuity
B) Surface color Discontinuity
C) Illumination discontinuity
D) None of the above

Solution: A

The chair and wall are far from each other, causing an edge in the image.

 

7) Finite difference filters in image processing are very susceptible to noise. To cope up with this, which of the following methods can you use so that there would be minimal distortions by noise?

A) Downsample the image
B) Convert the image to grayscale from RGB
C) Smooth the image
D) None of the above

Solution: C

Smoothing helps in reducing noise by forcing pixels to be more like their neighbours

 

8) Consider and image with width and height as 100×100. Each pixel in the image can have a color from Grayscale, i.e. values. How much space would this image require for storing?

Note: No compression is done.

A) 2,56,00,000
B) 25,60,000
C) 2,56,000
D) 8,00,000
E) 80,000
F) 8,000

Solution: E

The answer will be 8x100x100 because 8 bits will be required to represent a number from 0-256

 

9) [True or False] Quantizing an image will reduce the amount of memory required for storage.

A) TRUE
B) FALSE

Solution: A

The statement given is true.

 

10) Suppose we have a grayscale image, with most of the values of pixels being same. What can we use to compress the size of image?

A) Encode the pixels with same values in a dictionary
B) Encode the sequence of values of pixels
C) No compression can be done

Solution: A

Encoding same values of pixels will greatly reduce the size for storage

 

11) [True or False] JPEG is a lossy image compression technique

A) TRUE
B) FALSE

Solution: A

The reason for JPEG being a lossy compression technique is because of the use of quantization.

 

12) Given an image with only 2 pixels and 3 possible values for each pixel, what is the number of possible image histograms that can be formed?

A) 3
B) 6
C) 9
D) 12

Solution: C

The permutations possible of the histograms would be 9.

 

13) Suppose we have a 1D image with values as

 

[2, 5, 8, 5, 2]

Now we apply average filter on this image of size 3. What would be the value of the last second pixel?

 

A) The value would remain the same
B) The value would increase by 2
C) The value would decrease by 2
D) None of the above

Solution: A

(8+5+2)/3 will become 5. So there will be no change.

 

14) fMRI (Functional magnetic resonance imaging) is a technology where volumetric scans of the brain are acquired while the subject is performing some cognitive tasks over time. What is the dimensionality of fMRI output signals?

A) 1D
B) 2D
C) 3D
D) None of the above

Solution: D

The question itself mentions “volumetric scans” over time, so it would be a series of 3D scans

 

15) Which of the following methods is used as a model fitting method for edge detection?

A) SIFT
B) Difference of Gaussian detector
C) RANSAC
D) None of the above

Solution: C

RANSAC is used to find the best fit line in edge detection

 

16)

Suppose we have an image which is noisy. This type of noise in the image is called salt-and-pepper noise



[True or False] Median filter technique is the best way to denoise this image
A) TRUE
B) FALSE

Solution: A

Median filter technique helps reduce noise to a good enough extent

 

17) If we convolve an image with the matrix given below, what would be the relation between the original and modified image?



A) The image will be shifted to the right by 1 pixel
B) The image will be shifted down by 1 pixel
C) The image will be shifted to the left by 1 pixel
D) The image will be shifted up by 1 pixel
Solution: A

I would suggest you to try this yourself and see the result!

 

18) Which of the following is a correct way to sharpen an image?

A)

Convolve the image with identity matrix
Subtract this resulting image from the original
Add this subtracted result back to the original image
B)

Smooth the image
Subtract this smoothed image from the original
Add this subtracted result back to the original image
C)

Smooth the image
Add this smoothed image back to the original image
D) None of the above

Solution: B

Option B gives a correct way to sharpen an image

 

19) Below given images are two operations performed on a signal. Can you identify which is which?



A) Operation 1 is cross correlation between signal f and signal g, whereas operation 2 is convolution function applied to signal f and signal g
B) Operation 2 is cross correlation between signal f and signal g, whereas operation 1 is convolution function applied to signal f and signal g
Solution: A

Correlation and convolution are two different methods with give different result. Convolution defines how much the signals overlap, whereas correlation tries to find the relation between the signals

 

20) [True or False] By using template matching along with cross correlation, you can build a vision system for TV remote control

A) TRUE
B) FALSE

Solution: A

This is a excellent example of cross correlation in computer vision. Refer paper “Computer Vision for Interactive Computer Graphics,” W.Freeman et al, IEEE Computer Graphics and Applications

 

21) Suppose you are creating a face detector in the wild. Which of the following features would you select for creating a robust facial detector?

1. Location of iris, eyebrow and chin
2. Boolean feature: Is the person smiling or not
3. Angle of orientation of face
4. Is the person sitting or standing

A) 1, 2
B) 1, 3
C) 1, 2, 3
D) 1, 2, 3, 4

Solution: B

Options 1, 3 would be relevant features for the problem, but 2, 4 may not be

 

22) Which of the following is example of low level feature in an image?

A) HOG
B) SIFT
C) HAAR features
D) All of the above

Solution: D

All the above are examples of low-level features

 

23) In RGBA mode of color representation, what does A represent?

A) Depth of an image
B) Intensity of colors
C) Opacity of an image
D) None of the above

Solution: C

Opacity can be mentioned by introducing it as a fourth parameter in RGB

 

24) In Otsu thresholding technique, you remove the noise by thresholding the points which are irrelevant and keeping those which do not represent noise.


 
In the image given, at which point would you threshold on?

A) A
B) B
C) C
D) D

Solution: B

Line B would catch most of the noise in the image.

 

25) Which of the following data augmentation technique would you prefer for an object recognition problem?

`
A) Horizontal flipping
B) Rescaling
C) Zooming in the image
D) All of the above

Solution: D

All the mentioned techniques can be used for data augmentation.

